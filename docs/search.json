[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Eun Joo Kwon\n   55 Park Place SE, Desk 19\n   ekwon8@gsu.edu\n   Schedule an appointment\n\n\n\n\n\n   Any day\n   Asynchronous\n   Anywhere"
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "Syllabus",
    "section": "Course objectives",
    "text": "Course objectives\nData rarely speaks for itself. On their own, the facts contained in raw data are difficult to understand, and in the absence of beauty and order, it is impossible to understand the truth that the data shows.\nIn this class, you’ll learn how to use industry-standard graphic and data design techniques to create beautiful, understandable visualizations and uncover truth in data.\nBy the end of this course, you will become (1) literate in data and graphic design principles, and (2) an ethical data communicator, by producing beautiful, powerful, and clear visualizations of your own data. Specifically, you should:\n\nUnderstand the principles of data and graphic design\nEvaluate the credibility, ethics, and aesthetics of data visualizations\nCreate well-designed data visualizations with appropriate tools\nShare data and graphics in open forums\nBe curious and confident in consuming and producing data visualizations\n\nThis class will expose you to R—one of the most popular, sought-after, and in-demand statistical programming languages. Armed with the foundation of R skills you’ll learn in this class, you’ll know enough to be able to find how to visualize and analyze any sort of data-based question in the future."
  },
  {
    "objectID": "syllabus.html#important-pep-talk",
    "href": "syllabus.html#important-pep-talk",
    "title": "Syllabus",
    "section": "Important pep talk!",
    "text": "Important pep talk!\nI promise you can succeed in this class.\nLearning R can be difficult at first—it’s like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like {ggplot2}—made this wise observation:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\nEven experienced programmers find themselves bashing their heads against seemingly intractable errors. If you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, e-mail me, etc.\n\n\n\n\nAlison Horst: Gator error"
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course materials",
    "text": "Course materials\nMost of the readings and software in this class are free. There are free online version of all the textbooks, R and RStudio are inherently free.\n\nBooks, articles, and other materials\nWe’ll rely heavily on these books, which are mostly available online (for free!). I recommend getting the printed versions of these books if you are interested, but it is not required.\n\nAlberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016). $20 used, $50 new at Amazon.\nKieran Healy, Data Visualization: A Practical Introduction (Princeton: Princeton University Press, 2018), http://socviz.co/. FREE online; $20 used, $40 new at Amazon.\nClaus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/. FREE online; $36 used, $50 new at Amazon. You can use the online version.\n\nThere will occasionally be additional articles and videos to read and watch. When this happens, links to these other resources will be included on the content page for that session.\n\n\nR and RStudio\nYou will do all of your analysis with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations produces the actual statistics and graphical output, while RStudio provides a nice interface for running R code.\nR is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free Posit.cloud service, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R! We will have a shared class workspace in Posit.cloud that will let you quickly copy templates for examples, exercises, and mini projects.\nPosit.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets or more complicated analysis and graphics. You also can’t use your own custom fonts with Posit.cloud. Over the course of the semester, you’ll probably want to get around to installing R, RStudio, and other R packages on your computer and wean yourself off of Posit.cloud. This isn’t 100% necessary, but it’s helpful.\nYou can find instructions for installing R, RStudio, and all the tidyverse packages here.\n\n\nOnline help\nData science and statistical programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if you’ve been doing this stuff for years!).\nFortunately there are tons of online resources to help you with this. Two of the most important are StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and Posit Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nIf you use Mastodon or Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful.\nChatGPT is generally relatively okay with R questions, but it will often hallucinate responses—it really likes to imagine packages that don’t exist and then give you answers with them. Think of it as glorified autocomplete. I’d encourage you to read the first few paragraphs here about a few important things to keep in mind when using ChatGPT, though.\nSearching for help with R on Google can sometimes be tricky because the program name is, um, a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work will deal with {ggplot2}, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).\nAdditionally, we have a class chatroom at Slack where anyone in the class can ask questions and anyone can answer. I will monitor Slack regularly and will respond quickly. (It’s one of the rare Slack workspaces where I actually have notifications enabled!) Ask questions about the readings, exercises, and mini projects. You’ll likely have similar questions as your peers, and you’ll likely be able to answer other peoples’ questions too."
  },
  {
    "objectID": "syllabus.html#course-schedule",
    "href": "syllabus.html#course-schedule",
    "title": "Syllabus",
    "section": "Course schedule",
    "text": "Course schedule\nWe have no regularly scheduled meeting times.\nInstead, 100% of the class content is asynchronous. You can do the readings and watch the videos on your own schedule at whatever time works best for you. Many of you work full time and you have childcare and parental care responsibilities, leaving you with only evenings for coursework.\nEach week has (1) a set of readings and an accompanying lecture, (2) a lesson, (3) an example with lots of reference code, and (4) a short assignment. The schedule page provides an overview of all these moving parts.\nI recommend following this general process for each session:\n\nDo everything on the content page ()\nWork through the lesson page ()\nComplete the assignment () while referencing the example ()"
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\nBe nice. Be honest. Don’t cheat.\nWe will also follow Georgia State’s Code of Conduct and GSU’s Policy on Academic Honesty. Violation of Academic Honesty will result in an F in the course and possible disciplinary action.1\nThis syllabus reflects a plan for the semester. Deviations may become necessary as the course progresses.\n\nStudent hours\nPlease watch this video: (this is not me, btw—this is a different Andrew)\n\n\n\n\n \nStudent hours are set times dedicated to all of you (most professors call these “office hours”; I don’t2). This means that I will be in my office at home (wistfully) waiting for you to come by talk to me remotely with whatever questions you have. This is the best and easiest way to find me and the best chance for discussing class material and concerns.\nWe will generally meet online via Webex. Make an appointment with me here.If you would like to schedule an in-person meeting, please feel free to email me, and we can coordinate a suitable time. You can choose an online slot (15/30 minute), the confirmation e-mail will contain a link for a Webex meeting. You can also find me through e-mail and Slack.\n\n\nLate work\nWith the exception of the mini projects and the final project, there’s no penalty for late work. I would highly recommend staying caught up as much as possible, but if you need to turn something in late, that’s fine. I might not be able to give you good feedback if you turn something in late, but that’s really the only practical penalty—there’s no harm to your grade."
  },
  {
    "objectID": "syllabus.html#assignments-and-grades",
    "href": "syllabus.html#assignments-and-grades",
    "title": "Syllabus",
    "section": "Assignments and grades",
    "text": "Assignments and grades\nYou can find descriptions for all the assignments on the assignments page.\n\n\n\n\n\n\n\n\n\nAssignment\nPoints\nPercent\n\n\n\n\nReflections (15 × 10)\n150\n23%\n\n\nExercises (15 × 10)\n150\n23%\n\n\nMini project 1\n75\n12%\n\n\nMini project 2\n75\n12%\n\n\nFinal project\n200\n31%\n\n\nTotal\n650\n—\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n93–100%\nC\n73–76%\n\n\nA−\n90–92%\nC−\n70–72%\n\n\nB+\n87–89%\nD+\n67–69%\n\n\nB\n83–86%\nD\n63–66%\n\n\nB−\n80–82%\nD−\n60–62%\n\n\nC+\n77–79%\nF\n&lt; 60%"
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSo seriously, just don’t cheat or plagiarize!↩︎\nThere’s fairly widespread misunderstanding about what office hours actually are! Many students often think that they are the times I shouldn’t be disturbed, which is the exact opposite of what they’re for!↩︎"
  },
  {
    "objectID": "resource/visualization.html",
    "href": "resource/visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "The Stories Behind a Line\nAustralia as 100 people: You can make something like this with d3 and the potato project.\nMarrying Later, Staying Single Longer"
  },
  {
    "objectID": "resource/visualization.html#interesting-and-excellent-real-world-examples",
    "href": "resource/visualization.html#interesting-and-excellent-real-world-examples",
    "title": "Visualization",
    "section": "",
    "text": "The Stories Behind a Line\nAustralia as 100 people: You can make something like this with d3 and the potato project.\nMarrying Later, Staying Single Longer"
  },
  {
    "objectID": "resource/visualization.html#how-to-select-the-appropriate-chart-type",
    "href": "resource/visualization.html#how-to-select-the-appropriate-chart-type",
    "title": "Visualization",
    "section": "How to select the appropriate chart type",
    "text": "How to select the appropriate chart type\nMany people have created many useful tools for selecting the correct chart type for a given dataset or question. The Financial Times has an excellent diagram that shows what kind of charts are appropriate for which kinds of data you have:\n\nThe Financial Times’s “Visual Vocabulary” (PDF poster and interactive website)\n\nHere are some other fantastic resources too:\n\nThe Data Visualisation Catalogue: Descriptions, explanations, examples, and tools for creating 60 different types of visualizations.\nThe Data Viz Project: Descriptions and examples for 150 different types of visualizations. Also allows you to search by data shape and chart function (comparison, correlation, distribution, geographical, part to whole, trend over time, etc.).\nFrom Data to Viz: A decision tree for dozens of chart types with links to R and Python code.\nThe Chartmaker Directory: Examples of how to create 51 different types of visualizations in 31 different software packages, including Excel, Tableau, and R.\nR Graph Catalog: R code for 124 ggplot graphs.\nEmery’s Essentials: Descriptions and examples of 26 different chart types."
  },
  {
    "objectID": "resource/visualization.html#general-resources",
    "href": "resource/visualization.html#general-resources",
    "title": "Visualization",
    "section": "General resources",
    "text": "General resources\n\nStorytelling with Data: Blog and site full of resources by Cole Nussbaumer Knaflic.\nAnn K. Emery’s blog: Blog and tutorials by Ann Emery.\nEvergreen Data: Helful resources by Stephanie Evergreen.\nPolicyViz: Regular podcast and site full of helpful resources by Jon Schwabisch.\nVisualising Data: Fantastic collection of visualization resources, articles, and tutorials by Andy Kirk.\nInfo We Trust: Detailed explorations of visualizations by RJ Andrews, including a beautiful visual history of the field.\nFlowingData: Blog by Nathan Yau.\nInformation is Beautiful: Blog by David McCandless.\nJunk Charts: Blog by Kaiser Fung.\nWTF Visualizations: Visualizations that make you ask “wtf?”\nThe Data Visualization Checklist: A helpful set of criteria for grading the effectiveness of a graphic.\nData Literacy Starter Kit: Compilation of resources to become data literate by Laura Calloway.\nSeeing Data: A series of research projects about perceptions and visualizations."
  },
  {
    "objectID": "resource/visualization.html#visualization-in-excel",
    "href": "resource/visualization.html#visualization-in-excel",
    "title": "Visualization",
    "section": "Visualization in Excel",
    "text": "Visualization in Excel\n\nHow to Build Data Visualizations in Excel: Detailed tutorials for creating 14 different visualizations in Excel.\nAnn Emery’s tutorials: Fantastic series of tutorials for creating charts in Excel."
  },
  {
    "objectID": "resource/visualization.html#visualization-in-tableau",
    "href": "resource/visualization.html#visualization-in-tableau",
    "title": "Visualization",
    "section": "Visualization in Tableau",
    "text": "Visualization in Tableau\nBecause it is focused entirely on visualization (and because it’s a well-supported commercial product), Tableau has a phenomenal library of tutorials and training videos. There’s a helpful collections of videos here, as well."
  },
  {
    "objectID": "resource/style.html",
    "href": "resource/style.html",
    "title": "R style suggestions",
    "section": "",
    "text": "R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\n\nmpg %&gt;% \n  filter(cty &gt; 10, class == \"compact\")\n\nmpg %&gt;% filter(cty &gt; 10, class == \"compact\")\n\nmpg %&gt;% \n  filter(cty &gt; 10, \n         class == \"compact\")\n\nmpg %&gt;% filter(cty&gt;10, class==\"compact\")\n\nfilter(mpg,cty&gt;10,class==\"compact\")\n\nmpg %&gt;% \nfilter(cty &gt; 10, \n                        class == \"compact\")\n\nfilter ( mpg,cty&gt;10,     class==\"compact\" )\n\nBut you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times."
  },
  {
    "objectID": "resource/style.html#r-style-conventions",
    "href": "resource/style.html#r-style-conventions",
    "title": "R style suggestions",
    "section": "",
    "text": "R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\n\nmpg %&gt;% \n  filter(cty &gt; 10, class == \"compact\")\n\nmpg %&gt;% filter(cty &gt; 10, class == \"compact\")\n\nmpg %&gt;% \n  filter(cty &gt; 10, \n         class == \"compact\")\n\nmpg %&gt;% filter(cty&gt;10, class==\"compact\")\n\nfilter(mpg,cty&gt;10,class==\"compact\")\n\nmpg %&gt;% \nfilter(cty &gt; 10, \n                        class == \"compact\")\n\nfilter ( mpg,cty&gt;10,     class==\"compact\" )\n\nBut you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times."
  },
  {
    "objectID": "resource/style.html#main-style-things-to-pay-attention-to-for-this-class",
    "href": "resource/style.html#main-style-things-to-pay-attention-to-for-this-class",
    "title": "R style suggestions",
    "section": "Main style things to pay attention to for this class",
    "text": "Main style things to pay attention to for this class\n\nImportant note: I won’t ever grade you on any of this! If you submit something like filter(mpg,cty&gt;10,class==\"compact\"), I might recommend adding spaces, but it won’t affect your grade or points or anything.\n\n\nSpacing\n\nSee the “Spacing” section in the tidyverse style guide.\n\nPut spaces after commas (like in regular English):\n\n# Good\nfilter(mpg, cty &gt; 10)\n\n# Bad\nfilter(mpg , cty &gt; 10)\nfilter(mpg ,cty &gt; 10)\nfilter(mpg,cty &gt; 10)\n\nPut spaces around operators like +, -, &gt;, =, etc.:\n\n# Good\nfilter(mpg, cty &gt; 10)\n\n# Bad\nfilter(mpg, cty&gt;10)\nfilter(mpg, cty&gt; 10)\nfilter(mpg, cty &gt;10)\n\nDon’t put spaces around parentheses that are parts of functions:\n\n# Good\nfilter(mpg, cty &gt; 10)\n\n# Bad\nfilter (mpg, cty &gt; 10)\nfilter ( mpg, cty &gt; 10)\nfilter( mpg, cty &gt; 10 )\n\n\n\nLong lines\n\nSee the “Long lines” section in the tidyverse style guide.\n\nIt’s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to “Tools” &gt; “Global Options” &gt; “Code” &gt; “Display” and check the box for “Show margin”. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that’s fine. It’s just good practice to avoid going too far past it.\nYou can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:\n\n# Good\nfilter(mpg, cty &gt; 10, class == \"compact\")\n\n# Good\nfilter(mpg, cty &gt; 10, \n       class == \"compact\")\n\n# Good\nfilter(mpg,\n       cty &gt; 10,\n       class == \"compact\")\n\n# Bad\nfilter(mpg, cty &gt; 10, class %in% c(\"compact\", \"pickup\", \"midsize\", \"subcompact\", \"suv\", \"2seater\", \"minivan\"))\n\n# Good\nfilter(mpg, \n       cty &gt; 10, \n       class %in% c(\"compact\", \"pickup\", \"midsize\", \"subcompact\", \n                    \"suv\", \"2seater\", \"minivan\"))\n\n\n\nPipes (%&gt;%) and ggplot layers (+)\nPut each layer of a ggplot plot on separate lines, with the + at the end of the line, indented with two spaces:\n\n# Good\nggplot(mpg, aes(x = cty, y = hwy, color = class)) +\n  geom_point() +\n  geom_smooth() +\n  theme_bw()\n\n# Bad\nggplot(mpg, aes(x = cty, y = hwy, color = class)) +\n  geom_point() + geom_smooth() +\n  theme_bw()\n\n# Super bad\nggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw()\n\n# Super bad and won't even work\nggplot(mpg, aes(x = cty, y = hwy, color = class))\n  + geom_point()\n  + geom_smooth() \n  + theme_bw()\n\nPut each step in a dplyr pipeline on separate lines, with the %&gt;% at the end of the line, indented with two spaces:\n\n# Good\nmpg %&gt;% \n  filter(cty &gt; 10) %&gt;% \n  group_by(class) %&gt;% \n  summarize(avg_hwy = mean(hwy))\n\n# Bad\nmpg %&gt;% filter(cty &gt; 10) %&gt;% group_by(class) %&gt;% \n  summarize(avg_hwy = mean(hwy))\n\n# Super bad\nmpg %&gt;% filter(cty &gt; 10) %&gt;% group_by(class) %&gt;% summarize(avg_hwy = mean(hwy))\n\n# Super bad and won't even work\nmpg %&gt;% \n  filter(cty &gt; 10)\n  %&gt;% group_by(class)\n  %&gt;% summarize(avg_hwy = mean(hwy))\n\n\n\nComments\n\nSee the “Comments” section in the tidyverse style guide.\n\nComments should start with a comment symbol and a single space: #\n\n# Good\n\n#Bad\n\n    #Bad\n\nIf the comment is really short (and won’t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):\n\nmpg %&gt;% \n  filter(cty &gt; 10) %&gt;%  # Only rows where cty is 10 +\n  group_by(class) %&gt;%  # Divide into class groups\n  summarize(avg_hwy = mean(hwy))  # Find the average hwy in each group\n\nYou can add extra spaces to get inline comments to align, if you want:\n\nmpg %&gt;% \n  filter(cty &gt; 10) %&gt;%            # Only rows where cty is 10 +\n  group_by(class) %&gt;%             # Divide into class groups\n  summarize(avg_hwy = mean(hwy))  # Find the average hwy in each group\n\nIf the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to “Code” &gt; “Reflow comment”\n\n# Good\n# Happy families are all alike; every unhappy family is unhappy in its own way.\n# Everything was in confusion in the Oblonskys’ house. The wife had discovered\n# that the husband was carrying on an intrigue with a French girl, who had been\n# a governess in their family, and she had announced to her husband that she\n# could not go on living in the same house with him. This position of affairs\n# had now lasted three days, and not only the husband and wife themselves, but\n# all the members of their family and household, were painfully conscious of it.\n\n# Bad\n# Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it.\n\nThough, if you’re dealing with comments that are that long, consider putting the text in R Markdown instead and having it be actual prose."
  },
  {
    "objectID": "resource/r.html",
    "href": "resource/r.html",
    "title": "R",
    "section": "",
    "text": "I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work in this class will deal with {ggplot2}, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful. Also check out StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nThese resources are also really really helpful:\n\nR for Data Science: A free online book for learning the basics of R and the tidyverse.\nR and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, {ggplot2}, and other R-related things.\nStat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online.\nSTA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online.\nCSE 631: Principles & Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio."
  },
  {
    "objectID": "resource/r.html#learning-r",
    "href": "resource/r.html#learning-r",
    "title": "R",
    "section": "",
    "text": "I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work in this class will deal with {ggplot2}, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful. Also check out StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nThese resources are also really really helpful:\n\nR for Data Science: A free online book for learning the basics of R and the tidyverse.\nR and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, {ggplot2}, and other R-related things.\nStat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online.\nSTA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online.\nCSE 631: Principles & Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio."
  },
  {
    "objectID": "resource/r.html#r-in-the-wild",
    "href": "resource/r.html#r-in-the-wild",
    "title": "R",
    "section": "R in the wild",
    "text": "R in the wild\nA popular (and increasingly standard) way for sharing your analyses and visualizations is to post an annotated explanation of your process somewhere online. RStudio allows you to publish knitted HTML files directly to RPubs, but you can also post your output to a blog or other type of website.1 Reading these kinds of posts is one of the best ways to learn R, since they walk you through each step of the process and show the code and output.\nHere are some of the best examples I’ve come across:\n\nText analysis of Trump’s tweets confirms he writes only the (angrier) Android half (with a follow-up)\nBob Ross - Joy of Painting\nBechdel analysis using the tidyverse: There are also a bunch of other examples using data from FiveThirtyEight.\nSexism on the Silver Screen: Exploring film’s gender divide\nComparison of Quentin Tarantino Movies by Box Office and the Bechdel Test\nWho came to vote in Utah’s caucuses?\nHealth care indicators in Utah counties\nSong lyrics across the United States\nA decade (ish) of listening to Sigur Rós\nWhen is Tom peeping these days?: There are a also bunch of final projects from other R and data visualization classes here and here.\nMapping Fall Foliage\nGeneral (Attys) Distributions\nDisproving Approval"
  },
  {
    "objectID": "resource/r.html#footnotes",
    "href": "resource/r.html#footnotes",
    "title": "R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to be really fancy, you can use blogdown, which makes a complete website with R Markdown files. That’s actually how this site is built (see the source code). You can build your own site with this tutorial.↩︎"
  },
  {
    "objectID": "resource/install.html",
    "href": "resource/install.html",
    "title": "Installing R, RStudio, tidyverse, and tinytex",
    "section": "",
    "text": "You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code."
  },
  {
    "objectID": "resource/install.html#posit.cloud",
    "href": "resource/install.html#posit.cloud",
    "title": "Installing R, RStudio, tidyverse, and tinytex",
    "section": "Posit.cloud",
    "text": "Posit.cloud\nR is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free Posit.cloud service initially, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R! We will have a shared class workspace in Posit.cloud that will let you quickly copy templates for assignments.\nGo to https://posit.cloud/ and create an account. You’ll receive a link to join the shared class workspace separately. If you don’t get this link, let me know and I will invite you."
  },
  {
    "objectID": "resource/install.html#rstudio-on-your-computer",
    "href": "resource/install.html#rstudio-on-your-computer",
    "title": "Installing R, RStudio, tidyverse, and tinytex",
    "section": "RStudio on your computer",
    "text": "RStudio on your computer\nPosit.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets, more complicated analysis, or fancier graphics. Over the course of the semester, you should wean yourself off of Posit.cloud and install all these things locally. This is also important if you want to customize fonts, since Posit.cloud has extremely limited support for fonts other than Helvetica.\nHere’s how you install all these things\n\nInstall R\nFirst you need to install R itself (the engine).\n\nGo to the CRAN (Collective R Archive Network) website: https://cran.r-project.org/\nClick on “Download R for XXX”, where XXX is either Mac or Windows:\n\n\n\nIf you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is 4.3.0) and download it.\n\n\n\nIf you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n\n\n\nDouble click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\nIf you use macOS, download and install XQuartz. You do not need to do this on Windows.\n\n\n\nInstall RStudio\nNext, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\n\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\nThe website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\n\n\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n\n\nDouble click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n\nDouble click on RStudio to run it (check your applications folder or start menu).\n\n\nInstall tidyverse\nR packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter.\n\nThis can sometimes be tedious when you’re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including {ggplot2}) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.\n\nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel.\n\n\nInstall tinytex\nWhen you knit to PDF, R uses a special scientific typesetting program named LaTeX (pronounced “lay-tek” or “lah-tex”; for goofy nerdy reasons, the x is technically the “ch” sound in “Bach”, but most people just say it as “k”—saying “layteks” is frowned on for whatever reason).\nLaTeX is neat and makes pretty documents, but it’s a huge program—the macOS version, for instance, is nearly 4 GB! To make life easier, there’s an R package named {tinytex} that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.\nHere’s how to install {tinytex} so you can knit to pretty PDFs:\n\nUse the Packages in panel in RStudio to install {tinytex} like you did above with {tidyverse}. Alternatively, run install.packages(\"tinytex\") in the console.\nRun tinytex::install_tinytex() in the console.\nWait for a bit while R downloads and installs everything you need.\nThe end! You should now be able to knit to PDF."
  },
  {
    "objectID": "resource/graphics-editors.html",
    "href": "resource/graphics-editors.html",
    "title": "Graphics editors",
    "section": "",
    "text": "For exercise 14, mini-project 2, and your final project, you’ll create plots in R, export them as PDFs, and then edit, enhance, and lay out those PDFs using some sort of graphics editor.\nIn the past, I’ve had students use Adobe Illustrator for this, since GSU provided free student access to Adobe Creative Cloud, which includes Illustrator. The most common Creative Cloud programs people use for data visualization-related work are:\n\nPhotoshop: edit photos and other bitmap images\nIllustrator: edit vector images\nInDesign: lay out text and images for books, magazines, brochures, posters, and all other printed things\n\nHowever, GSU no longer provides off-campus access to Adobe software. If you’re faculty or staff, you can still access Creative Cloud for free; if you’re a student you have to use an on-campus computer lab.\nThis is sad because knowing how to use programs like Illustrator is incredibly valuable. Even if you never touch R again after this class, the graphic design programs included in Creative Cloud are industry-standard and used literally everywhere, and knowing how to use them is an important skill!\nThere are some alternative options though.\n\nCreative Cloud as a student\nYou can use the whole Creative Cloud Suite for $20/month as a student, and they have a 14-day free trial. Creative Cloud is a subscription service, so you can subscribe on and off as often as you want.\n\n\nAffinity Suite\nAffinity is a smaller rival to Adobe and they have their own set of three programs for graphic design-related tasks. Unlike Creative Cloud, the programs in the Affinity Suite are not subscription-based. You buy them and then you own them forever.\nThere are three programs that are general equivalents of the core Adobe programs:\n\nAffinity Photo = Photoshop\nAffinity Designer = Illustrator\nAffinity Publisher = InDesign\n\n\n\n\n\n\n\nCaveat!\n\n\n\nBig caveat here: I’ve never actually used any of these Affinity programs. I use Adobe stuff for all my work. But I’ve heard fantastic things about them and have seen them in action—they’re as good as Adobe’s stuff.\n\n\n\n\nOpen source alternatives\nThe open source community has created free programs that are rough equivalents of these core Adobe programs too:\n\nGIMP = Photoshop\nInkscape = Illustrator\nScribus = InDesign\n\nThese are all free and they work on macOS and Windows (and Linux if you’re into that), but they can be a little lot rough around the edges and tricky to work with. Adobe, Affinity, and other companies have full time developers focused on making good user interfaces and experiences; these open source clones do not. You can make the same kind of output with GIMP, Inkscape, and Scribus that you can with Adobe Creative Cloud and the Affinity Suite, but there’s a bit of an extra learning curve (and a lot of bumps along the way).\nBut you can’t beat free.\n\n\nSummary\n\n\n\n\n\n\n  \n    \n    \n    \n    \n  \n  \n    \n    \n      File type\n      Adobe\n      Affinity\n      Open source\n    \n  \n  \n    Bitmaps\n\nPhotoshop\n\nPhoto\n\nGIMP\n\n    Vectors\n\nIllustrator\n\nDesigner\n\nInkscape\n\n    Layout\n\nInDesign\n\nPublisher\n\nScribus\n\n    Cost\n\n$$$Monthly Creative Cloud subscription\n\n$One-time purchase\n\nFree\n\n    Notes\n\nIndustry standard\n\n\nFree, but rough learning curve"
  },
  {
    "objectID": "resource/data.html",
    "href": "resource/data.html",
    "title": "Data",
    "section": "",
    "text": "There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n\nData is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\nGoogle Dataset Search: Google indexes thousands of public datasets; search for them here.\nKaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test.\n360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\nUS City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing.\nPolitical science and economics datasets: There’s a wealth of data available for political science- and economics-related topics:\n\nFrançois Briatte’s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities.\nThomas Leeper’s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.).\nErik Gahner’s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)"
  },
  {
    "objectID": "news/index.html",
    "href": "news/index.html",
    "title": "News",
    "section": "",
    "text": "Subscribe!\n\n\n\nYou can use a feed reader like Feedly or use an RSS-to-email service like Blogtrottr to subscribe to these updates and messages. I’ll also e-mail out links to them when there are new updates.\n\n\n\n RSS\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nWednesday December 13, 2023 at 2:15 PM\n\n\nWhat comes next?\n\n\ncourse details\n\n\n\n\nWednesday November 29, 2023 at 2:53 PM\n\n\nFinal deadlines for things\n\n\ncourse details\n\n\n\n\nWednesday November 29, 2023 at 2:34 PM\n\n\nWeeks 11, 12, and 13 tips and FAQs\n\n\nFAQs\n\n\n\n\nMonday November 6, 2023 at 11:09 PM\n\n\nNo more answer keys\n\n\ncourse details\n\n\n\n\nMonday November 6, 2023 at 11:04 PM\n\n\nGraphics editors\n\n\ncourse details\n\n\n\n\nMonday November 6, 2023 at 11:02 PM\n\n\nMini project 1 feedback\n\n\nfaq,feedback\n\n\n\n\nMonday November 6, 2023 at 10:54 PM\n\n\nWeeks 8, 9, and 10 tips and FAQs\n\n\nFAQs\n\n\n\n\nThursday October 12, 2023 at 4:37 PM\n\n\nSlow down, simplify and do small things\n\n\nadvice\n\n\n\n\nThursday October 12, 2023 at 4:30 PM\n\n\nTips for debugging and cleaning broken code\n\n\nadvice\n\n\n\n\nTuesday October 10, 2023 at 12:04 PM\n\n\nWeek 7 tips and FAQs\n\n\nFAQs\n\n\n\n\nTuesday October 3, 2023 at 3:00 PM\n\n\nTime for mini project 1!\n\n\ncourse details\n\n\n\n\nTuesday October 3, 2023 at 2:48 PM\n\n\nMaking cleaner, nicer R Markdown output\n\n\nadvice\n\n\n\n\nTuesday October 3, 2023 at 2:44 PM\n\n\nWeek 6 FAQs\n\n\nFAQs\n\n\n\n\nWednesday September 27, 2023 at 3:16 PM\n\n\nWeek 5 FAQs\n\n\nFAQs\n\n\n\n\nTuesday September 19, 2023 at 9:22 AM\n\n\nWeek 4 FAQs\n\n\nFAQs\n\n\n\n\nTuesday September 12, 2023 at 8:20 PM\n\n\nA few quick general R tips\n\n\nFAQs\n\n\n\n\nTuesday September 12, 2023 at 8:19 PM\n\n\nDeadlines, late work, and student hours\n\n\nadvice,course details\n\n\n\n\nTuesday September 12, 2023 at 8:14 PM\n\n\nWeek 3 FAQs\n\n\nFAQs\n\n\n\n\nTuesday September 5, 2023 at 8:06 PM\n\n\nZeroes in the gradebook\n\n\ncourse details\n\n\n\n\nTuesday September 5, 2023 at 7:58 PM\n\n\nWeek 2 FAQs\n\n\nFAQs\n\n\n\n\nWednesday August 30, 2023 at 9:08 PM\n\n\nWeek 1 FAQs\n\n\nFAQs\n\n\n\n\nMonday August 21, 2023 at 10:08 AM\n\n\nPre-class FAQs\n\n\nFAQs\n\n\n\n\nMonday August 21, 2023 at 9:52 AM\n\n\nCopy, paste, and tweak\n\n\nadvice\n\n\n\n\nMonday August 21, 2023 at 9:33 AM\n\n\nThe 30-minute rule and how to ask for help\n\n\nadvice\n\n\n\n\nMonday August 21, 2023 at 9:23 AM\n\n\nBuilding community\n\n\nadvice\n\n\n\n\nMonday August 21, 2023 at 8:58 AM\n\n\nFirst day of class!\n\n\ngetting started\n\n\n\n\nFriday August 18, 2023 at 5:09 PM\n\n\nAnnouncements and updates\n\n\ngetting started\n\n\n\n\nFriday August 18, 2023 at 5:06 PM\n\n\nImportant R stuff\n\n\ngetting started\n\n\n\n\nFriday August 18, 2023 at 5:02 PM\n\n\nWelcome to class!\n\n\ngetting started\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "news/2023-11-29_final-deadlines.html",
    "href": "news/2023-11-29_final-deadlines.html",
    "title": "Final deadlines for things",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nRemember from the syllabus that there aren’t really any official deadlines:\n\nWith the exception of the mini projects and the final project, there’s no penalty for late work. There’s no such thing as late work. I would highly recommend staying caught up as much as possible, but if you need to turn something in late, that’s fine—there’s no penalty. I might not be able to give you good feedback if you turn something in late, but that’s really the only practical penalty—there’s no harm to your grade.\n\nTwo deadlines do matter though:\n\nEverything except the final project needs to be turned in by 11:59 PM on Tuesday, December 5 (that’s the day after the last day of classes)\nThe final project has to be submitted by 11:59 PM on Monday, December 11 (I have to submit grades shortly thereafter)"
  },
  {
    "objectID": "news/2023-11-06_no-more-answer-keys.html",
    "href": "news/2023-11-06_no-more-answer-keys.html",
    "title": "No more answer keys",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nOne last little comment. You may have noticed that iCollege doesn’t have answer keys for some of the more recent exercises. That’s by design.\nAfter week 8, there are no more answer keys.\nThat’s because starting with the annotations assignment, all the way until the end, your assignments are a lot more open ended. There are no official final correct plots. Most of these assignments go something like “find some data you like and visualize it some way.” For the week on annotations, you made some sort of plot and put annotations on it. For the week on interactivity, you made some sort of plot and fed it to ggplotly(). For the week on time, you’ll make a plot that visualizes time. For the week on space, you’ll make some sort of map.\nThere are no right answers.\nDo whatever you want with whatever data you want. Do whatever is most useful and interesting for you."
  },
  {
    "objectID": "news/2023-11-06_feedback-mini-project-01.html",
    "href": "news/2023-11-06_feedback-mini-project-01.html",
    "title": "Mini project 1 feedback",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nGreat work with your first mini projects! You successfully took real world data, cleaned it up, made a plot with it, and told a story about rats in New York!\nI left some similar comments for a lot of you, so I figured I’d compile those here along with examples of how to address them.\nFirst, I’ll load and clean the data so I can illustrate stuff below:\n\nlibrary(tidyverse)\n\nrats_raw &lt;- read_csv(\n  \"https://datavizf23.classes.andrewheiss.com/files/data/external_data/Rat_Sightings.csv\", \n  na = c(\"\", \"NA\", \"N/A\")\n)\n\nrats_clean &lt;- rats_raw %&gt;%\n  rename(created_date = `Created Date`,\n         location_type = `Location Type`,\n         borough = Borough) %&gt;%\n  mutate(created_date = mdy_hms(created_date)) %&gt;%\n  mutate(sighting_year = year(created_date),\n         sighting_month = month(created_date),\n         sighting_day = day(created_date),\n         sighting_weekday = wday(created_date, label = TRUE, abbr = FALSE)) %&gt;%\n  filter(borough != \"Unspecified\")\n\n\nWarnings and messages\n\nYour knitted document has warnings and package loading messages.\n\nYou should turn off those warnings and messages. See this and this for more about how.\n\n\nALL CAPS\n\nConsider converting the borough names to NOT ALL CAPS, since you don’t repeat all caps anywhere else\n\nIn the data, the borough names are all spelled with ALL CAPS:\n\nrats_clean %&gt;% \n  select(created_date, borough)\n## # A tibble: 101,913 × 2\n##    created_date        borough      \n##    &lt;dttm&gt;              &lt;chr&gt;        \n##  1 2015-09-04 00:00:00 MANHATTAN    \n##  2 2015-09-04 00:00:00 STATEN ISLAND\n##  3 2015-09-04 00:00:00 STATEN ISLAND\n##  4 2015-09-04 00:00:00 BROOKLYN     \n##  5 2015-09-04 00:00:00 BRONX        \n##  6 2015-09-04 00:00:00 BROOKLYN     \n##  7 2015-09-04 00:00:00 QUEENS       \n##  8 2015-09-04 00:00:00 BROOKLYN     \n##  9 2015-09-04 00:00:00 MANHATTAN    \n## 10 2015-09-04 00:00:00 STATEN ISLAND\n## # ℹ 101,903 more rows\n\nYou can convert that to not all caps with one of two functions from the {stringr} package (which is one of the nine that R loads when you run library(tidyverse)): str_to_title() changes text to title case (where each word is capitalized) while str_to_sentence() changes text to sentence case (where the first letter in a sentence is capitalized):\n\nstr_to_title(\"OH NO! RATS IN NEW YORK\")\n## [1] \"Oh No! Rats In New York\"\nstr_to_sentence(\"OH NO! RATS IN NEW YORK\")\n## [1] \"Oh no! Rats in new york\"\n\nYou can use it with mutate():\n\nrats_clean &lt;- rats_clean %&gt;% \n  mutate(borough = str_to_title(borough))\n\nrats_clean %&gt;% \n  select(created_date, borough)\n## # A tibble: 101,913 × 2\n##    created_date        borough      \n##    &lt;dttm&gt;              &lt;chr&gt;        \n##  1 2015-09-04 00:00:00 Manhattan    \n##  2 2015-09-04 00:00:00 Staten Island\n##  3 2015-09-04 00:00:00 Staten Island\n##  4 2015-09-04 00:00:00 Brooklyn     \n##  5 2015-09-04 00:00:00 Bronx        \n##  6 2015-09-04 00:00:00 Brooklyn     \n##  7 2015-09-04 00:00:00 Queens       \n##  8 2015-09-04 00:00:00 Brooklyn     \n##  9 2015-09-04 00:00:00 Manhattan    \n## 10 2015-09-04 00:00:00 Staten Island\n## # ℹ 101,903 more rows\n\n(I’d use str_to_title() here since Staten Island is two words; if you use str_to_sentence() it’ll turn into “Staten island”.)\nNow your plots will have nicer borough names:\n\nrats_summarized &lt;- rats_clean %&gt;% \n  group_by(borough) %&gt;% \n  summarize(total = n())\n\nggplot(rats_summarized, aes(x = borough, y = total)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\nSorting and ordering\n\nConsider sorting the locations by number of sightings instead of alphabetically\n\nIn the plot above, the boroughs on the x-axis are in alphabetic order. If we want to tell a better story, though, it’s helpful to reorder them so that we can more easily see which boroughs have the most and least rats. See here for more about reordering categories. We can sort the data and then use fct_inorder() from the {forcats} package (also one of the nine that gets loaded with library(tidyverse)) to lock these borough names in the right order:\n\nrats_summarized &lt;- rats_clean %&gt;% \n  group_by(borough) %&gt;% \n  summarize(total = n()) %&gt;% \n  # Sort by total in descending order\n  arrange(desc(total)) %&gt;% \n  # Lock bhe borough names in place\n  mutate(borough = fct_inorder(borough))\n\nggplot(rats_summarized, aes(x = borough, y = total)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\nUnbalanced facets\n\nThe facets are a little unbalanced given that there are 8 panels. Consider adding… something?… to that empty panel, like explanatory text or information about the data source. Or make it use 4 columns and 2 rows, or 2 columns and 4 rows so there’s no empty space\n\nLots of you used facets to show trends over time. By default R tries to make the grid as square as possible, so here there are 3 rows and 3 columns, but that leaves an empty panel in the bottom right corner.\n\nrats_by_year &lt;- rats_clean %&gt;% \n  group_by(borough, sighting_year) %&gt;% \n  summarize(total = n())\n## `summarise()` has grouped output by 'borough'. You can override using the\n## `.groups` argument.\n\nggplot(rats_by_year, aes(x = borough, y = total)) +\n  geom_col() +\n  facet_wrap(vars(sighting_year))\n\n\n\n\n\n\n\n\nFor the sake of balance, you can get rid of that panel by changing the layout. There are 8 panels here, so we could make a rectangle that’s 4 wide and 2 tall (or 2 wide and 4 tall if you want a tall rectangle instead) using the nrow or ncol arguments to facet_wrap():\n\n4 columns4 rows\n\n\n\nggplot(rats_by_year, aes(x = borough, y = total)) +\n  geom_col() +\n  facet_wrap(vars(sighting_year), ncol = 4)\n\n\n\n\n\n\n\n\n\n\n\nggplot(rats_by_year, aes(x = borough, y = total)) +\n  geom_col() +\n  facet_wrap(vars(sighting_year), nrow = 4)\n\n\n\n\n\n\n\n\n\n\n\nAlternatively you can stick something in that empty panel like your legend (though in this example it’s better to not even have a legend because it’s redundant with the x-axis). The reposition_legend() function from the {lemon} package makes this really easy:\n\nlibrary(lemon)\n\np &lt;- ggplot(rats_by_year, aes(x = borough, y = total, fill = borough)) +\n  geom_col() +\n  facet_wrap(vars(sighting_year)) +\n  guides(fill = guide_legend(ncol = 2, title.position = \"top\"))\n\nreposition_legend(p, position = \"bottom left\", panel = \"panel-3-3\")\n\n\n\n\n\n\n\n\nYou can even be fancy and add some explanatory text to that corner. It takes a big of extra work—you essentially have to create a fake text-only plot using grid::textGrob() and then use inset_element() from the {patchwork} to place it on top of the main plot:\n\nlibrary(grid)  # For making custom grid grobs\nlibrary(patchwork)\n## Warning: package 'patchwork' was built under R version 4.2.3\n\n# Make a little text-only plot\nextra_note &lt;- textGrob(\"Here's some text\\nabout rats. Neato.\", \n                       gp = gpar(fontface = \"bold\"))\n\n# Run this if you want to see it by itself:\n# grid.draw(extra_note)\n\np &lt;- ggplot(rats_by_year, aes(x = borough, y = total)) +\n  geom_col() +\n  facet_wrap(vars(sighting_year))\n\n# Add the text-only plot as an inset plot with patchwork\np + inset_element(extra_note, left = 0.7, bottom = 0.0, right = 1, top = 0.3)\n\n\n\n\n\n\n\n\n\n\nOverlapping text\n\nThe labels along the x-axis are unreadable and overlapping.\n\nThere are lots of ways to fix this—see this whole blog post for some different options. Here are some quick examples (none of these are fabulous, but they’re a start):\n\nSwap x- and y-axesRotate labelsDodge labels\n\n\n\nggplot(rats_by_year, aes(x = total, y = borough)) +\n  geom_col() +\n  facet_wrap(vars(sighting_year), ncol = 4)\n\n\n\n\n\n\n\n\n\n\n\nggplot(rats_by_year, aes(x = borough, y = total)) +\n  geom_col() +\n  facet_wrap(vars(sighting_year), ncol = 4) +\n  theme(axis.text.x = element_text(angle = 30, hjust = 0.5, vjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nggplot(rats_by_year, aes(x = borough, y = total)) +\n  geom_col() +\n  facet_wrap(vars(sighting_year), ncol = 4) +\n  scale_x_discrete(guide = guide_axis(n.dodge = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommas\n\nConsider adding automatic commas to the x-axis by including library(scales) and adding scale_x_continuous(labels = label_comma())\n\nYou can make nicer labels by formatting them with label_comma() (or any of the other label_*() functions) from the {scales} package. See here for a lot more details.\n\nlibrary(scales)\n\nggplot(rats_summarized, aes(x = borough, y = total)) +\n  geom_col() +\n  scale_y_continuous(labels = label_comma())"
  },
  {
    "objectID": "news/2023-10-12_slow-down-do-small-things.html",
    "href": "news/2023-10-12_slow-down-do-small-things.html",
    "title": "Slow down, simplify and do small things",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nNow that we’re at the middle of the course, your plots and data manipulation are becoming more detailed and complex, which is good! Remember exercise 1, so long ago? All you had to do was this:\n\nlibrary(tidyverse)\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +\n  geom_point()\n\nThat’s all! Literally 3 lines of code.\nBut now with your mini projects, and (in the future) interactivity, maps, text analysis, and your final project, your code is getting longer and more complex. You’ll have lots and lots of ggplot layers and functions chained together with %&gt;%. You’ve learned so much!\nIt is incredibly tempting to write out all the code you want in one go and then try to run a complete chunk and hope that you got it all correct. And then when it’s not correct, you try to change a bunch of things, hoping that they’ll fix it and then they don’t and you stay stuck and frustrated. You’ll have a chunk of code that was 20–30 lines with an error somewhere and won’t be able to find what went wrong or what was broken.\nDon’t do this!\nHere’s my best piece of advice for making more complex plots and for figuring out how to fix errors:\n\nSlow down, simplify, and do small things\n\nRun your code incrementally (see this past post here for some video examples about how to run stuff incrementally.). Start with a super basic plot and run it, then add a layer for labels and run it, then add a layer to change the fill gradient and run it, then add a layer to change the theme and run it, and so on. It feels slow, but it helps you understand what’s going on and helps you fix things when they break.\nThis is not just my advice. Julia Evans’s fantastic The Pocket Guide to Debugging has the same piece of advice:\n\n\n\nPage 39 from Julia Evans’s The Pocket Guide to Debugging\n\n\nWhen something doesn’t work as expected, change just one thing at a time. Or even better, simplify it and then change one thing at a time.\nHere’s a quick common example. Let’s say you have a plot like this and you want to use the plasma viridis scale for the colors of the points. It looks like it should work, but the colors aren’t right! Those are just the default colors!\n\nlibrary(tidyverse)\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  labs(x = \"Displacement\",\n       y = \"Highway MPG\",\n       color = \"Drive\") +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.9) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nHere’s the process I would go through to figure out what’s wrong and fix it:\n\n1. Strip it down to a basic plot2. Add simplified broken part3. Figure out broken part4. Add some parts back in5. Add the rest back in\n\n\nRight now there are a bunch of other layers (themes, labels, etc.). Maybe one of those is messing stuff up? We want to make sure the underlying plot works fine, so we’ll strip down the plot to its simplest form—just the geoms\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nGood, that works. Next we want to change the colors so that they use the viridis plasma palette. We used scale_fill_viridis_d() originally, but we also included a bunch of extra options (option = \"plamsa\", end = 0.9). Before using those, let’s simplify it down and just use the default settings:\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\n\n\nThe colors still didn’t change. But now we have a simplified working example of our broken code and we can examine it without worrying about the labels, themes, extra options, and all those other things. This should make it easier to see what’s going on.\nThe issue here is that we used the color aesthetic (color = drv) and we’re trying to change it with scale_fill_*(). That lets us control filled things (i.e. fill = drv). Since we’re working with the color aesthetic, we need to use scale_color_*(). Let’s try scale_color_viridis_d() and see if that fixes it:\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  scale_color_viridis_d()\n\n\n\n\n\n\n\n\n\n\nThat fixed it! It’s still not exactly what we wanted yet—we want the plasma palette and end = 0.9—but it’s working now and we can add that back in:\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  scale_color_viridis_d(option = \"plasma\", end = 0.9)\n\n\n\n\n\n\n\n\n\n\nCool, the palette changed and the other settings worked. The problem seems to be fixed now, so we can re-add all those other layers from the original plot. It’s fixed!\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  labs(x = \"Displacement\",\n       y = \"Highway MPG\",\n       color = \"Drive\") +\n  scale_color_viridis_d(option = \"plasma\", end = 0.9) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "news/2023-10-10_faqs-week-07.html",
    "href": "news/2023-10-10_faqs-week-07.html",
    "title": "Week 7 tips and FAQs",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nI just finished grading all your exercises from week 7 and am happy with how you’ve been doing! I have a few quick tips and tricks and suggestions here based on lots of the feedback I gave. Enjoy!\n\nI tried to knit my document and got an error about duplicate chunk labels. Why?\nYou can (and should!) name your R code chunks—see here for more about how and why. All chunk names must be unique, though.\nOften you’ll copy and paste a chunk from earlier in your document to later, like to make a second plot based on the first. That’s fine—just make sure that you change the chunk name.\nIf there are chunks with repeated names, R will yell at you:\n\n\n\nDuplicated chunk names\n\n\nTo fix it, change the name of one of the duplicated names to something unique:\n\n\n\nUnique chunk names\n\n\n\n\nI tried calculating something with sum() or cor() and R gave me NA instead of a number. Why?\nThis nearly always happens because of missing values. Let’s make a quick little dataset to illustrate what’s going on (and how to fix it):\n\nlibrary(tidyverse)\n\nexample &lt;- tibble(\n  x = c(1, 2, 3, 4, 5),\n  y = c(6, 7, NA, 9, 10),\n  z = c(2, 6, 5, 7, 3)\n)\n\nexample\n## # A tibble: 5 × 3\n##       x     y     z\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     1     6     2\n## 2     2     7     6\n## 3     3    NA     5\n## 4     4     9     7\n## 5     5    10     3\n\nThe y column has a missing value (NA), which will mess up any math we do.\nWithout running any code, what’s the average of the x column? We can find that with math (add all the numbers up and divide by how many numbers there are):\n\\[\n\\frac{1 + 2 + 3 + 4 + 5}{5} = 3\n\\]\nNeat. We can confirm with R:\n\n# With dplyr\nexample %&gt;% \n  summarize(avg = mean(x))\n## # A tibble: 1 × 1\n##     avg\n##   &lt;dbl&gt;\n## 1     3\n\n# With base R\nmean(example$x)\n## [1] 3\n\nWhat’s the average of the y column? Math time:\n\\[\n\\frac{6 + 7 + \\text{?} + 9 + 10}{5} = \\text{Who even knows}\n\\]\nWe have no way of knowing what the average is because of that missing value.\nIf we try it with R, it gives us NA instead of a number:\n\nexample %&gt;% \n  summarize(avg = mean(y))\n## # A tibble: 1 × 1\n##     avg\n##   &lt;dbl&gt;\n## 1    NA\n\nTo fix this, we can tell R to remove all the missing values from the column before calculating the average so that it does this:\n\\[\n\\frac{6 + 7 + 9 + 10}{4} = 8\n\\]\nInclude the argument na.rm = TRUE to do that:\n\nexample %&gt;% \n  summarize(avg = mean(y, na.rm = TRUE))\n## # A tibble: 1 × 1\n##     avg\n##   &lt;dbl&gt;\n## 1     8\n\nThis works for lots of R’s calculating functions, like sum(), min(), max(), sd(), median(), mean(), and so on:\n\nexample %&gt;% \n  summarize(\n    total = sum(y, na.rm = TRUE),\n    avg = mean(y, na.rm = TRUE),\n    median = median(y, na.rm = TRUE),\n    min = min(y, na.rm = TRUE),\n    max = max(y, na.rm = TRUE),\n    std_dev = sd(y, na.rm = TRUE)\n  )\n## # A tibble: 1 × 6\n##   total   avg median   min   max std_dev\n##   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n## 1    32     8      8     6    10    1.83\n\nThis works a little differently with cor() because you’re working with multiple columns instead of just one. If there are any missing values in any of the columns you’re correlating, you’ll get NA for the columns that use it. Here, we have a correlation between x and z because there are no missing values in either of those, but we get NA for the correlation between x and y and between z and y:\n\nexample %&gt;% \n  cor()\n##           x  y         z\n## x 1.0000000 NA 0.2287479\n## y        NA  1        NA\n## z 0.2287479 NA 1.0000000\n\nAdding na.rm to cor() doesn’t work because cor() doesn’t actually have an argument for na.rm:\n\nexample %&gt;%\n  cor(na.rm = TRUE)\n## Error in cor(., na.rm = TRUE): unused argument (na.rm = TRUE)\n\nInstead, if you look at the documentation for cor() (run ?cor in your R console or search for it in the Help panel in RStudio), you’ll see an argument named use instead. By default it will use all the rows in the data (use = \"everything\"), but we can change it to use = \"complete.obs\". This will remove all rows where something is missing before calculating the correlation:\n\nexample %&gt;%\n  cor(use = \"complete.obs\")\n##           x         y         z\n## x 1.0000000 1.0000000 0.2300895\n## y 1.0000000 1.0000000 0.2300895\n## z 0.2300895 0.2300895 1.0000000\n\n\n\nI want my bars to be sorted in my plot. How can I control their order?\nSorting categories by different values is important for showing trends in your data. By default, R will plot categorical variables in alphabetical order, but often you’ll want these categories to use some sort of numeric order, likely based on a different column.\nThere are a few different ways to sort categories. First, let’s make a summarized dataset of the total population in each continent in 2007 (using our trusty ol’ gapminder data):\n\nlibrary(gapminder)\n## Warning: package 'gapminder' was built under R version 4.2.3\n\n# Find the total population in each continent in 2007\npopulation_by_continent &lt;- gapminder %&gt;% \n  filter(year == 2007) %&gt;% \n  group_by(continent) %&gt;% \n  summarize(total_population = sum(pop))\npopulation_by_continent\n## # A tibble: 5 × 2\n##   continent total_population\n##   &lt;fct&gt;                &lt;dbl&gt;\n## 1 Africa           929539692\n## 2 Americas         898871184\n## 3 Asia            3811953827\n## 4 Europe           586098529\n## 5 Oceania           24549947\n\nBy default the continents will be in alphabetic order:\n\nggplot(\n  population_by_continent,\n  aes(x = continent, y = total_population, fill = continent)\n) +\n  geom_col() +\n  guides(fill = \"none\")  # The legend is redundant because of the x-axis\n\n\n\n\n\n\n\n\nIn this case it’s more useful to plot these in order of total population. My favorite approach for this is to (1) sort the data how I want it with arrange() and (2) lock the order of the category in place with fct_inorder(). Note how the mini dataset is now sorted and Oceania comes first:\n\nplot_data_sorted &lt;- population_by_continent %&gt;% \n  # Sort by population\n  arrange(total_population) %&gt;% \n  # Make continent use the order it's currently in\n  mutate(continent = fct_inorder(continent))\nplot_data_sorted\n## # A tibble: 5 × 2\n##   continent total_population\n##   &lt;fct&gt;                &lt;dbl&gt;\n## 1 Oceania           24549947\n## 2 Europe           586098529\n## 3 Americas         898871184\n## 4 Africa           929539692\n## 5 Asia            3811953827\n\nIf we plot it, the continents will be in order:\n\nggplot(\n  plot_data_sorted,\n  aes(x = continent, y = total_population, fill = continent)\n) +\n  geom_col() +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\nThis plots the continents in reverse order, with Oceania on the left. We can reverse this by either arranging the data in descending population order, or by using fct_rev() to reverse the continent order:\n\nSort in descending orderUse fct_rev()\n\n\n\nplot_data_sorted &lt;- population_by_continent %&gt;% \n  # Sort by population in descending order\n  arrange(desc(total_population)) %&gt;% \n  # Lock in the continent order\n  mutate(continent = fct_inorder(continent))\n\nggplot(\n  plot_data_sorted,\n  aes(x = continent, y = total_population, fill = continent)\n) +\n  geom_col() +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nplot_data_sorted &lt;- population_by_continent %&gt;% \n  # Sort by population in ascending order\n  arrange(total_population) %&gt;% \n  # Lock in the continent order\n  mutate(continent = fct_inorder(continent))\n\nggplot(\n  plot_data_sorted,\n  # Reverse the continent order with fct_rev()\n  aes(x = fct_rev(continent), y = total_population, fill = fct_rev(continent))\n) +\n  geom_col() +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nAn alternative to the two-step arrange() %&gt;% mutate(blah = fct_inorder(blah)) is to use fct_reorder(), which takes two arguments: (1) the column you want to be reordered and (2) the column you want to sort it by:\n\nplot_data_sorted &lt;- population_by_continent %&gt;% \n  # Sort continent by total_population in descending order\n  mutate(continent = fct_reorder(continent, total_population, .desc = TRUE))\n\nggplot(\n  plot_data_sorted,\n  aes(x = continent, y = total_population, fill = continent)\n) +\n  geom_col() +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\nThat’s only one line instead of two, which is nice, but I tend to be fan of the two step approach because it’s more explicit and gives me more control over sorting. For instance, here I want all the gapminder countries to be sorted by year (descending), continent, and life expectancy so we can see descending life expectancy within each continent over time.\nI’m sure there’s a way to sort by multiple columns in different orders like this with fct_reorder(), but I don’t know how. Plus, if I run this super_sorted_data code up until the end of arrange(), I can look at it in RStudio to make sure all the ordering I want is right. That’s harder to do with fct_reorder().\n\nsuper_sorted_data &lt;- gapminder %&gt;% \n  filter(year &gt;= 1997) %&gt;% \n  # Get the countries in order of year (descending), continent, and life expectancy\n  arrange(desc(year), continent, lifeExp) %&gt;%\n  # Lock the country name order in place + lock the year in place\n  mutate(\n    country = fct_inorder(country),\n    # year is currently a number, so we need to change it to a factor before\n    # reordering it\n    year = fct_inorder(factor(year))\n  )\n\nggplot(super_sorted_data, aes(y = country, x = lifeExp, fill = continent)) +\n  geom_col() +\n  facet_wrap(vars(year)) +\n  # Reverse the order legend so that Oceania is at the top, since it's at the\n  # top in the plot\n  guides(fill = guide_legend(reverse = TRUE)) +\n  theme_minimal() +\n  # Remove country names and y-axis gridlines + put legend on the bottom\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\nYou can also specify any arbitrary category order with fct_relevel()\n\nplot_data_sorted &lt;- population_by_continent %&gt;% \n  # Use this specific continent order\n  mutate(continent = fct_relevel(\n    continent, \n    c(\"Asia\", \"Oceania\", \"Europe\", \"Americas\", \"Africa\"))\n  )\n\nggplot(\n  plot_data_sorted,\n  aes(x = continent, y = total_population, fill = continent)\n) +\n  geom_col() +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\n\nMy data has full US state names but I want to use abbreviations (or regions). Is there a way to automatically convert from names to something else?\nYep! R has a few state-related variables built in (run ?state in your R console to see them all):\n\nstate.name\n##  [1] \"Alabama\"        \"Alaska\"         \"Arizona\"        \"Arkansas\"      \n##  [5] \"California\"     \"Colorado\"       \"Connecticut\"    \"Delaware\"      \n##  [9] \"Florida\"        \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n## [13] \"Illinois\"       \"Indiana\"        \"Iowa\"           \"Kansas\"        \n## [17] \"Kentucky\"       \"Louisiana\"      \"Maine\"          \"Maryland\"      \n## [21] \"Massachusetts\"  \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n## [25] \"Missouri\"       \"Montana\"        \"Nebraska\"       \"Nevada\"        \n## [29] \"New Hampshire\"  \"New Jersey\"     \"New Mexico\"     \"New York\"      \n## [33] \"North Carolina\" \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n## [37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"   \"South Carolina\"\n## [41] \"South Dakota\"   \"Tennessee\"      \"Texas\"          \"Utah\"          \n## [45] \"Vermont\"        \"Virginia\"       \"Washington\"     \"West Virginia\" \n## [49] \"Wisconsin\"      \"Wyoming\"\nstate.abb\n##  [1] \"AL\" \"AK\" \"AZ\" \"AR\" \"CA\" \"CO\" \"CT\" \"DE\" \"FL\" \"GA\" \"HI\" \"ID\" \"IL\" \"IN\" \"IA\"\n## [16] \"KS\" \"KY\" \"LA\" \"ME\" \"MD\" \"MA\" \"MI\" \"MN\" \"MS\" \"MO\" \"MT\" \"NE\" \"NV\" \"NH\" \"NJ\"\n## [31] \"NM\" \"NY\" \"NC\" \"ND\" \"OH\" \"OK\" \"OR\" \"PA\" \"RI\" \"SC\" \"SD\" \"TN\" \"TX\" \"UT\" \"VT\"\n## [46] \"VA\" \"WA\" \"WV\" \"WI\" \"WY\"\nstate.region\n##  [1] South         West          West          South         West         \n##  [6] West          Northeast     South         South         South        \n## [11] West          West          North Central North Central North Central\n## [16] North Central South         South         Northeast     South        \n## [21] Northeast     North Central North Central South         North Central\n## [26] West          North Central West          Northeast     Northeast    \n## [31] West          Northeast     South         North Central North Central\n## [36] South         West          Northeast     Northeast     South        \n## [41] North Central South         South         West          Northeast    \n## [46] South         West          South         North Central West         \n## Levels: Northeast South North Central West\n\nThese aren’t datasets—they’re single vectors—but you can make a little dataset with columns for each of those details, like this:\n\nstate_details &lt;- tibble(\n  state = state.name,\n  state_abb = state.abb,\n  state_division = state.division,\n  state_region = state.region\n) %&gt;% \n  # Add DC manually\n  add_row(\n    state = \"Washington, DC\",\n    state_abb = \"DC\",\n    state_division = \"South Atlantic\",\n    state_region = \"South\"\n  )\nstate_details\n## # A tibble: 51 × 4\n##    state       state_abb state_division     state_region\n##    &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;              &lt;chr&gt;       \n##  1 Alabama     AL        East South Central South       \n##  2 Alaska      AK        Pacific            West        \n##  3 Arizona     AZ        Mountain           West        \n##  4 Arkansas    AR        West South Central South       \n##  5 California  CA        Pacific            West        \n##  6 Colorado    CO        Mountain           West        \n##  7 Connecticut CT        New England        Northeast   \n##  8 Delaware    DE        South Atlantic     South       \n##  9 Florida     FL        South Atlantic     South       \n## 10 Georgia     GA        South Atlantic     South       \n## # ℹ 41 more rows\n\nYou can join this dataset to any data you have that has state names or state abbreviations. Joining the data will bring all the columns from state_details into your data wherever rows match. You’ll learn a lot more about joining things in sesison 12 too.\nFor instance, imagine you have a dataset that looks like this, similar to the unemployment data from exercise 8:\n\nsome_state_data &lt;- tribble(\n  ~state, ~something,\n  \"Wyoming\", 5,\n  \"North Carolina\", 9,\n  \"Nevada\", 10,\n  \"Georgia\", 3,\n  \"Rhode Island\", 1,\n  \"Washington, DC\", 6\n)\nsome_state_data\n## # A tibble: 6 × 2\n##   state          something\n##   &lt;chr&gt;              &lt;dbl&gt;\n## 1 Wyoming                5\n## 2 North Carolina         9\n## 3 Nevada                10\n## 4 Georgia                3\n## 5 Rhode Island           1\n## 6 Washington, DC         6\n\nWe can merge in (or join) the state_details data so that we add columns for abbreviation, region, and so on, using left_join() (again, see lesson 12 for more about all this):\n\n# Join the state details\ndata_with_state_details &lt;- some_state_data %&gt;% \n  left_join(state_details, by = join_by(state))\ndata_with_state_details\n## # A tibble: 6 × 5\n##   state          something state_abb state_division state_region\n##   &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;       \n## 1 Wyoming                5 WY        Mountain       West        \n## 2 North Carolina         9 NC        South Atlantic South       \n## 3 Nevada                10 NV        Mountain       West        \n## 4 Georgia                3 GA        South Atlantic South       \n## 5 Rhode Island           1 RI        New England    Northeast   \n## 6 Washington, DC         6 DC        South Atlantic South\n\nNow your data_with_state_details data has new columns for abbreviations, divisions, regions, and everything else that was in state_details:\n\n# Use it\nggplot(\n  data_with_state_details, \n  aes(x = state_abb, y = something, fill = state_region)\n) +\n  geom_col()\n\n\n\n\n\n\n\n\nAnd for fun, we can fix the ordering:\n\n# Fix the ordering\ndata_with_state_details &lt;- some_state_data %&gt;% \n  left_join(state_details, by = join_by(state)) %&gt;% \n  arrange(desc(something)) %&gt;% \n  mutate(state_abb = fct_inorder(state_abb))\n\nggplot(\n  data_with_state_details, \n  aes(x = state_abb, y = something, fill = state_region)\n) +\n  geom_col()"
  },
  {
    "objectID": "news/2023-10-03_faqs-week-06.html",
    "href": "news/2023-10-03_faqs-week-06.html",
    "title": "Week 6 FAQs",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nGreat work with exercise 6 this week! You’re starting to get the hang of ggplot and R!\nJust a couple quick common issues this week:\n\nMy histogram bars are too wide / too narrow / not visible. How do I fix that?\nIn exercise 6, a lot of you ran into issues with the GDP per capita histogram. The main issue was related to bin widths.\nHistograms work by taking a variable, cutting it up into smaller buckets, and counting how many rows appear in each bucket. For example, here’s a histogram of life expectancy from gapminder, with the binwidth argument set to 5:\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder_2007 &lt;- gapminder %&gt;% \n  filter(year == 2007)\n\nggplot(gapminder_2007, aes(x = lifeExp)) +\n  geom_histogram(binwidth = 5, color = \"white\", boundary = 0)\n\n\n\n\n\n\n\n\nThe binwidth = 5 setting means that each of those bars shows the count of countries with life expectancies in five-year buckets: 35–40, 40–45, 45–50, and so on.\nIf we change that to binwidth = 1, we get narrower bars because we have smaller buckets—each bar here shows the count of countries with life expectancies between 50–51, 51–52, 52–53, and so on.\n\nggplot(gapminder_2007, aes(x = lifeExp)) +\n  geom_histogram(binwidth = 1, color = \"white\", boundary = 0)\n\n\n\n\n\n\n\n\nIf we change it to binwidth = 20, we get huge bars because the buckets are huge. Now each bar shows the count of countries with life expectancies between 20–40, 40–60, 60–80, and 80–100:\n\nggplot(gapminder_2007, aes(x = lifeExp)) +\n  geom_histogram(binwidth = 20, color = \"white\", boundary = 0)\n\n\n\n\n\n\n\n\nThere is no one correct good universal value for the bin width and it depends entirely on your data.\nLots of you ran into an issue when copying/pasting code from the example, where one of the example histograms used binwidth = 1, since that was appropriate for that variable.\nWatch what happens if you plot a histogram of GDP per capita using binwidth = 1:\n\nggplot(gapminder_2007, aes(x = gdpPercap)) +\n  geom_histogram(binwidth = 1, color = \"white\", boundary = 0)\n\n\n\n\n\n\n\n\nhaha yeah that’s delightfully wrong. Each bar here is showing the count of countries with GDP per capita is $10,000–$10,001, then $10,001–$10.002, then $10,002–$10,003, and so on. Basically every country has its own unique GDP per capita, so the count for each of those super narrow bars is 1 (there’s one exception where two countries fall in the same bucket, which is why the y-axis goes up to 2). You can’t actually see any of the bars here because they’re too narrow—all you can really see is the white border around the bars.\nTo actually see what’s happening, you need a bigger bin width. How much bigger is up to you. With life expectancy we played around with 1, 5, and 20, but those bucket sizes are waaaay too small for GDP per capita. Try bigger values instead. But again, there’s no right number here!\n\nbinwidth = 1000binwidth = 2000binwidth = 5000binwidth = 10000\n\n\n\nggplot(gapminder_2007, aes(x = gdpPercap)) +\n  geom_histogram(binwidth = 1000, color = \"white\", boundary = 0)\n\n\n\n\n\n\n\n\n\n\n\nggplot(gapminder_2007, aes(x = gdpPercap)) +\n  geom_histogram(binwidth = 2000, color = \"white\", boundary = 0)\n\n\n\n\n\n\n\n\n\n\n\nggplot(gapminder_2007, aes(x = gdpPercap)) +\n  geom_histogram(binwidth = 5000, color = \"white\", boundary = 0)\n\n\n\n\n\n\n\n\n\n\n\nggplot(gapminder_2007, aes(x = gdpPercap)) +\n  geom_histogram(binwidth = 10000, color = \"white\", boundary = 0)\n\n\n\n\n\n\n\n\n\n\n\n\n\nI wrote some text and when I knit, it shows up in ugly monospaced font and it flows off the edge of the page? Why?\nYou’re used to indenting paragraphs in Word or Google Docs. First-line indentation is a normal thing with word processors.\nIndenting lines is unnecessary with Markdown and will mess up your text.\nFor example, let’s say you type something like this:\n    It was the best of times, it was the worst of times, it was the age of \nwisdom, it was the age of foolishness, it was the epoch of belief, it was the \nepoch of incredulity, it was the season of Light, it was the season of Darkness, \nit was the spring of hope, it was the winter of despair, we had everything \nbefore us, we had nothing before us, we were all going direct to Heaven, we were \nall going direct the other way—in short, the period was so far like the present \nperiod, that some of its noisiest authorities insisted on its being received, \nfor good or for evil, in the superlative degree of comparison only.\n\n    There were a king with a large jaw and a queen with a plain face, on the \nthrone of England; there were a king with a large jaw and a queen with a fair \nface, on the throne of France. In both countries it was clearer than crystal to \nthe lords of the State preserves of loaves and fishes, that things in general \nwere settled for ever. \nThat looks like Word-style text, with indented paragraphs. When you knit it, though, it’ll turn into code-formatted monospaced text that runs off the edge of the page:\n\nIt was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way—in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\n\nThere were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n\nThat’s because Markdown treats anything that is indented with four spaces as code, not as text.\nYou shouldn’t indent your text. Instead, add an empty line between each paragraph to separate them:\nIt was the best of times, it was the worst of times, it was the age of \nwisdom, it was the age of foolishness, it was the epoch of belief, it was the \nepoch of incredulity, it was the season of Light, it was the season of Darkness, \nit was the spring of hope, it was the winter of despair, we had everything \nbefore us, we had nothing before us, we were all going direct to Heaven, we were \nall going direct the other way—in short, the period was so far like the present \nperiod, that some of its noisiest authorities insisted on its being received, \nfor good or for evil, in the superlative degree of comparison only.\n\nThere were a king with a large jaw and a queen with a plain face, on the \nthrone of England; there were a king with a large jaw and a queen with a fair \nface, on the throne of France. In both countries it was clearer than crystal to \nthe lords of the State preserves of loaves and fishes, that things in general \nwere settled for ever. \n↑ that will turn into this ↓\n\nIt was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way—in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\nThere were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever."
  },
  {
    "objectID": "news/2023-09-27_faqs_week-05.html",
    "href": "news/2023-09-27_faqs_week-05.html",
    "title": "Week 5 FAQs",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nGreat work with your ugly plots last week! Hopefully it gave you good exposure to the power of ggplot themes. In the future, you’ll want to avoid such awful design sins and follow CRAP for real, but now you know how to make all sorts of adjustments to your plots in the future.\nThere are a few good and important FAQs for this week. Here we go!\n\nI made a bunch of changes to my plot with theme() but when I used ggsave(), none of them actually saved. Why?\nThis is a really common occurrence—don’t worry! And it’s easy to fix!\nIn the code I gave you in exercise 5, you stored the results of ggplot() as an object named ugly_plot, like this (this isn’t the same data as exercise 5, but shows the same general principle):\n\nugly_plot &lt;- ggplot(mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point()\n\nugly_plot\n\n\n\n\n\n\n\n\nThat ugly_plot object contains the basic underlying plot that you wanted to adjust. You then used it with {ggThemeAssist} to make modifications, something like this:\n\nugly_plot + \n  theme_dark(base_family = \"mono\") +\n  theme(\n    legend.position = c(0.5, 0.5),\n    legend.title = element_text(family = \"Comic Sans MS\", size = rel(3)),\n    panel.grid = element_line(color = \"purple\")\n  )\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n## not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n\n\n\n\n\n\n\nThat’s great and nice and ugly and it displays in your document just fine. If you then use ggsave() like this:\n\nggsave(\"my_ugly_plot.png\", ugly_plot)\n\n…you’ll see that it actually doesn’t save all the theme() changes. That’s because it’s saving the ugly_plot object, which is just the underlying base plot before adding theme changes.\nIf you want to keep the theme changes you make, you need to store them in an object, either overwriting the original ugly_plot object, or creating a new object:\n\nCreate new objectOverwrite ugly_plot\n\n\n\nugly_plot1 &lt;- ugly_plot + \n  theme_dark(base_family = \"mono\") +\n  theme(\n    legend.position = c(0.5, 0.5),\n    legend.title = element_text(family = \"Comic Sans MS\", size = rel(3)),\n    panel.grid = element_line(color = \"purple\")\n  )\n# Show the plot\nugly_plot1\n\n# Save the plot\nggsave(\"my_ugly_plot.png\", ugly_plot1)\n\n\n\n\nugly_plot &lt;- ugly_plot + \n  theme_dark(base_family = \"mono\") +\n  theme(\n    legend.position = c(0.5, 0.5),\n    legend.title = element_text(family = \"Comic Sans MS\", size = rel(3)),\n    panel.grid = element_line(color = \"purple\")\n  )\n# Show the plot\nugly_plot\n\n# Save the plot\nggsave(\"my_ugly_plot.png\", ugly_plot)\n\n\n\n\n\n\nIn chapter 22, Wilke talks about tables—is there a way to make pretty tables with R?\nAbsolutely! We don’t have time in this class to cover tables, but there’s a whole world of packages for making beautiful tables with R. Three of the more common ones are {gt}, {kableExtra}, and {flextable}:\n\n## Warning: package 'gt' was built under R version 4.2.3\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n  \n  \n    \n    \n      Package\n      \n        Output support\n      \n       \n      Notes\n    \n    \n      HTML\n      PDF\n      Word\n    \n  \n  \n    {gt}\n\nGreat\n\nOkay\n\nOkay\n\nExamples\n\nHas the goal of becoming the “grammar of tables” (hence “gt”). It is supported by developers at Posit and gets updated and improved regularly. It’ll likely become the main table-making package for R.\n\n    {kableExtra}\n\nGreat\n\nGreat\n\nOkay\n\nExamples\n\nWorks really well for HTML output and has the best support for PDF output, but development has stalled for the past couple years and it seems to maybe be abandoned, which is sad.\n\n    {flextable}\n\nGreat\n\nOkay\n\nGreat\n\nExamples\n\nWorks really well for HTML output and has the best support for Word output. It’s not abandoned and gets regular updates.\n\n  \n  \n  \n\n\n\n\nHere’s a quick illustration of these three packages. All three are incredibly powerful and let you do all sorts of really neat formatting things ({gt} even makes interactive HTML tables!), so make sure you check out the documentation and examples. I personally use all three, depending on which output I’m working with. When knitting to HTML, I use {gt}; when knitting to PDF I use {gt} or {kableExtra}; when knitting to Word I use {flextable}.\n\nDataset to table-ify{gt}{kableExtra}{flextable}\n\n\n\nlibrary(tidyverse)\n\ncars_summary &lt;- mpg %&gt;% \n  group_by(year, drv) %&gt;%\n  summarize(\n    n = n(),\n    avg_mpg = mean(hwy),\n    median_mpg = median(hwy),\n    min_mpg = min(hwy),\n    max_mpg = max(hwy)\n  )\n\n\n\n\nlibrary(gt)\n\ncars_summary %&gt;% \n  gt() %&gt;% \n  cols_label(\n    drv = \"Drive\",\n    n = \"N\",\n    avg_mpg = \"Average\",\n    median_mpg = \"Median\",\n    min_mpg = \"Minimum\",\n    max_mpg = \"Maximum\"\n  ) %&gt;% \n  tab_spanner(\n    label = \"Highway MPG\",\n    columns = c(avg_mpg, median_mpg, min_mpg, max_mpg)\n  ) %&gt;% \n  fmt_number(\n    columns = avg_mpg,\n    decimals = 2\n  ) %&gt;% \n  tab_options(\n    row_group.as_column = TRUE\n  )\n\n\n\n\n\n  \n    \n    \n      \n      Drive\n      N\n      \n        Highway MPG\n      \n    \n    \n      Average\n      Median\n      Minimum\n      Maximum\n    \n  \n  \n    1999\n4\n49\n18.84\n17\n15\n26\n    f\n57\n27.91\n26\n21\n44\n    r\n11\n20.64\n21\n16\n26\n    2008\n4\n54\n19.48\n19\n12\n28\n    f\n49\n28.45\n29\n17\n37\n    r\n14\n21.29\n21\n15\n26\n  \n  \n  \n\n\n\n\n\n\n\nlibrary(kableExtra)\n\ncars_summary %&gt;% \n  ungroup() %&gt;% \n  select(-year) %&gt;% \n  kbl(\n    col.names = c(\"Drive\", \"N\", \"Average\", \"Median\", \"Minimum\", \"Maximum\"),\n    digits = 2\n  ) %&gt;% \n  kable_styling() %&gt;% \n  pack_rows(\"1999\", 1, 3) %&gt;% \n  pack_rows(\"2008\", 4, 6) %&gt;% \n  add_header_above(c(\" \" = 2, \"Highway MPG\" = 4))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHighway MPG\n\n\n\nDrive\nN\nAverage\nMedian\nMinimum\nMaximum\n\n\n\n\n1999\n\n\n4\n49\n18.84\n17\n15\n26\n\n\nf\n57\n27.91\n26\n21\n44\n\n\nr\n11\n20.64\n21\n16\n26\n\n\n2008\n\n\n4\n54\n19.48\n19\n12\n28\n\n\nf\n49\n28.45\n29\n17\n37\n\n\nr\n14\n21.29\n21\n15\n26\n\n\n\n\n\n\n\n\n\n\nlibrary(flextable)\n## Warning: package 'flextable' was built under R version 4.2.3\n\ncars_summary %&gt;% \n  rename(\n    \"Year\" = year,\n    \"Drive\" = drv,\n    \"N\" = n,\n    \"Average\" = avg_mpg,\n    \"Median\" = median_mpg,\n    \"Minimum\" = min_mpg,\n    \"Maximum\" = max_mpg\n    ) %&gt;% \n  mutate(Year = as.character(Year)) %&gt;% \n  flextable() %&gt;% \n  colformat_double(j = \"Average\", digits = 2) %&gt;%\n  add_header_row(values = c(\" \", \"Highway MPG\"), colwidths = c(3, 4)) %&gt;% \n  align(i = 1, part = \"header\", align = \"center\") %&gt;% \n  merge_v(j = ~ Year) %&gt;% \n  valign(j = 1, valign = \"top\")\n\n\nFlextable example Highway MPGYearDriveNAverageMedianMinimumMaximum199944918.84171526f5727.91262144r1120.64211626200845419.48191228f4928.45291737r1421.29211526\n\n\n\n\n\nYou can also create more specialized tables for specific situations, like side-by-side regression results tables with {modelsummary} (which uses {gt}, {kableExtra}, or {flextable} behind the scenes)\n\nlibrary(modelsummary)\n## Warning: package 'modelsummary' was built under R version 4.2.3\n\nmodel1 &lt;- lm(hwy ~ displ, data = mpg)\nmodel2 &lt;- lm(hwy ~ displ + drv, data = mpg)\n\nmodelsummary(\n  list(model1, model2),\n  stars = TRUE,\n  # Rename the coefficients\n  coef_rename = c(\n    \"(Intercept)\" = \"Intercept\",\n    \"displ\" = \"Displacement\",\n    \"drvf\" = \"Drive (front)\",\n    \"drvr\" = \"Drive (rear)\"),\n  # Get rid of some of the extra goodness-of-fit statistics\n  gof_omit = \"IC|RMSE|F|Log\",\n  # Use {gt}\n  output = \"gt\"\n)\n\n\n\n\n\n  \n    \n    \n       \n      (1)\n      (2)\n    \n  \n  \n    Intercept\n35.698***\n30.825***\n    \n(0.720)\n(0.924)\n    Displacement\n-3.531***\n-2.914***\n    \n(0.195)\n(0.218)\n    Drive (front)\n\n4.791***\n    \n\n(0.530)\n    Drive (rear)\n\n5.258***\n    \n\n(0.734)\n    Num.Obs.\n234\n234\n    R2\n0.587\n0.736\n    R2 Adj.\n0.585\n0.732\n  \n  \n    \n      + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n    \n  \n  \n\n\n\n\n\n\nDouble encoding and excessive legends\nAs you’ve read, double encoding aesthetics can be helpful for accessibility and printing reasons—for instance, if points have colors and shapes, they’re still readable by people who are colorblind or if the image is printed in black and white:\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv, shape = drv)) +\n  geom_point()\n\n\n\n\n\n\n\n\nSometimes the double encoding can be excessive though, and you can safely remove legends. For example, in exercises 3 and 4, you made bar charts showing counts of different things (words spoken in The Lord of the Rings; pandemic-era construction projects in New York City), and lots of you colored the bars, which is great!\n\ncar_counts &lt;- mpg %&gt;% \n  group_by(drv) %&gt;% \n  summarize(n = n())\n\nggplot(car_counts, aes(x = drv, y = n, fill = drv)) +\n  geom_col()\n\n\n\n\n\n\n\n\nCar drive here is double encoded: it’s on the x-axis and it’s the fill. That’s great, but having the legend here is actually a little excessive. Both the x-axis and the legend tell us what the different colors of drives are (four-, front-, and rear-wheeled drives), so we can safely remove the legend and get a little more space in the plot area:\n\nggplot(car_counts, aes(x = drv, y = n, fill = drv)) +\n  geom_col() +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\n\nLegends are cool, but I’ve read that directly labeling things can be better. Is there a way to label things without a legend?\nYes! Later in the semester we’ll cover annotations, but in the meantime, you can check out a couple packages that let you directly label geoms that have been mapped to aesthetics.\n\n{geomtextpath}\nThe {geomtextpath} package lets you add labels directly to paths and lines with functions like geom_textline() and geom_labelline() and geom_labelsmooth().\nLike, here’s the relationship between penguin bill lengths and penguin weights across three different species:\n\n# This isn't on CRAN, so you need to install it by running this:\n# remotes::install_github(\"AllanCameron/geomtextpath\")\nlibrary(geomtextpath)\n## Warning: package 'geomtextpath' was built under R version 4.2.3\nlibrary(palmerpenguins)  # Penguin data\n## Warning: package 'palmerpenguins' was built under R version 4.2.3\n\n# Get rid of the rows that are missing sex\npenguins &lt;- penguins %&gt;% drop_na(sex)\n\nggplot(\n  penguins, \n  aes(x = bill_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point(alpha = 0.5) +  # Make the points a little bit transparent\n  geom_labelsmooth(\n    aes(label = species), \n    # This spreads the letters out a bit\n    text_smoothing = 80\n  ) +\n  # Turn off the legend bc we don't need it now\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\nAnd the average continent-level life expectancy across time:\n\nlibrary(gapminder)\n## Warning: package 'gapminder' was built under R version 4.2.3\n\ngapminder_lifeexp &lt;- gapminder %&gt;% \n  group_by(continent, year) %&gt;% \n  summarize(avg_lifeexp = mean(lifeExp))\n\nggplot(\n  gapminder_lifeexp, \n  aes(x = year, y = avg_lifeexp, color = continent)\n) +\n  geom_textline(\n    aes(label = continent, hjust = continent),\n    linewidth = 1, size = 4\n  ) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\n\n\n{ggdirectlabel}\nA new package named {ggdirectlabel} lets you add legends directly to your plot area:\n\n# This also isn't on CRAN, so you need to install it by running this:\n# remotes::install_github(\"MattCowgill/ggdirectlabel\")\nlibrary(ggdirectlabel)\n\nggplot(\n  penguins, \n  aes(x = bill_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point(alpha = 0.5) +\n  geom_smooth() +\n  geom_richlegend(\n    aes(label = species),  # Use the species as the fake legend labels\n    legend.position = \"topleft\",  # Put it in the top left\n    hjust = 0  # Make the text left-aligned (horizontal adjustment, or hjust)\n  ) +\n  guides(color = \"none\")"
  },
  {
    "objectID": "news/2023-09-12_general-tips.html",
    "href": "news/2023-09-12_general-tips.html",
    "title": "A few quick general R tips",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nAs I’ve been looking through your exercises, I’ve noticed a few little R issues that might sometimes be tripping you up. They’re super minor, but can make life easier:\n\nTalking about packages and functions\nYou’ve probably noticed that on the course website here, I put package names in {}s, like {ggplot2} or {gghalves}. This is a normal convention in the R world—people generally either put package names in braces or in bold.\nWhen writing about functions, I typically format them as code, followed by empty open and closed parentheses, like geom_point(). That signals that it’s actual runnable code and not the name of a package. Remember that you can format things like inline code by using single backticks.\nFor instance, this:\nYou can use `geom_point()` from {ggplot2} to make scatterplots.\nwill knit into this:\n\nYou can use geom_point() from {ggplot2} to make scatterplots.\n\n\n\nNicer {ggplot2} documentation\nThe ggplot documentation within R (i.e. in the help panel) is good, but I find that it’s nicer to use the documentation website. It’s the same exact content, but the website version shows plots for each of the examples. Scroll down to the examples section for geom_point(), for instance.\n\n\nInstalling vs. using packages\nMake sure you don’t include code to install packages in your Rmd files. Like, don’t include install.packages(\"gapminder\") or whatever. If you do, R will reinstall that package every time you knit your document, which is excessive. All you need to do is load the package with library().\n\n\n\n\n\n\nInstalling and using packages\n\n\n\nYou install packages once on your computer. You can do this either with install.packages() or by clicking on “Install package” in the “Packages” panel in RStudio:\n# Only do this once on your computer! Don't include this line of code in a document\ninstall.packages(\"gapminder\")\nYou load packages every time you want to use functions or data from that package in your R session.:\n# This loads the gapminder package and lets you use it\nlibrary(gapminder)\n\n\n\n\nThe tidyverse package shortcut\nWhen you run library(tidyverse), R shows a message that says it has loaded 9 packages:\n── Attaching core tidyverse packages ────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \nThe tidyverse package developers have found that those 9 are some of the most common packages that people use, so they created library(tidyverse) as a shortcut for loading them all at the same time. Alternatively, you could start your documents like this:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(forcats)\n# etc.\nBut that’s a lot of typing. It’s easier to just do library(tidyverse)\nIf you load the tidyverse package, you don’t need to load those 9 individual packages. Doing this is entirely redundant:\nlibrary(tidyverse)\nlibrary(ggplot2)  # This is alreaady loaded bc tidyverse\nlibrary(dplyr)  # This is also already loaded bc tidyverse\nI’ve tried to point out in your exercises if/when you do this\n\n\nChunk labels\nLabeling your R chunks is a good thing to do, since it helps with document navigation and is generally good practice. If you’re using chunk labels make sure you don’t use spaces in them. R will still knit a document with spaceful names, but it converts the spaces to underscores before doing it. So instead of naming chunks like {r My Neat Chunk, message=FALSE}, use something like {r my-neat-chunk} or {r my_neat_chunk}.\n\n\nCode style\nUnlike other programming languages (grumbles at Python), R is fairly forgiving with the style of the code you write. You can have extra spaces, you can omit spaces, you can indent things however you want, etc.\nBut if you want to write readable code (you do!) and write code that others can work with (you do!), you should follow some basic style guidelines. I’ve summarized a few of the most important ones here.\nI’m never going to grade you on any of this, by the way! These are just a set of best practices that you should get into the habit of using."
  },
  {
    "objectID": "news/2023-09-12_deadlines-late-work.html",
    "href": "news/2023-09-12_deadlines-late-work.html",
    "title": "Deadlines, late work, and student hours",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nOne quick reminder about my late work policy, since I’ve been getting a lot of Slack messages and e-mails about it.\nRecall from the syllabus that with the exception of the mini projects and final project, there are no actual hard and fast deadlines in this class. If you need to turn an exercise in late, cool. I might not be able to get you feedback if stuff comes in too late, but you’ll still get full credit. There’s no penalty for late work.\nAlso, remember from the syllabus that I have really open student hours. If you want to meet with me for help or for whatever reason, sign up for a time at my Calendly page. You can also access that link at the home page of the course website and at the top of the syllabus page"
  },
  {
    "objectID": "news/2023-09-05_faqs_week-02.html",
    "href": "news/2023-09-05_faqs_week-02.html",
    "title": "Week 2 FAQs",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nSince exercise 2 involved no code, I don’t have much in the way of code tips or anything :)\nIn the comments for exercise 2, I went through the CRAP acronym and pointed out little things that can be changed or improved for each of your designs. You don’t have to redo your posters. I gave that feedback to show what the CRAP design tinkering process looks like in real life. For instance, a few of you had had text and graphics that were only a couple pixels misaligned—just tweaking that to make everything perfectly inline does a lot to make designs feel more put together.\nAs I mentioned in the lecture, once you know about CRAP, you’ll start using it everywhere—every billboard, poster, book, website, etc. you see, you’ll try to identify what the designed tried to align each element with, which fonts they used, why they used the colors they did, why they grouped the text and images the way they did, and so on. Or when you come across something that you intuitively feel is ugly and poorly designed, you’ll be able to say why.\nIf you want to know more about basic design skills like this, I’d highly recommend getting The Non-Designer’s Design and Type Book. It’s a fantastic resource and goes way more in depth into CRAP and basic typography."
  },
  {
    "objectID": "news/2023-08-21_reaching-out-when-stuck.html",
    "href": "news/2023-08-21_reaching-out-when-stuck.html",
    "title": "The 30-minute rule and how to ask for help",
    "section": "",
    "text": "← News\n\n\n\nWhen working through the examples or exercises, you will get stuck. It’s inevitable.\nYou absolutely can (and should) reach out when stuck. I’m super responsive on Slack and e-mail. If you ask a question on the #help channel in Slack, others can help too. No question is too tricky or embarrassing, I promise. Computers are extraordinarily literal and tiny typos will often mess you up for a long time—I’ve lost countless time because of missing commas and misspelled words (curse you lenght()). Please feel comfortable reaching out.\nI’m a big believer in the 30-minute rule:\n\n\n\n\n\n\nThe 30-minute rule\n\n\n\nIf you’re stuck on an issue for more than 30 minutes, stop, take a break, and ask for help somewhere.\n\n\nThere are a few easy guidelines to remember when asking for help:\n\nBe kind.\nAsk questions with as complete information as possible. Do not just post a photo from your phone and say something like “help?” or “my code isn’t working” and that’s it—provide more background (it’s hard to read your computer’s mind).\nExplain specifically what you’re trying to do and provide code when possible. You can actually format R code on Slack if you click on the little lightning icon in the bottom left of the typing area and search for “text snippet”—that’ll open a dialog that will let you paste in text and add R syntax highlighting. You can also paste your code between triple backticks on Slack and it’ll format it in monospaced font (though not with the neat syntax highlighting that you get when using Slack’s text snippet thing):\n```\nggplot(blah) + geom_point()\n```\nYou can also take screenshots to supplement your question (use ⌘+shift+4 on macOS to save a screenshot to your Desktop, or ⌘+⌥+shift+4 to save a screenshot directly to the clipboard; use Windows+shift+S on Windows to save a screenshot directly to the clipboard). Do not send a screenshot as your entire question though (it’s very tempting to just post a picture of an error and hope that someone can fix it)—provide context. Screenshots are helpful, but code/text is better—it’s harder for people to get the code out of a screenshot in Slack and into RStudio on their computer to troubleshoot, since you can’t copy/paste from an image.\nTry making your question a reproducible example (or reprex). You’ll learn more about reprexes in session 8, but it’s an incredibly important skill to have. See here for some resources about it\nAsk in public. It is very tempting to send me private messages on Slack for assistance, and that’s great and I’m happy to respond like that. However, one way to help build a stronger community for this class is to ask questions in public in the different #help channels instead. There are a couple reasons for this:\n\nIt reduces duplication: Many of you will have almost identical questions and I often end up copying and pasting my answers between different private conversations asking for help. Having those questions and answers in the public #help channels instead will let you get answers to common questions faster\nIt allows you to help: Some of you have R experience already, and even if you don’t, as the semester goes on, you’ll get more comfortable with it and will start being able to answer your classmates’ questions. You might have just fixed a similar issue in a past exercise, or you might be able to spot a typo in their code, or you might otherwise know how to help. Step in and help! Slack is for building a community, not just for getting assistance from me.\n\n\nAnd that’s it. Ask questions in ways that will help answerers answer them and be nice about it. When answering questions, be nice about it. Ask lots of questions. Answer lots of questions. Do it all in public.\nOnce again, do not suffer in silence. I’ve had past students tell me that’s like the one thing they’ll remember from my classes—do not suffer in silence. I mean it, and I’ll keep saying it throughout the semester (because often in your past courses and degrees, you’ve been discouraged from reaching out or from building communities or whatever—that is not the case here).\nRemember that you can always sign up for a time to meet with me at my Calendly page! I’m also on Slack and accessible via e-mail!"
  },
  {
    "objectID": "news/2023-08-21_faqs_week-00.html",
    "href": "news/2023-08-21_faqs_week-00.html",
    "title": "Pre-class FAQs",
    "section": "",
    "text": "← News\n\n\n\nThanks to everyone who has filled out the pre-class welcome survey (see the link in your e-mail)! I’m excited to get to know you all more this semester!\nLots of you had similar questions and concerns, so I’ve consolidated the common ones here in a list of FAQs.\n\nCan I reach out if I have questions?\nABSOLUTELY. DO NOT ATTEMPT TO DO THIS CLASS WITHOUT EVER ASKING A SINGLE QUESTION.\nMake sure you read this about building a community and this about the 30-minute rule and how to ask for help.\n\n\nWill we cover {X}?\n\nGeospatial data and visualizations? Absolutely! Maps are one of my favorite kinds of visualization and we have a whole session on them.\nInteractive visualizations? Yep!\nAnimated visualizations? Not directly, but we’ll briefly mention the {gganimate} package, which is surprisingly easy to use—it’s just regular ggplot code with a few extra lines to animate it\n\n\n\nWill you have regular office hours?\nI don’t have a set time every week for student hours (see here for why I don’t call them office hours). Instead, I use an appointment system. If you want to meet with me online, visit my Calendly page and sign up for a slot. There’s also a link to it on the homepage and syllabus page of the class website.\nI’ll also generally be on campus on Mondays if you want to meet IRL. Use the in-person slot at Calendly to sign up for a time to meet me on campus.\n\n\nI don’t have a background in anything related to PMAP (public management and policy)—will I be lost?\nThat’s 1000% fine! I’ve designed this class to be accessible and useful for people working with any kind of data. Lots of the examples we’ll play with will often be related to political science-y things, but not always. And the kind of data doesn’t actually matter. When learning new statistical programming and visualization tools and techniques, the industry standard is to work with toy datasets. For instance, in your first exercise you’ll make a plot of car mileage. I know nothing about cars at all, but I (along with everyone else in the R world) use that dataset all the time for quick little tests of plots or models or code or whatever, since it’s built into R. I also often use data about penguins and lots of other domains in assignments and examples.\nDon’t worry about the precise nature of the data. In your final project, you’ll make visualizations with whatever data you want. For all the other assignments and exercises and mini-projects, just go with the data I’ve provided. It’s all actual real world data (refugee inflows, unemployment, election results, texts of books from the 19th century, counts and locations of rat sightings in NYC, etc.). If it’s not related to your field, don’t worry—the principles of code and data visualization and graphic design still apply."
  },
  {
    "objectID": "news/2023-08-21_building-community.html",
    "href": "news/2023-08-21_building-community.html",
    "title": "Building community",
    "section": "",
    "text": "← News\n\n\n\nThe most common question from the pre-class survey was some variation of this:\n\nI have no background in programming or in R—will I be able to learn things and succeed in this class?\n\nAbsolutely.\nI’ve designed this class to be accessible by people with any level of R experience. If you’ve never used R—awesome! If you’ve been using R for years—awesome! You’ll all learn new skills.\nTo get the most out of this class and minimize the inevitable frustration that comes with learning a programming language, do not try to do everything on your own. Community is essential.\n\nClass community\nThroughout these pandemic years I’ve been studying the research on online teaching to see how to make my online classes as successful as possible. One almost universal key to success I’ve found is that learning requires social interaction to be successful (Glazier 2016). In fact, one of the greatest predictors of success is whether students build relationships with their classmates and their professor.\nThis is true for both online classes and face-to-face classes! It’s entirely possible to take a face-to-face class and have almost no social interaction. The worst, most useless class I ever took as an undergraduate was a required Intro to US History class that met twice a week for 2 hours in a massive auditorium with 900 (!!!) other students. It was awful and I learned nothing.\nSo, interact with me and your classmates! Don’t lurk in silence in the shadows. Do reach out and talk to your classmates (and me!).\nTo that end, one of the best ways to get to know each other is to help each other. Ask questions on Slack. If you see someone’s question there and you know how to help, answer!\nYou can also work on exercises and projects in groups if you want. Use Webex or Zoom to share screens with each other during calls so you can see what you’re doing. In the real programming world, a common technique for working on code is pair programming, where two people work on the same script. If we were meeting in class, we’d do this face-to-face, working during class in groups of 2–3 with me walking around and helping. Feel free to replicate that experience and work with others. It’s not cheating! (As long as you don’t turn in identical work)\n\n\nOnline community\nOne of the great parts about the online R community is that the team at Posit has actually hired people to help promote community norms. Like that’s their whole job—community evangelist. The R world is probably one of the nicest corners of the programming internet because of these norms—there’s an emphasis on helping beginners, being kind and respectful, and cheerfully helping as much as possible. Watch the #rstats hashtag on Twitter and Mastodon, look at the “r” tag on StackOverflow, and look at discussions at the Posit Community and you’ll see kindness in action.\nThis was not always the case. 10ish years ago, before the RStudio people made a concerted effort to create community, the online R world was pretty mean and toxic, with forums run by a few really grumpy statisticians who’d belittle you if you asked a poorly worded question. It was awful. That kind of attitude often still persists in other languages (hooo boy try asking a beginner question about Python at StackOverflow 😬), but the R world has tried really hard to be welcoming.\nI embrace that attitude when teaching R, and I encourage you all to do the same. Reach out for help early and often. Do not suffer in silence. Do not spend hours and hours stuck on an issue before reaching out for help.\n\nHelp make this semester fantastic by talking, reaching out, and otherwise socially interacting!\n\n\n\n\n\nReferences\n\nGlazier, Rebecca A. 2016. “Building Rapport to Improve Retention and Success in Online Classes.” Journal of Political Science Education 12 (4): 437–56. https://doi.org/10.1080/15512169.2016.1155994."
  },
  {
    "objectID": "news/2023-08-18_important-r-stuff.html",
    "href": "news/2023-08-18_important-r-stuff.html",
    "title": "Important R stuff",
    "section": "",
    "text": "← News\n\n\n\nFrom reading the syllabus you’ll notice that we’re using R in this class (though you can certainly use Stata too if you really want). R is free (!!), and highly sought after by employers, but it has a little bit of an initial learning curve. You’ll get the hang of it, though, I promise! I’m a certified RStudio instructor (see https://education.rstudio.com/trainers/), so I’ve been specially trained to teach statistical programming. Again, I promise that you’ll learn a ton.\nWhile R and RStudio are free and open source, they can sometimes be tricky to install. To make life simpler, you can run RStudio in your browser without having to install anything (!!!) using a service called Posit.cloud. Over the course of the semester you’ll want to eventually install everything on your actual computer, since it’ll run faster there, but you won’t need to install anything to get started. I’ll e-mail you a link to join the class workspace.\nIn the meantime, you can also install everything on your actual computer if you follow these instructions (eventually you should do this—it’s best to run R on your own computer, but it’s totally fine to run everything at Posit.cloud for the first few weeks)\nOnce again, I promise that you can do this and have a blast in this class!"
  },
  {
    "objectID": "lesson/index.html",
    "href": "lesson/index.html",
    "title": "Interactive lessons",
    "section": "",
    "text": "Each class session has an interactive lesson that you will work through after doing the readings and watching the lecture. These lessons are a central part of the class—they will teach you how to use {ggplot2} and other packages in the tidyverse to create beautiful and truthful visualizations with R.\nInteractive code sections look like this. Make changes in the text box and click on the green “Run Code” button to see the results. Sometimes there will be a button with a hint or solution.\n\n\n\n\n\n\nYour turn\n\n\n\nModify the code here to show the relationship between health and wealth for 2002 instead of 2007.\n\n\n\n\nIf you’re curious how this works, each interactive code section is a miniature Shiny app hosted at shinyapps.io. Each app uses {learnr} to provide interactivity, and these {learnr} apps are embedded in this website with some HTML and Javascript wizardry."
  },
  {
    "objectID": "lesson/14-lesson.html",
    "href": "lesson/14-lesson.html",
    "title": "Enhancing graphics",
    "section": "",
    "text": "There’s no lesson for this session. In your exercise this week you’ll export a plot from ggplot, open it in a vector editing pregram, and make it extra pretty and well-designed. The best way to learn this is by actually doing it.\nSo head over to the example to see how it’s done, or the exercise to get started!"
  },
  {
    "objectID": "lesson/12-lesson.html",
    "href": "lesson/12-lesson.html",
    "title": "Space",
    "section": "",
    "text": "There is a short lesson this week! You’ll learn the basics of joining two different datasets together, both vertically and horizontally.\nThere are a few imaginary datasets I’ve created for you to play with:\nx\n## # A tibble: 3 × 2\n##      id some_variable\n##   &lt;dbl&gt; &lt;chr&gt;        \n## 1     1 x1           \n## 2     2 x2           \n## 3     3 x3\ny\n## # A tibble: 3 × 2\n##      id some_other_variable\n##   &lt;dbl&gt; &lt;chr&gt;              \n## 1     1 y1                 \n## 2     2 y2                 \n## 3     4 y4\nnational_data\n## # A tibble: 9 × 5\n##   state  year unemployment inflation population\n##   &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n## 1 GA     2018          5         2          100\n## 2 GA     2019          5.3       1.8        200\n## 3 GA     2020          5.2       2.5        300\n## 4 NC     2018          6.1       1.8        350\n## 5 NC     2019          5.9       1.6        375\n## 6 NC     2020          5.3       1.8        400\n## 7 CO     2018          4.7       2.7        200\n## 8 CO     2019          4.4       2.6        300\n## 9 CO     2020          5.1       2.5        400\nnational_data_2019\n## # A tibble: 3 × 4\n##   state unemployment inflation population\n##   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n## 1 GA             5.3       1.8        200\n## 2 NC             5.9       1.6        375\n## 3 CO             4.4       2.6        300\nnational_libraries\n## # A tibble: 6 × 4\n##   state  year libraries schools\n##   &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 CO     2018       230     470\n## 2 CO     2019       240     440\n## 3 CO     2020       270     510\n## 4 NC     2018       200     610\n## 5 NC     2019       210     590\n## 6 NC     2020       220     530\nnational_libraries_2019\n## # A tibble: 2 × 3\n##   state libraries schools\n##   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 CO          240     440\n## 2 NC          210     590\npuerto_rico_data\n## # A tibble: 3 × 4\n##   state unemployment population  year\n##   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n## 1 PR             3.1        150  2018\n## 2 PR             3.2        250  2019\n## 3 PR             3.3        350  2020\nstate_regions\n## # A tibble: 51 × 2\n##    region    state\n##    &lt;chr&gt;     &lt;chr&gt;\n##  1 West      AK   \n##  2 South     AL   \n##  3 South     AR   \n##  4 West      AZ   \n##  5 West      CA   \n##  6 West      CO   \n##  7 Northeast CT   \n##  8 South     DC   \n##  9 South     DE   \n## 10 South     FL   \n## # ℹ 41 more rows"
  },
  {
    "objectID": "lesson/12-lesson.html#combining-datasets-vertically",
    "href": "lesson/12-lesson.html#combining-datasets-vertically",
    "title": "Space",
    "section": "Combining datasets vertically",
    "text": "Combining datasets vertically\nRecall from the Lord of the Rings data in exercise 3 that you had to combine three different CSV files into dataset. You used bind_rows() to stack each of these on top of each other.\n\nlotr &lt;- bind_rows(fellowship, tt, rotk)\n\nThat worked well because each of the individual data frames had the same columns in them, and R was able to line up the matching columns. If columns were missing, R would have placed NA in the appropriate locations.\n\n\n\n\n\n\nYour turn\n\n\n\nCombine national_data and puerto_rico_data into a single dataset named us_data using bind_rows. Pay attention to what happens with the inflation column. Also notice that the columns in the Puerto Rico data are in a different order."
  },
  {
    "objectID": "lesson/12-lesson.html#combining-datasets-horizontally",
    "href": "lesson/12-lesson.html#combining-datasets-horizontally",
    "title": "Space",
    "section": "Combining datasets horizontally",
    "text": "Combining datasets horizontally\nBinding rows vertically is the easiest way to combine two datasets, but most often you won’t be doing that. You’ll only do this if you’re combining datasets that come from the same source, like if a state offers separate CSV files of the same data for each county.\nIn most cases, though, you’ll need to combine completely different datasets, bringing one or more columns from one into another. With vertical combining, R needs column names with the same names in order to figure out where the data lines up. With horizontal combining, R needs values inside one or more columns to be the same in order to figure out where the data lines up.\nThere is technically a function named bind_cols(), but you’ll rarely want to use it. It doesn’t attempt to match any rows—it just glues two datasets together:\n\nbind_cols(national_data, \n          # Repeat PR 3 times so that it has the same number of rows as national_data\n          bind_rows(puerto_rico_data, puerto_rico_data, puerto_rico_data))\n## New names:\n## • `state` -&gt; `state...1`\n## • `year` -&gt; `year...2`\n## • `unemployment` -&gt; `unemployment...3`\n## • `population` -&gt; `population...5`\n## • `state` -&gt; `state...6`\n## • `unemployment` -&gt; `unemployment...7`\n## • `population` -&gt; `population...8`\n## • `year` -&gt; `year...9`\n## # A tibble: 9 × 9\n##   state...1 year...2 unemployment...3 inflation population...5 state...6\n##   &lt;chr&gt;        &lt;dbl&gt;            &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;    \n## 1 GA            2018              5         2              100 PR       \n## 2 GA            2019              5.3       1.8            200 PR       \n## 3 GA            2020              5.2       2.5            300 PR       \n## 4 NC            2018              6.1       1.8            350 PR       \n## 5 NC            2019              5.9       1.6            375 PR       \n## 6 NC            2020              5.3       1.8            400 PR       \n## 7 CO            2018              4.7       2.7            200 PR       \n## 8 CO            2019              4.4       2.6            300 PR       \n## 9 CO            2020              5.1       2.5            400 PR       \n## # ℹ 3 more variables: unemployment...7 &lt;dbl&gt;, population...8 &lt;dbl&gt;,\n## #   year...9 &lt;dbl&gt;\n\nThat’s… not great.\nInstead, we need to use a function that is more careful about bringing in data. Fortunately there are a few good options:\n\ninner_join()\nleft_join()\nright_join()\n\nThe most helpful way of understanding these different functions is to go here and stare at the animations for a little while to see which pieces of which dataset go where. (There are lots of others, like full_join(), semi_join(), and anti_join(), and they have helpful animations, but I rarely use those.)\nFor each of these functions, you need at least one common ID column in both datasets in order for R to know where things line up.\nLet’s practice how these all work and see what the differences between them are."
  },
  {
    "objectID": "lesson/12-lesson.html#inner_join",
    "href": "lesson/12-lesson.html#inner_join",
    "title": "Space",
    "section": "inner_join()",
    "text": "inner_join()\nFirst, go to this page in a new tab and stare at the mesmerizing animation.\nLet’s look at two datasets, x and y:\n\nx\n## # A tibble: 3 × 2\n##      id some_variable\n##   &lt;dbl&gt; &lt;chr&gt;        \n## 1     1 x1           \n## 2     2 x2           \n## 3     3 x3\n\n\ny\n## # A tibble: 3 × 2\n##      id some_other_variable\n##   &lt;dbl&gt; &lt;chr&gt;              \n## 1     1 y1                 \n## 2     2 y2                 \n## 3     4 y4\n\nBoth datasets have an id column that is the same across each (though the values aren’t necessarily the same). Because there’s a shared column, we can join these two based on that column.\nIf we use inner_join(), the resulting dataset will only keep the rows from the first where there are matching values from the second:\n\ninner_join(x, y, by = \"id\")\n## # A tibble: 2 × 3\n##      id some_variable some_other_variable\n##   &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;              \n## 1     1 x1            y1                 \n## 2     2 x2            y2\n\nNotice how it got rid of the row with id = 3 from the first and the row with id = 4 from the second.\nYou can also write this with pipes, which is really common when working with {dplyr}:\n\nx %&gt;% \n  inner_join(y, by = \"id\")\n## # A tibble: 2 × 3\n##      id some_variable some_other_variable\n##   &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;              \n## 1     1 x1            y1                 \n## 2     2 x2            y2\n\nLet’s say we have two datasets: national_data_2019 and national_libraries_2019:\n\nnational_data_2019\n## # A tibble: 3 × 4\n##   state unemployment inflation population\n##   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n## 1 GA             5.3       1.8        200\n## 2 NC             5.9       1.6        375\n## 3 CO             4.4       2.6        300\n\n\nnational_libraries_2019\n## # A tibble: 2 × 3\n##   state libraries schools\n##   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 CO          240     440\n## 2 NC          210     590\n\nWe want to bring the libraries and schools columns into the general national data. Notice how both datasets have a state column.\n\n\n\n\n\n\nYour turn\n\n\n\nCreate a new dataset named combined_data that uses inner_join() to merge national_data_2019 and national_libraries_2019."
  },
  {
    "objectID": "lesson/12-lesson.html#left_join",
    "href": "lesson/12-lesson.html#left_join",
    "title": "Space",
    "section": "left_join()",
    "text": "left_join()\nAgain, go to this page in a new tab and stare at the animation.\nLeft joining is less destructive than inner joining. With left joining, any rows in the first dataset that don’t have matches in the second don’t get thrown away and instead are filled with NA:\n\nleft_join(x, y, by = \"id\")\n## # A tibble: 3 × 3\n##      id some_variable some_other_variable\n##   &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;              \n## 1     1 x1            y1                 \n## 2     2 x2            y2                 \n## 3     3 x3            &lt;NA&gt;\n\nNotice how the row with id = 4 from the second dataset is gone, but the row with id = 3 from the first is still there, with NA for some_other_variable.\nI find this much more useful when combining data. I often have a larger dataset with all the main variables I care about, perhaps with every combination of country and year over 20 years and 180 countries. If I find another dataset I want to join, and it has missing data for some of the years or countries, I don’t want the combined data to throw away all the rows from the main big dataset that don’t match! I still want those!\n(Look at this for a real life example: I create a dataset I name panel_skeleton that is just all the combinations of countries and years (Afghanistan 1990, Afghanistan 1991, etc.), and then I bring in all sorts of other datasets that match the same countries and years. When there aren’t matches, nothing in the skeleton gets thrown away—R just adds missing values instead.)\n\n\n\n\n\n\nYour turn\n\n\n\nCreate a new dataset named combined_data that uses left_join() to merge national_data_2019 and national_libraries_2019.\n\n\n\n\n\n\nLeft joining is also often surprisingly helpful for recoding lots of variables. Right now in our fake national data, we have a column for state, but it would be nice if we could have a column for region so we could facet or fill or color by region in a plot. Hunting around on the internet, you find this dataset that has a column for state and a column for abbreviations:\n\nstate_regions\n## # A tibble: 51 × 2\n##    region    state\n##    &lt;chr&gt;     &lt;chr&gt;\n##  1 West      AK   \n##  2 South     AL   \n##  3 South     AR   \n##  4 West      AZ   \n##  5 West      CA   \n##  6 West      CO   \n##  7 Northeast CT   \n##  8 South     DC   \n##  9 South     DE   \n## 10 South     FL   \n## # ℹ 41 more rows\n\n\n\n\n\n\n\nYour turn\n\n\n\nCreate a new dataset named national_data_with_region that uses left_join() to combine national_data_2019 with state_regions.\n\n\n\n\n\n\nBecause left_join() only keeps rows from the second dataset that match the first, we don’t actually bring in all 50 rows from the state_regions data—only the rows that match the first dataset (national_data_2019) come over. We could have done with if some massive recoding (mutate(region = ifelse(state == \"GA\" | state == \"NC\", \"South\", ifelse(state == \"CO\"), \"West\", NA))), but that’s awful. Left joining is far easier here.\nYou can also join by multiple columns. So far we’ve been working with just national_data_2019, but if you look at national_data, you’ll see there are rows for different years across these states:\n\nnational_data\n## # A tibble: 9 × 5\n##   state  year unemployment inflation population\n##   &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n## 1 GA     2018          5         2          100\n## 2 GA     2019          5.3       1.8        200\n## 3 GA     2020          5.2       2.5        300\n## 4 NC     2018          6.1       1.8        350\n## 5 NC     2019          5.9       1.6        375\n## 6 NC     2020          5.3       1.8        400\n## 7 CO     2018          4.7       2.7        200\n## 8 CO     2019          4.4       2.6        300\n## 9 CO     2020          5.1       2.5        400\n\nPreviously, we’ve been specifying the ID column with by = \"state\", but now we have two ID columns: state and year. We can specify both with by = c(\"state\", \"year\").\n\n\n\n\n\n\nYour turn\n\n\n\nCreate a new dataset named national_data_combined that uses left_join() to combine national_data with national_libraries by state and year.\n\n\n\n\n\n\nIf one dataset has things like state and year, but another only has state, left_join() will still work, but it will only join where the state is the same. For instance, here’s what happens when we join the region data to the yearly national data:\n\nnational_data_with_region &lt;- national_data %&gt;% \n  left_join(state_regions, by = \"state\")\nnational_data_with_region\n## # A tibble: 9 × 6\n##   state  year unemployment inflation population region\n##   &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; \n## 1 GA     2018          5         2          100 South \n## 2 GA     2019          5.3       1.8        200 South \n## 3 GA     2020          5.2       2.5        300 South \n## 4 NC     2018          6.1       1.8        350 South \n## 5 NC     2019          5.9       1.6        375 South \n## 6 NC     2020          5.3       1.8        400 South \n## 7 CO     2018          4.7       2.7        200 West  \n## 8 CO     2019          4.4       2.6        300 West  \n## 9 CO     2020          5.1       2.5        400 West\n\nThe “South” region gets added to every row where the state is “GA” and “NC”, even though those rows only appear once in state_regions. left_join() will still match all the values even if states are repeated. Magic!"
  },
  {
    "objectID": "lesson/12-lesson.html#common-column-names",
    "href": "lesson/12-lesson.html#common-column-names",
    "title": "Space",
    "section": "Common column names",
    "text": "Common column names\nSo far, the column names in both datasets have been the same, which has greatly simplified life. In fact, if the columns have the same name, we can technically leave out the by argument and R will guess:\n\nnational_data %&gt;% \n  left_join(national_libraries)\n## Joining with `by = join_by(state, year)`\n## # A tibble: 9 × 7\n##   state  year unemployment inflation population libraries schools\n##   &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 GA     2018          5         2          100        NA      NA\n## 2 GA     2019          5.3       1.8        200        NA      NA\n## 3 GA     2020          5.2       2.5        300        NA      NA\n## 4 NC     2018          6.1       1.8        350       200     610\n## 5 NC     2019          5.9       1.6        375       210     590\n## 6 NC     2020          5.3       1.8        400       220     530\n## 7 CO     2018          4.7       2.7        200       230     470\n## 8 CO     2019          4.4       2.6        300       240     440\n## 9 CO     2020          5.1       2.5        400       270     510\n\nIt’s good practice to be specific about the columns you want and actually use by, but I will often run left_join() without it and then copy the message that it generates (“by = c(\"state\", \"year\")”) and paste it into my code.\nBut what if the column names don’t match? Let’s rename the state column in our state/region table for fun:\n\nstate_regions_different &lt;- state_regions %&gt;% \n  rename(ST = state)\nstate_regions_different\n## # A tibble: 51 × 2\n##    region    ST   \n##    &lt;chr&gt;     &lt;chr&gt;\n##  1 West      AK   \n##  2 South     AL   \n##  3 South     AR   \n##  4 West      AZ   \n##  5 West      CA   \n##  6 West      CO   \n##  7 Northeast CT   \n##  8 South     DC   \n##  9 South     DE   \n## 10 South     FL   \n## # ℹ 41 more rows\n\nNow watch what happens when we try to join the datasets:\n\nnational_data %&gt;% \n  left_join(state_regions_different)\n## Error in `left_join()`:\n## ! `by` must be supplied when `x` and `y` have no common variables.\n## ℹ Use `cross_join()` to perform a cross-join.\n\nThere are no common variables, so we get an error. The state and ST columns really are common variables, but R doesn’t know that.\nWe have two options:\n\nRename one of the columns so it matches (either change state to ST or change ST to state)\nTell left_join() which columns are the same\n\nWe can do option two by modifying the by argument like so:\n\nnational_data %&gt;% \n  left_join(state_regions_different, by = c(\"state\" = \"ST\"))\n## # A tibble: 9 × 6\n##   state  year unemployment inflation population region\n##   &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; \n## 1 GA     2018          5         2          100 South \n## 2 GA     2019          5.3       1.8        200 South \n## 3 GA     2020          5.2       2.5        300 South \n## 4 NC     2018          6.1       1.8        350 South \n## 5 NC     2019          5.9       1.6        375 South \n## 6 NC     2020          5.3       1.8        400 South \n## 7 CO     2018          4.7       2.7        200 West  \n## 8 CO     2019          4.4       2.6        300 West  \n## 9 CO     2020          5.1       2.5        400 West"
  },
  {
    "objectID": "lesson/12-lesson.html#right_join",
    "href": "lesson/12-lesson.html#right_join",
    "title": "Space",
    "section": "right_join()",
    "text": "right_join()\nOnce again, go to this page in a new tab and watch the animation.\nright_join() works exactly like left_join(), but in reverse. The second dataset is the base data. Any rows in the second dataset that don’t match in the first will be kept, and any rows from the first that don’t match will get thrown away.\nWatch what happens if we right join national_data and state_regions:\n\nnational_data %&gt;% \n  right_join(state_regions, by = \"state\")\n## # A tibble: 57 × 6\n##    state  year unemployment inflation population region\n##    &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; \n##  1 GA     2018          5         2          100 South \n##  2 GA     2019          5.3       1.8        200 South \n##  3 GA     2020          5.2       2.5        300 South \n##  4 NC     2018          6.1       1.8        350 South \n##  5 NC     2019          5.9       1.6        375 South \n##  6 NC     2020          5.3       1.8        400 South \n##  7 CO     2018          4.7       2.7        200 West  \n##  8 CO     2019          4.4       2.6        300 West  \n##  9 CO     2020          5.1       2.5        400 West  \n## 10 AK       NA         NA        NA           NA West  \n## # ℹ 47 more rows\n\nYikes. R kept all the rows in state_regions, brought in the columns from national_data and filled most of the new columns with NA, and then repeated Colorado (and NC and GA) three times for each of the years from national_data. That’s a mess.\nIf we reverse the order, we’ll get the correct merged data:\n\nstate_regions %&gt;% \n  right_join(national_data, by = \"state\")\n## # A tibble: 9 × 6\n##   region state  year unemployment inflation population\n##   &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n## 1 West   CO     2018          4.7       2.7        200\n## 2 West   CO     2019          4.4       2.6        300\n## 3 West   CO     2020          5.1       2.5        400\n## 4 South  GA     2018          5         2          100\n## 5 South  GA     2019          5.3       1.8        200\n## 6 South  GA     2020          5.2       2.5        300\n## 7 South  NC     2018          6.1       1.8        350\n## 8 South  NC     2019          5.9       1.6        375\n## 9 South  NC     2020          5.3       1.8        400\n\nI rarely use right_join() because I find it more intuitive to just use left_join() since in my head, I’m taking a dataset and stacking columns onto the end of it. If you want to right join instead, neat—just remember to order things correctly."
  },
  {
    "objectID": "lesson/10-lesson.html",
    "href": "lesson/10-lesson.html",
    "title": "Interactivity",
    "section": "",
    "text": "Again, there’s no lesson for this. The only way to learn how to use ggplotly() and create dashboards with {flexdashboard} is to try them out in RStudio, not in a mini browser-based R session here.\nSo head over to the exercise to get started!"
  },
  {
    "objectID": "lesson/08-lesson.html",
    "href": "lesson/08-lesson.html",
    "title": "Comparisons",
    "section": "",
    "text": "Like the previous session, there isn’t really a lesson this week. You’re not learning how to use any new functions—you’re learning how to apply the geoms you already know in cool and exciting ways. But don’t worry! You’ll have a lesson for session 9!\nFor the lesson, read through the code examples in the example to see how to make small multiples, sparklines, geofacets, and slopegraphs."
  },
  {
    "objectID": "lesson/06-lesson.html",
    "href": "lesson/06-lesson.html",
    "title": "Uncertainty",
    "section": "",
    "text": "Throughout this lesson, you’ll use the built-in mpg dataset to make histograms, density plots, box plots, violin plots, and other graphics that show uncertainty.\nSorry if mpg is getting repetitive! For short interactive things like this, it’s easier to use built-in and easy-to-load datasets like mpg and gapminder instead of loading CSV files, hence our constant reuse of the dataset. This is fairly normal too—the majority of examples in R help pages (and in peoples’ blog posts) use things like mpg or gapminder, or even iris, which measures the lengths and widths of a bunch of iris flowers in the 1930s (fun fact! I don’t like using iris because the data was originally used in an article in the Annals of Eugenics (😬) in 1936, and the data was collected to advance eugenics, and there’s no good reason to use data like that in 2023.)\nSo we work with cars instead of racist flower data.\nThe mpg dataset is available in R as soon as you load ggplot2 (or tidyverse). Yu don’t have to run read_csv() or anything—it’s just there in the background already.\nAs a reminder, here are the first few rows of the mpg dataset:\nhead(mpg)\n## # A tibble: 6 × 11\n##   manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n##   &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n## 1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n## 2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n## 3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…\n## 4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…\n## 5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…\n## 6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…"
  },
  {
    "objectID": "lesson/06-lesson.html#histograms",
    "href": "lesson/06-lesson.html#histograms",
    "title": "Uncertainty",
    "section": "Histograms",
    "text": "Histograms\nWhen working with histograms, you always need to think about the bin width. Histograms calculate the counts of rows within specific ranges of data, and the shape of the histogram will change depending on how wide or narrow these ranges (or bins, or buckets) are.\n\n\n\n\n\n\nYour turn\n\n\n\nChange this code to add a specific bin width for city miles per gallon cty (hint: binwidth). Play around with different widths until you find one that represents the data well.\n\n\n\n\n\n\nBy default, histograms are filled with a dark grey color and the bars have no borders. Additionally, R places the center of the bars at specific numbers: if you have a bin width of 5, for instance, a bar will show the range from 7.5 to 12.5 instead of 5-10 or 10-15.\n\n\n\n\n\n\nYour turn\n\n\n\nDo the following:\n\nAdd a specific bin width\nAdd a white border (hint: color)\nFill with #E16462\nMake it so the bars start at whole numbers like 10 or 20 (hint: boundary)\n\n\n\n\n\n\n\nYou can add extra aesthetics to encode additional information about the distribution of variables across categories.\n\n\n\n\n\n\nYour turn\n\n\n\nMake a histogram of cty and fill by drv (drive: front, rear, and 4-wheel). Make sure you specify a good bin width.\n\n\n\n\n\n\nThat’s too much information! Instead of only filling, you can separate the data into multiple plots.\n\n\n\n\n\n\nYour turn\n\n\n\nMake a histogram of cty fill and facet by drv. Make sure you specify a good bin width. Make sure you specify a good bin width."
  },
  {
    "objectID": "lesson/06-lesson.html#density-plots",
    "href": "lesson/06-lesson.html#density-plots",
    "title": "Uncertainty",
    "section": "Density plots",
    "text": "Density plots\nWhen working with density plots in this class you don’t need to worry too much about the calculus behind the scenes that creates the curves. But you can change those settings if you really want.\n\n\n\n\n\n\nYour turn\n\n\n\nDo the following:\n\nFill this density plot with #E16462\nAdd a border (hint: color) using #9C3836, with linewidth = 1\nChange the bandwidth (hint: bw) to 0.5, then 1, then 10\n\n\n\n\n\n\n\nLike histograms, you can map other variables onto the plot. It’s often a good idea to make the curves semi-transparent so you can see the different distributions.\n\n\n\n\n\n\nYour turn\n\n\n\nDo the following:\n\nFill this plot using the drv variable\nMake the density plots 50% transparent\n\n\n\n\n\n\n\nEven with transparency, it’s often difficult to interpret density plots like this. As an alternative, you can use the {ggridges} package to make ridge plots. Look at the documentation and examples for {ggridges} for lots of details about different plots you can make.\n\n\n\n\n\n\nYour turn\n\n\n\nConvert this plot into a ridge plot."
  },
  {
    "objectID": "lesson/06-lesson.html#boxes-violins-and-dots",
    "href": "lesson/06-lesson.html#boxes-violins-and-dots",
    "title": "Uncertainty",
    "section": "Boxes, violins, and dots",
    "text": "Boxes, violins, and dots\nFinally, you can use things like boxplots and violin plots to show the distribution of variables, either by themselves or across categories.\nBox plots show the distribution of a variable by highlighting specific details, like the 25th, 50th (median) and 75th percentile, as well as the assumed minimum, assumed maximum, and outliers:\n\n\n\nAnatomy of a boxplot\n\n\nWhen making boxplots with ggplot, you need to map the variable of interest to the x aesthetic (or y if you want a vertical boxplot), and you can optionally map a second categorical variable to the y aesthetic (or x if you want a vertical boxplot).\nYou can adjust the fill and color of the plot, and you can change what counts as outliers with the coef argument. By default outliers are any point that is beyond the 75th percentile + 1.5 × the interquartile range (or below the 25th percentile + 1.5 × IQR), but that’s adjustable.\n\n\n\n\n\n\nYour turn\n\n\n\nDo the following:\n\nFill the boxplot with #E6AD3C\nColor the boxplot with #5ABD51\nChange the definition of outliers to be 5 times the IQR\n\n\n\n\n\n\n\nYou can also use violin plots instead of boxplot, which show the mirrored density distribution. When doing this, it’s often helpful to add other geoms like jittered points to show more of the data\n\n\n\n\n\n\nYour turn\n\n\n\nDo the following\n\nChange this boxplot to use violins instead\nAdd jittered points with a jittering width of 0.1 and sized at 0.5"
  },
  {
    "objectID": "lesson/04-lesson.html",
    "href": "lesson/04-lesson.html",
    "title": "Amounts and proportions",
    "section": "",
    "text": "When you visualize proportions with ggplot, you’ll typically go through a two-step process:"
  },
  {
    "objectID": "lesson/04-lesson.html#manipulating-data-with-dplyr",
    "href": "lesson/04-lesson.html#manipulating-data-with-dplyr",
    "title": "Amounts and proportions",
    "section": "Manipulating data with {dplyr}",
    "text": "Manipulating data with {dplyr}\nYou had some experience with {dplyr} functions in the RStudio primers, but we’ll briefly review them here.\nThere are 6 important verbs that you’ll typically use when working with data:\n\nExtract rows/cases with filter()\nExtract columns/variables with select()\nArrange/sort rows with arrange()\nMake new columns/variables with mutate()\nMake group summaries with group_by %&gt;% summarize()\n\nEvery {dplyr} verb follows the same pattern. The first argument is always a data frame, and the function always returns a data frame:\n\nVERB(DATA_TO_TRANSFORM, STUFF_IT_DOES)\n\n\nFiltering with filter()\nThe filter() function takes two arguments: a data frame to transform, and a set of tests. It will return each row for which the test is TRUE.\nThis code, for instance, will look at the gapminder dataset and return all rows where country is equal to “Denmark”:\n\nfilter(gapminder, country == \"Denmark\")\n## # A tibble: 12 × 6\n##    country continent  year lifeExp     pop gdpPercap\n##    &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;\n##  1 Denmark Europe     1952    70.8 4334000     9692.\n##  2 Denmark Europe     1957    71.8 4487831    11100.\n##  3 Denmark Europe     1962    72.4 4646899    13583.\n##  4 Denmark Europe     1967    73.0 4838800    15937.\n##  5 Denmark Europe     1972    73.5 4991596    18866.\n##  6 Denmark Europe     1977    74.7 5088419    20423.\n##  7 Denmark Europe     1982    74.6 5117810    21688.\n##  8 Denmark Europe     1987    74.8 5127024    25116.\n##  9 Denmark Europe     1992    75.3 5171393    26407.\n## 10 Denmark Europe     1997    76.1 5283663    29804.\n## 11 Denmark Europe     2002    77.2 5374693    32167.\n## 12 Denmark Europe     2007    78.3 5468120    35278.\n\nNotice that there are two equal signs (==). This is because it’s a logical test, similar to greater than (&gt;) or less than (&lt;). When you use a single equal sign, you set an argument (like data = gapminder); when you use two, you are doing a test. There are lots of different ways to do logical tests:\n\n\n\nTest\nMeaning\n\n\n\n\nx &lt; y\nLess than\n\n\nx &gt; y\nGreater than\n\n\nx == y\nEqual to\n\n\nx &lt;= y\nLess than or equal to\n\n\nx &gt;= y\nGreater than or equal to\n\n\nx != y\nNot equal to\n\n\nx %in% y\nIn (group membership)\n\n\nis.na(x)\nIs missing\n\n\n!is.na(x)\nIs not missing\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nUse filter() and logical tests to show:\n\nThe data for Canada\nAll data for countries in Oceania\nRows where life expectancy is greater than 82\n\n\n\n\n\n\n\nYou can also use multiple conditions, and these will extract rows that meet every test. By default, if you separate the tests with a comma, R will consider this an “and” test and find rows that are both Denmark and greater than 2000.\n\nfilter(gapminder, country == \"Denmark\", year &gt; 2000)\n## # A tibble: 2 × 6\n##   country continent  year lifeExp     pop gdpPercap\n##   &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;\n## 1 Denmark Europe     2002    77.2 5374693    32167.\n## 2 Denmark Europe     2007    78.3 5468120    35278.\n\nYou can also use “or” with “|” and “not” with “!”:\n\n\n\nOperator\nMeaning\n\n\n\n\na & b\nand\n\n\na | b\nor\n\n\n!a\nnot\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nUse filter() and logical tests to show:\n\nCanada before 1970\nCountries where life expectancy in 2007 is below 50\nCountries where life expectancy in 2007 is below 50 and are not in Africa\n\n\n\n\n\n\n\nBeware of some common mistakes! You can’t collapse multiple tests into one. Instead, use two separate tests:\n\n# This won't work!\nfilter(gapminder, 1960 &lt; year &lt; 1980)\n\n# This will work\nfilter(gapminder, 1960 &lt; year, year &lt; 1980)\n\nAlso, you can avoid stringing together lots of tests by using the %in% operator, which checks to see if a value is in a list of values.\n\n# This works, but is tedious\nfilter(gapminder, \n       country == \"Mexico\" | country == \"Canada\" | country == \"United States\")\n\n# This is more concise and easier to add other countries later\nfilter(gapminder, \n       country %in% c(\"Mexico\", \"Canada\", \"United States\"))\n\n\n\nAdding new columns with mutate()\nYou create new columns with the mutate() function. You can create a single column like this:\n\nmutate(gapminder, gdp = gdpPercap * pop)\n## # A tibble: 1,704 × 7\n##    country     continent  year lifeExp      pop gdpPercap          gdp\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n##  1 Afghanistan Asia       1952    28.8  8425333      779.  6567086330.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.  7585448670.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.  8758855797.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.  9648014150.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.  9678553274.\n##  6 Afghanistan Asia       1977    38.4 14880372      786. 11697659231.\n##  7 Afghanistan Asia       1982    39.9 12881816      978. 12598563401.\n##  8 Afghanistan Asia       1987    40.8 13867957      852. 11820990309.\n##  9 Afghanistan Asia       1992    41.7 16317921      649. 10595901589.\n## 10 Afghanistan Asia       1997    41.8 22227415      635. 14121995875.\n## # ℹ 1,694 more rows\n\nAnd you can create multiple columns by including a comma-separated list of new columns to create:\n\nmutate(gapminder, gdp = gdpPercap * pop,\n                  pop_mill = round(pop / 1000000))\n## # A tibble: 1,704 × 8\n##    country     continent  year lifeExp      pop gdpPercap          gdp pop_mill\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n##  1 Afghanistan Asia       1952    28.8  8425333      779.  6567086330.        8\n##  2 Afghanistan Asia       1957    30.3  9240934      821.  7585448670.        9\n##  3 Afghanistan Asia       1962    32.0 10267083      853.  8758855797.       10\n##  4 Afghanistan Asia       1967    34.0 11537966      836.  9648014150.       12\n##  5 Afghanistan Asia       1972    36.1 13079460      740.  9678553274.       13\n##  6 Afghanistan Asia       1977    38.4 14880372      786. 11697659231.       15\n##  7 Afghanistan Asia       1982    39.9 12881816      978. 12598563401.       13\n##  8 Afghanistan Asia       1987    40.8 13867957      852. 11820990309.       14\n##  9 Afghanistan Asia       1992    41.7 16317921      649. 10595901589.       16\n## 10 Afghanistan Asia       1997    41.8 22227415      635. 14121995875.       22\n## # ℹ 1,694 more rows\n\nYou can also do conditional tests within mutate() using the ifelse() function. This works like the =IFELSE function in Excel. Feed the function three arguments: (1) a test, (2) the value if the test is true, and (3) the value if the test is false:\n\nifelse(TEST, VALUE_IF_TRUE, VALUE_IF_FALSE)\n\nWe can create a new column that is a binary indicator for whether the country’s row is after 1960:\n\nmutate(gapminder, after_1960 = ifelse(year &gt; 1960, TRUE, FALSE))\n## # A tibble: 1,704 × 7\n##    country     continent  year lifeExp      pop gdpPercap after_1960\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt; &lt;lgl&gt;     \n##  1 Afghanistan Asia       1952    28.8  8425333      779. FALSE     \n##  2 Afghanistan Asia       1957    30.3  9240934      821. FALSE     \n##  3 Afghanistan Asia       1962    32.0 10267083      853. TRUE      \n##  4 Afghanistan Asia       1967    34.0 11537966      836. TRUE      \n##  5 Afghanistan Asia       1972    36.1 13079460      740. TRUE      \n##  6 Afghanistan Asia       1977    38.4 14880372      786. TRUE      \n##  7 Afghanistan Asia       1982    39.9 12881816      978. TRUE      \n##  8 Afghanistan Asia       1987    40.8 13867957      852. TRUE      \n##  9 Afghanistan Asia       1992    41.7 16317921      649. TRUE      \n## 10 Afghanistan Asia       1997    41.8 22227415      635. TRUE      \n## # ℹ 1,694 more rows\n\nWe can also use text labels instead of TRUE and FALSE:\n\nmutate(gapminder, \n       after_1960 = ifelse(year &gt; 1960, \"After 1960\", \"Before 1960\"))\n## # A tibble: 1,704 × 7\n##    country     continent  year lifeExp      pop gdpPercap after_1960 \n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;      \n##  1 Afghanistan Asia       1952    28.8  8425333      779. Before 1960\n##  2 Afghanistan Asia       1957    30.3  9240934      821. Before 1960\n##  3 Afghanistan Asia       1962    32.0 10267083      853. After 1960 \n##  4 Afghanistan Asia       1967    34.0 11537966      836. After 1960 \n##  5 Afghanistan Asia       1972    36.1 13079460      740. After 1960 \n##  6 Afghanistan Asia       1977    38.4 14880372      786. After 1960 \n##  7 Afghanistan Asia       1982    39.9 12881816      978. After 1960 \n##  8 Afghanistan Asia       1987    40.8 13867957      852. After 1960 \n##  9 Afghanistan Asia       1992    41.7 16317921      649. After 1960 \n## 10 Afghanistan Asia       1997    41.8 22227415      635. After 1960 \n## # ℹ 1,694 more rows\n\n\n\n\n\n\n\nYour turn\n\n\n\nUse mutate() to:\n\nAdd an africa column that is TRUE if the country is on the African continent\nAdd a column for logged GDP per capita\nAdd an africa_asia column that says “Africa or Asia” if the country is in Africa or Asia, and “Not Africa or Asia” if it’s not\n\n\n\n\n\n\n\n\n\nCombining multiple verbs with pipes (%&gt;%)\nWhat if you want to filter to include only rows from 2002 and make a new column with the logged GDP per capita? Doing this requires both filter() and mutate(), so we need to find a way to use both at once.\nOne solution is to use intermediate variables for each step:\n\ngapminder_2002_filtered &lt;- filter(gapminder, year == 2002)\n\ngapminder_2002_logged &lt;- mutate(gapminder_2002_filtered, log_gdpPercap = log(gdpPercap))\n\nThat works fine, but your environment panel will start getting full of lots of intermediate data frames.\nAnother solution is to nest the functions inside each other. Remember that all {dplyr} functions return data frames, so you can feed the results of one into another:\n\nfilter(mutate(gapminder, log_gdpPercap = log(gdpPercap)), \n       year == 2002)\n\nThat works too, but it gets really complicated once you have even more functions, and it’s hard to keep track of which function’s arguments go where. I’d avoid doing this entirely.\nOne really nice solution is to use a pipe, or %&gt;%. The pipe takes an object on the left and passes it as the first argument of the function on the right.\n\n# gapminder will automatically get placed in the _____ spot\ngapminder %&gt;% filter(_____, country == \"Canada\")\n\nThese two lines of code do the same thing:\n\nfilter(gapminder, country == \"Canada\")\n\ngapminder %&gt;% filter(country == \"Canada\")\n\nUsing pipes, you can start with a data frame, pass it to one verb, then pass the output of that verb to the next verb, and so on. When reading any code with a %&gt;%, it’s easiest to read the %&gt;% as “and then”. This would read:\n\nTake the gapminder dataset and then filter it so that it only has rows from 2002 and then add a new column with the logged GDP per capita\n\n\ngapminder %&gt;% \n  filter(year == 2002) %&gt;% \n  mutate(log_gdpPercap = log(gdpPercap))\n\nHere’s another way to think about pipes more conceptually. This isn’t valid R code, obviously, but imagine you’re going to take yourself, and then wake up, get out of bed, get dressed, and leave the house. Writing that whole process as nested functions would look like this:\n\nleave_house(get_dressed(get_out_of_bed(wake_up(me, time = \"8:00\"), side = \"correct\"), pants = TRUE, shirt = TRUE), car = TRUE, bike = FALSE)\n\nInstead of nesting everything, we can use pipes to chain these together. This would read\n\nTake myself, and then wake up at 8:00, and then get out of bed on the correct side, and then get dressed with pants and a shirt, and then leave the house in a car\n\n\nme %&gt;% \n  wake_up(time = \"8:00\") %&gt;% \n  get_out_of_bed(side = \"correct\") %&gt;% \n  get_dressed(pants = TRUE, shirt = TRUE) %&gt;% \n  leave_house(car = TRUE, bike = FALSE)\n\n\n\nSummarizing data by groups with group_by() %&gt;% summarize()\nThe summarize() verb takes an entire frame and calculates summary information about it. For instance, this will find the average life expectancy for the whole gapminder data:\n\ngapminder %&gt;% summarize(mean_life = mean(lifeExp))\n## # A tibble: 1 × 1\n##   mean_life\n##       &lt;dbl&gt;\n## 1      59.5\n\nYou can also make multiple summary variables, just like mutate(), and it will return a column for each:\n\ngapminder %&gt;% summarize(mean_life = mean(lifeExp),\n                        min_life = min(lifeExp))\n## # A tibble: 1 × 2\n##   mean_life min_life\n##       &lt;dbl&gt;    &lt;dbl&gt;\n## 1      59.5     23.6\n\n\n\n\n\n\n\nYour turn\n\n\n\nUse summarize() to calculate:\n\nThe first (minimum) year in the gapminder dataset\nThe last (maximum) year in the dataset\nThe number of rows in the dataset (use the {dplyr} cheatsheet)\nThe number of distinct countries in the dataset (use the {dplyr} cheatsheet)\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nUse filter() and summarize() to calculate the median life expectancy on the African continent in 2007:\n\n\n\n\n\n\nNotice that summarize() on its own summarizes the whole dataset, so you only get a single row back. These values are the averages and minimums for the entire data frame. If you group your data into separate subgroups, you can use summarize() to calculate summary statistics for each group. Do this with group_by().\nThe group_by() function puts rows into groups based on values in a column. If you run this:\n\ngapminder %&gt;% group_by(continent)\n## # A tibble: 1,704 × 6\n## # Groups:   continent [5]\n##    country     continent  year lifeExp      pop gdpPercap\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # ℹ 1,694 more rows\n\n…you won’t see anything different! R has put the dataset into separate invisible groups behind the scenes, but you haven’t done anything with those groups, so nothing has really happened. If you do things with those groups with summarize(), though, group_by() becomes much more useful.\nFor instance, this will take the gapminder data frame, group it by continent, and then summarize it by calculating the number of distinct countries in each group. It will return one row for each group, so there should be a row for each continent:\n\ngapminder %&gt;% \n  group_by(continent) %&gt;% \n  summarize(n_countries = n_distinct(country)) \n## # A tibble: 5 × 2\n##   continent n_countries\n##   &lt;fct&gt;           &lt;int&gt;\n## 1 Africa             52\n## 2 Americas           25\n## 3 Asia               33\n## 4 Europe             30\n## 5 Oceania             2\n\nYou can calculate multiple summary statistics, as before:\n\ngapminder %&gt;% \n  group_by(continent) %&gt;% \n  summarize(n_countries = n_distinct(country),\n            avg_life_exp = mean(lifeExp)) \n## # A tibble: 5 × 3\n##   continent n_countries avg_life_exp\n##   &lt;fct&gt;           &lt;int&gt;        &lt;dbl&gt;\n## 1 Africa             52         48.9\n## 2 Americas           25         64.7\n## 3 Asia               33         60.1\n## 4 Europe             30         71.9\n## 5 Oceania             2         74.3\n\n\n\n\n\n\n\nYour turn\n\n\n\nFind the minimum, maximum, and median life expectancy for each continent:\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn\n\n\n\nFind the minimum, maximum, and median life expectancy for each continent in 2007 only:\n\n\n\n\n\n\nFinally, you can group by multiple columns and R will create subgroups for every combination of the groups and return the number of rows of combinations. For instance, we can calculate the average life expectancy by both year and continent and we’ll get 60 rows, since there are 5 continents and 12 years (5 × 12 = 60):\n\ngapminder %&gt;% \n  group_by(continent, year) %&gt;% \n  summarize(avg_life_exp = mean(lifeExp)) \n## # A tibble: 60 × 3\n## # Groups:   continent [5]\n##    continent  year avg_life_exp\n##    &lt;fct&gt;     &lt;int&gt;        &lt;dbl&gt;\n##  1 Africa     1952         39.1\n##  2 Africa     1957         41.3\n##  3 Africa     1962         43.3\n##  4 Africa     1967         45.3\n##  5 Africa     1972         47.5\n##  6 Africa     1977         49.6\n##  7 Africa     1982         51.6\n##  8 Africa     1987         53.3\n##  9 Africa     1992         53.6\n## 10 Africa     1997         53.6\n## # ℹ 50 more rows\n\n\n\nSelecting with select()\nThe last two verbs are far simpler than filter(), mutate(), and group_by() %&gt;% summarize().\nYou can choose specific columns with the select() verb. This will only keep two columns: lifeExp and year:\n\ngapminder %&gt;% select(lifeExp, year)\n## # A tibble: 1,704 × 2\n##    lifeExp  year\n##      &lt;dbl&gt; &lt;int&gt;\n##  1    28.8  1952\n##  2    30.3  1957\n##  3    32.0  1962\n##  4    34.0  1967\n##  5    36.1  1972\n##  6    38.4  1977\n##  7    39.9  1982\n##  8    40.8  1987\n##  9    41.7  1992\n## 10    41.8  1997\n## # ℹ 1,694 more rows\n\nYou can remove specific columns by prefacing the column names with -, like -lifeExp:\n\ngapminder %&gt;% select(-lifeExp)\n## # A tibble: 1,704 × 5\n##    country     continent  year      pop gdpPercap\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;    &lt;int&gt;     &lt;dbl&gt;\n##  1 Afghanistan Asia       1952  8425333      779.\n##  2 Afghanistan Asia       1957  9240934      821.\n##  3 Afghanistan Asia       1962 10267083      853.\n##  4 Afghanistan Asia       1967 11537966      836.\n##  5 Afghanistan Asia       1972 13079460      740.\n##  6 Afghanistan Asia       1977 14880372      786.\n##  7 Afghanistan Asia       1982 12881816      978.\n##  8 Afghanistan Asia       1987 13867957      852.\n##  9 Afghanistan Asia       1992 16317921      649.\n## 10 Afghanistan Asia       1997 22227415      635.\n## # ℹ 1,694 more rows\n\nYou can also rename columns using select(). Follow this pattern: select(old_name = new_name).\n\ngapminder %&gt;% select(year, country, life_expectancy = lifeExp)\n## # A tibble: 1,704 × 3\n##     year country     life_expectancy\n##    &lt;int&gt; &lt;fct&gt;                 &lt;dbl&gt;\n##  1  1952 Afghanistan            28.8\n##  2  1957 Afghanistan            30.3\n##  3  1962 Afghanistan            32.0\n##  4  1967 Afghanistan            34.0\n##  5  1972 Afghanistan            36.1\n##  6  1977 Afghanistan            38.4\n##  7  1982 Afghanistan            39.9\n##  8  1987 Afghanistan            40.8\n##  9  1992 Afghanistan            41.7\n## 10  1997 Afghanistan            41.8\n## # ℹ 1,694 more rows\n\nAlternatively, there’s a special rename() verb that will, um, rename, while keeping all the other columns:\n\ngapminder %&gt;% rename(life_expectancy = lifeExp)\n## # A tibble: 1,704 × 6\n##    country     continent  year life_expectancy      pop gdpPercap\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;           &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n##  1 Afghanistan Asia       1952            28.8  8425333      779.\n##  2 Afghanistan Asia       1957            30.3  9240934      821.\n##  3 Afghanistan Asia       1962            32.0 10267083      853.\n##  4 Afghanistan Asia       1967            34.0 11537966      836.\n##  5 Afghanistan Asia       1972            36.1 13079460      740.\n##  6 Afghanistan Asia       1977            38.4 14880372      786.\n##  7 Afghanistan Asia       1982            39.9 12881816      978.\n##  8 Afghanistan Asia       1987            40.8 13867957      852.\n##  9 Afghanistan Asia       1992            41.7 16317921      649.\n## 10 Afghanistan Asia       1997            41.8 22227415      635.\n## # ℹ 1,694 more rows\n\n\n\nArranging data with arrange()\nThe arrange() verb sorts data. By default it sorts ascendingly, putting the lowest values first:\n\ngapminder %&gt;% arrange(lifeExp)\n## # A tibble: 1,704 × 6\n##    country      continent  year lifeExp     pop gdpPercap\n##    &lt;fct&gt;        &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;\n##  1 Rwanda       Africa     1992    23.6 7290203      737.\n##  2 Afghanistan  Asia       1952    28.8 8425333      779.\n##  3 Gambia       Africa     1952    30    284320      485.\n##  4 Angola       Africa     1952    30.0 4232095     3521.\n##  5 Sierra Leone Africa     1952    30.3 2143249      880.\n##  6 Afghanistan  Asia       1957    30.3 9240934      821.\n##  7 Cambodia     Asia       1977    31.2 6978607      525.\n##  8 Mozambique   Africa     1952    31.3 6446316      469.\n##  9 Sierra Leone Africa     1957    31.6 2295678     1004.\n## 10 Burkina Faso Africa     1952    32.0 4469979      543.\n## # ℹ 1,694 more rows\n\nYou can reverse that by wrapping the column name with desc():\n\ngapminder %&gt;% arrange(desc(lifeExp))\n## # A tibble: 1,704 × 6\n##    country          continent  year lifeExp       pop gdpPercap\n##    &lt;fct&gt;            &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;     &lt;int&gt;     &lt;dbl&gt;\n##  1 Japan            Asia       2007    82.6 127467972    31656.\n##  2 Hong Kong, China Asia       2007    82.2   6980412    39725.\n##  3 Japan            Asia       2002    82   127065841    28605.\n##  4 Iceland          Europe     2007    81.8    301931    36181.\n##  5 Switzerland      Europe     2007    81.7   7554661    37506.\n##  6 Hong Kong, China Asia       2002    81.5   6762476    30209.\n##  7 Australia        Oceania    2007    81.2  20434176    34435.\n##  8 Spain            Europe     2007    80.9  40448191    28821.\n##  9 Sweden           Europe     2007    80.9   9031088    33860.\n## 10 Israel           Asia       2007    80.7   6426679    25523.\n## # ℹ 1,694 more rows\n\nYou can sort by multiple columns by specifying them in a comma separated list. For example, we can sort by continent and then sort by life expectancy within the continents:\n\ngapminder %&gt;% \n  arrange(continent, desc(lifeExp))\n## # A tibble: 1,704 × 6\n##    country   continent  year lifeExp      pop gdpPercap\n##    &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n##  1 Reunion   Africa     2007    76.4   798094     7670.\n##  2 Reunion   Africa     2002    75.7   743981     6316.\n##  3 Reunion   Africa     1997    74.8   684810     6072.\n##  4 Libya     Africa     2007    74.0  6036914    12057.\n##  5 Tunisia   Africa     2007    73.9 10276158     7093.\n##  6 Reunion   Africa     1992    73.6   622191     6101.\n##  7 Tunisia   Africa     2002    73.0  9770575     5723.\n##  8 Mauritius Africa     2007    72.8  1250882    10957.\n##  9 Libya     Africa     2002    72.7  5368585     9535.\n## 10 Algeria   Africa     2007    72.3 33333216     6223.\n## # ℹ 1,694 more rows\n\n\n\nThat’s it!\nThose are the main verbs you’ll deal with in this class. There are dozens of other really useful ones—check out the {dplyr} and {tidyr} cheat sheet for examples."
  },
  {
    "objectID": "lesson/04-lesson.html#changing-colors-shapes-and-sizes-with-scale_",
    "href": "lesson/04-lesson.html#changing-colors-shapes-and-sizes-with-scale_",
    "title": "Amounts and proportions",
    "section": "Changing colors, shapes, and sizes, with scale_*()",
    "text": "Changing colors, shapes, and sizes, with scale_*()\nRecall from session 3 that the grammar of graphics uses a set of layers to define elements of plots:\n\n\n\nIn tomorrow’s session, you’ll learn all about the Theme layer. Here we’ll briefly cover the Scales layer, which we use for changing aspects of the different aesthetics, like using logged axes or changing colors or shapes.\nAll the functions that deal with scales conveniently follow the same naming pattern:\n\nscale_AESTHETIC_DETAILS()\n\nHere are some common scale functions:\n\nscale_x_continuous()\nscale_y_reverse()\nscale_color_viridis_c()\nscale_shape_manual(values = c(19, 13, 15))\nscale_fill_manual(values = c(\"red\", \"orange\", \"blue\"))\n\nYou can see a list of all of the possible scale functions here, and you should reference that documentation (and the excellent examples) often when working with these functions.\nAs long as you have mapped a variable to an aesthetic with aes(), you can use the scale_*() functions to deal with it. For instance, in this ggplot, we have mapped variables to x, y, and fill, which means we can use those corresponding scale functions to manipulate how those aesthetics are shown. Here we reverse the y-axis (ew, don’t really do this), and we use a discrete viridis color palette:\n\ncontinent_counts &lt;- gapminder %&gt;% \n  group_by(continent) %&gt;% \n  summarize(countries = n_distinct(country))\n\nggplot(continent_counts, aes(x = continent, y = countries, fill = continent)) +\n  geom_col() +\n  scale_y_reverse() +  # lol this is bad; don't do it in real life\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\nYou can also use different arguments in the scale functions—again, check the documentation for examples. For instance, if we want to use the plasma palette from the viridis package, we can set that as an option:\n\nggplot(continent_counts, aes(x = continent, y = countries, fill = continent)) +\n  geom_col() +\n  scale_fill_viridis_d(option = \"plasma\")\n\n\n\n\n\n\n\n\nThat yellow might be too bright and hard to see, so we can tell ggplot to not use the full range of the palette, ending at 90% of the range instead:\n\nggplot(continent_counts, aes(x = continent, y = countries, fill = continent)) +\n  geom_col() +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.9)\n\n\n\n\n\n\n\n\nInstead of letting R calculate the colors from a general palette, you can also specify your own colors with scale_fill_manual() and feeding it a list of values—generally as hex codes or a name from a list of built-in R colors:\n\nggplot(continent_counts, aes(x = continent, y = countries, fill = continent)) +\n  geom_col() +\n  scale_fill_manual(values = c(\"chartreuse4\", \"cornsilk4\", \"black\", \"#fc03b6\", \"#5c47d6\"))\n\n\n\n\n\n\n\n\nScale functions also work for other aesthetics like shape or color or size. For instance, consider this plot, which has all three:\n\ngapminder_2007 &lt;- gapminder %&gt;% \n  filter(year == 2007)\n\nggplot(gapminder_2007,\n       aes(x = gdpPercap, y = lifeExp, \n           color = continent, shape = continent, size = pop)) +\n  geom_point() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nWe can change the colors of the points with scale_color_*():\n\nggplot(gapminder_2007,\n       aes(x = gdpPercap, y = lifeExp, \n           color = continent, shape = continent, size = pop)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_color_manual(values = c(\"chartreuse4\", \"cornsilk4\", \"black\", \"#fc03b6\", \"#5c47d6\"))\n\n\n\n\n\n\n\n\nWe can change the shapes with scale_shape_*(). If you run ?pch in your console or search for pch in the help, you can see all the possible shapes.\n\nggplot(gapminder_2007,\n       aes(x = gdpPercap, y = lifeExp, \n           color = continent, shape = continent, size = pop)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_shape_manual(values = c(12, 9, 17, 19, 15))\n\n\n\n\n\n\n\n\nYou can change the size with scale_size_*(). Here we make it so the smallest possible size is 1 and the largest is 15:\n\nggplot(gapminder_2007,\n       aes(x = gdpPercap, y = lifeExp, \n           color = continent, shape = continent, size = pop)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_size_continuous(range = c(1, 15))\n\n\n\n\n\n\n\n\nWe can even do all three at once:\n\nggplot(gapminder_2007,\n       aes(x = gdpPercap, y = lifeExp, \n           color = continent, shape = continent, size = pop)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_color_manual(values = c(\"chartreuse4\", \"cornsilk4\", \"black\", \"#fc03b6\", \"#5c47d6\")) +\n  scale_shape_manual(values = c(12, 9, 17, 19, 15)) +\n  scale_size_continuous(range = c(1, 15))\n\n\n\n\n\n\n\n\nPhew. That’s ugly.\nOne last thing we can do with scales is format how they show up on the plot. Notice how the population legend uses scientific notation like 2.50e+08. This means you need to move the decimal point 8 places to the right, making it 250000000. Leaving it in scientific notation isn’t great because it makes it really hard to read and interpret.\nIf you load the {scales} library (which is installed as part of {tidyverse} but isn’t automatically loaded), you can use some neat helper functions to reformat the text that shows up in plots. For instance, we can make it so population is formatted as a number with commas every 3 numbers, and the x-axis is formatted as dollars:\n\nlibrary(scales)\n\nggplot(gapminder_2007,\n       aes(x = gdpPercap, y = lifeExp, \n           color = continent, shape = continent, size = pop)) +\n  geom_point() +\n  scale_x_log10(labels = label_dollar()) +\n  scale_size_continuous(labels = label_comma())\n\n\n\n\n\n\n\n\nCheck the documentation for {scales} for details about all the labelling functions it has, including dates, percentages, p-values, LaTeX math, etc."
  },
  {
    "objectID": "lesson/02-lesson.html",
    "href": "lesson/02-lesson.html",
    "title": "Graphic design",
    "section": "",
    "text": "Recall from the last section of the lecture that you’ll typically work with one of two image file types: bitmap images and vector images.\nBitmaps store image information as tiny squares, or pixels. Specific files types compress these images in different ways: JPEG files smudge together groups of similarly colored pixels to save repetition, while PNG and GIF files look for fields of the exact same color.\n\n\n\nYou use bitmap images for things that go on the internet and when you place images in Word (technically modern versions of Word can handle some types of vector images, but that support isn’t universal yet).\nVector images, on the other hand, do not store image information as pixels. Instead, these use mathematical formulas to draw lines and curves and fill areas with specific colors. This makes them a little more complicated to draw and create, but it also means that you can scale them up or down infinitely—a vector image will look just as crisp on a postage stamp as it would on a billboard.\n\n\n\nHere are some general guidelines:\n\nIf an image has lots of colors (like a photograph), you should use a bitmap file type designed for lots of colors, like JPEG. This is the case regardless of where the image will ultimately end up. If you’re putting it on the internet, it needs to be a JPEG. If you’re blowing it up to fit on a billboard, it will still need to be a JPEG (and you have to use a fancy super high quality camera to get a high enough resolution for that kind of expansion)\nIf an image has a few colors and some text and is not a photograph and you’re using the image in Word or on the internet, you should use a bitmap file type designed for carefully compressing a few colors, like PNG.\nIf an image has a few colors and some text and is not a photograph and you’re planning on using it in multiple sizes (like a logo), or using it in fancier production software like Adobe InDesign (for print) or Adobe After Effects (for video), you should use a vector file type like PDF or SVG."
  },
  {
    "objectID": "lesson/02-lesson.html#file-types",
    "href": "lesson/02-lesson.html#file-types",
    "title": "Graphic design",
    "section": "",
    "text": "Recall from the last section of the lecture that you’ll typically work with one of two image file types: bitmap images and vector images.\nBitmaps store image information as tiny squares, or pixels. Specific files types compress these images in different ways: JPEG files smudge together groups of similarly colored pixels to save repetition, while PNG and GIF files look for fields of the exact same color.\n\n\n\nYou use bitmap images for things that go on the internet and when you place images in Word (technically modern versions of Word can handle some types of vector images, but that support isn’t universal yet).\nVector images, on the other hand, do not store image information as pixels. Instead, these use mathematical formulas to draw lines and curves and fill areas with specific colors. This makes them a little more complicated to draw and create, but it also means that you can scale them up or down infinitely—a vector image will look just as crisp on a postage stamp as it would on a billboard.\n\n\n\nHere are some general guidelines:\n\nIf an image has lots of colors (like a photograph), you should use a bitmap file type designed for lots of colors, like JPEG. This is the case regardless of where the image will ultimately end up. If you’re putting it on the internet, it needs to be a JPEG. If you’re blowing it up to fit on a billboard, it will still need to be a JPEG (and you have to use a fancy super high quality camera to get a high enough resolution for that kind of expansion)\nIf an image has a few colors and some text and is not a photograph and you’re using the image in Word or on the internet, you should use a bitmap file type designed for carefully compressing a few colors, like PNG.\nIf an image has a few colors and some text and is not a photograph and you’re planning on using it in multiple sizes (like a logo), or using it in fancier production software like Adobe InDesign (for print) or Adobe After Effects (for video), you should use a vector file type like PDF or SVG."
  },
  {
    "objectID": "lesson/02-lesson.html#select-the-best-file-type",
    "href": "lesson/02-lesson.html#select-the-best-file-type",
    "title": "Graphic design",
    "section": "Select the best file type",
    "text": "Select the best file type\nPractice deciding what kind of file type you should use by looking at these images and choosing what you think works the best.\n\n\n\n\n\n\n\n\nPNG\n\n\nJPG\n\n\nPDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPNG\n\n\nJPG\n\n\nPDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPNG\n\n\nJPG\n\n\nPDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPNG\n\n\nJPG\n\n\nPDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPNG\n\n\nJPG\n\n\nPDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPNG\n\n\nJPG\n\n\nPDF"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Data Visualization\n        ",
    "section": "",
    "text": "Data Visualization\n        \n        \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n        \n        \n            Andrew Young School of Policy StudiesGeorgia State University\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\nTA\n\n   Eun Joo Kwon\n   55 Park Place SE, Desk 19\n   ekwon8@gsu.edu\n   Schedule an appointment\n\n\n\nCourse details\n\n   Any day\n   Asynchronous\n   Anywhere\n\n\n\nContacting me\nE-mail is the best ways to get in contact with me. I will try to respond to all course-related e-mails within two days! You can also schedule a meeting by clicking the “Schedule an appointment” link. If you have any questions or if there’s anything else you’d like assistance with, feel free to let me know!"
  },
  {
    "objectID": "example/15-example.html",
    "href": "example/15-example.html",
    "title": "Sharing R output online",
    "section": "",
    "text": "The easiest way to get a knitted R Markdown onto the internet is to use RPubs. We talked about this in session 10, and you used it to turn in exercise 10. After knitting an HTML document in RStudio, click on the “Publish” button in the top right corner to upload the document to the RPubs server and get a URL that you can share with others:\n\n\n\nYou don’t have to set up a web server or anything—it’s all pretty automatic and seamless.\n\n\n\nIf you have something slightly more complex, like a collection of R Markdown files that do related things, it’s easy to stitch them all together in an R Markdown website. RStudio supports these automatically—after telling RStudio to consider an RStudio project to be a website, it will knit all the .Rmd files in the root of your project directory every time you click on the “Build Website” button.\nRStudio generates a standalone folder named public with static HTML pages of all your knitted documents. You then have to put that folder on the internet somewhere, either on a web server you have access to, or a free service like Netlify.\nSee this page for complete documentation, or follow these tutorials by Lucy D’Agostino McGowan and Emily Zabor.\nThese websites are especially helpful for standalone projects like research papers and reports. I’ve had students do their master’s capstone projects with these, with specific pages for their introduction, literature review, data cleaning, exploratory data analysis, modeling, and results.\nI typically make a website for each of my research projects and will include pages with IRB details, copies of survey experiments, data cleaning, results, and so on. Here are some examples:\n\nNGO Crackdowns and Philanthropy\nAre Donors Really Responding?\nThe Power of Ranking\nConstraint Closure\n\nYou can also make really neat small websites like Desirée De Leon’s Teacup Giraffes for teaching basic statistics.\n\n\n\nIf you want more control (i.e. total control) over the HTML output and the structure of a website, you can use a package named {blogdown} to convert R Markdown files into an entire website. This course website is built with {blogdown}: you can see all the underlying R Markdown files at GitHub.\nLike R Markdown websites, blogdown generates a complete static version of the knitted website and puts it in a folder named public. You’re then responsible for putting that somewhere on the internet, either on your own server or by using a free hosting service like Netlify.\nBlogdown is incredibly well documented, and there are lots of tutorials for how to get started. Alison Hill’s tutorial here is the best place to get started—follow it and you’ll have a basic blog completely free.\n\n\n\nIf you don’t want to create a website, you can use a package named {bookdown} to stitch a collection of R Markdown files into a PDF, Word, or HTML book. (You could even put all your exercises from this class into a single book!). {bookdown} is incredibly well documented too (as a {bookdown} book), and you can get familiar with it fairly quickly.\nDozens of real-world books, dissertations, and theses have been written with {bookdown}, including both Claus Wilke’s and Kieran Healy’s books from this course. Because of the magic of Markdown, you can create parallel HTML and PDF versions of your book and post one type of output on the internet and print and bind the other one.\n\n\n\nR Markdown isn’t just for PDF, Word, and HTML documents. You can also make slides! All the slides for this course were made in R Markdown with a package named {xaringan}. You can see the documentation here, and see the main example presentation here. You can also see all the R Markdown files I wrote to create the slides for this class here.\n\n\n\nAnd finally, if you want to share code (and keep track of versions of your code), GitHub is one of the best places for that. Posting your code at places like GitHub lets other people see and borrow and adapt and make suggestions to your code. You can see all my different repositories and projects here, for example.\nJenny Bryan has a useful {bookdown} website explaining how to get started, and GitHub itself has excellent materials for learning how to use git.\nIf you don’t want to go through the process of creating a full-blown git repository, GitHub also lets you make “gists”, which are single shareable files of code. (See all mine here for examples). Gists are excellent ways to share reproducible examples (or reprexes), and the {reprex} package in R generates output that you can paste directly into a new gist for sharing (see this one, for instance, which I used to show someone how to run and plot logistic regression with R)."
  },
  {
    "objectID": "example/15-example.html#publishing-your-stuff-online",
    "href": "example/15-example.html#publishing-your-stuff-online",
    "title": "Sharing R output online",
    "section": "",
    "text": "The easiest way to get a knitted R Markdown onto the internet is to use RPubs. We talked about this in session 10, and you used it to turn in exercise 10. After knitting an HTML document in RStudio, click on the “Publish” button in the top right corner to upload the document to the RPubs server and get a URL that you can share with others:\n\n\n\nYou don’t have to set up a web server or anything—it’s all pretty automatic and seamless.\n\n\n\nIf you have something slightly more complex, like a collection of R Markdown files that do related things, it’s easy to stitch them all together in an R Markdown website. RStudio supports these automatically—after telling RStudio to consider an RStudio project to be a website, it will knit all the .Rmd files in the root of your project directory every time you click on the “Build Website” button.\nRStudio generates a standalone folder named public with static HTML pages of all your knitted documents. You then have to put that folder on the internet somewhere, either on a web server you have access to, or a free service like Netlify.\nSee this page for complete documentation, or follow these tutorials by Lucy D’Agostino McGowan and Emily Zabor.\nThese websites are especially helpful for standalone projects like research papers and reports. I’ve had students do their master’s capstone projects with these, with specific pages for their introduction, literature review, data cleaning, exploratory data analysis, modeling, and results.\nI typically make a website for each of my research projects and will include pages with IRB details, copies of survey experiments, data cleaning, results, and so on. Here are some examples:\n\nNGO Crackdowns and Philanthropy\nAre Donors Really Responding?\nThe Power of Ranking\nConstraint Closure\n\nYou can also make really neat small websites like Desirée De Leon’s Teacup Giraffes for teaching basic statistics.\n\n\n\nIf you want more control (i.e. total control) over the HTML output and the structure of a website, you can use a package named {blogdown} to convert R Markdown files into an entire website. This course website is built with {blogdown}: you can see all the underlying R Markdown files at GitHub.\nLike R Markdown websites, blogdown generates a complete static version of the knitted website and puts it in a folder named public. You’re then responsible for putting that somewhere on the internet, either on your own server or by using a free hosting service like Netlify.\nBlogdown is incredibly well documented, and there are lots of tutorials for how to get started. Alison Hill’s tutorial here is the best place to get started—follow it and you’ll have a basic blog completely free.\n\n\n\nIf you don’t want to create a website, you can use a package named {bookdown} to stitch a collection of R Markdown files into a PDF, Word, or HTML book. (You could even put all your exercises from this class into a single book!). {bookdown} is incredibly well documented too (as a {bookdown} book), and you can get familiar with it fairly quickly.\nDozens of real-world books, dissertations, and theses have been written with {bookdown}, including both Claus Wilke’s and Kieran Healy’s books from this course. Because of the magic of Markdown, you can create parallel HTML and PDF versions of your book and post one type of output on the internet and print and bind the other one.\n\n\n\nR Markdown isn’t just for PDF, Word, and HTML documents. You can also make slides! All the slides for this course were made in R Markdown with a package named {xaringan}. You can see the documentation here, and see the main example presentation here. You can also see all the R Markdown files I wrote to create the slides for this class here.\n\n\n\nAnd finally, if you want to share code (and keep track of versions of your code), GitHub is one of the best places for that. Posting your code at places like GitHub lets other people see and borrow and adapt and make suggestions to your code. You can see all my different repositories and projects here, for example.\nJenny Bryan has a useful {bookdown} website explaining how to get started, and GitHub itself has excellent materials for learning how to use git.\nIf you don’t want to go through the process of creating a full-blown git repository, GitHub also lets you make “gists”, which are single shareable files of code. (See all mine here for examples). Gists are excellent ways to share reproducible examples (or reprexes), and the {reprex} package in R generates output that you can paste directly into a new gist for sharing (see this one, for instance, which I used to show someone how to run and plot logistic regression with R)."
  },
  {
    "objectID": "example/15-example.html#telling-stories-with-data",
    "href": "example/15-example.html#telling-stories-with-data",
    "title": "Sharing R output online",
    "section": "Telling stories with data",
    "text": "Telling stories with data\nIf you’re interested in learning more about data storytelling and science communication, check out these resources:\n\n BUSM 491R: Telling Stories with Data (BYU, Fall 2017)\n Cole Nussbaumer Knaflic, Storytelling with Data: A Data Visualization Guide for Business Professionals (Hoboken, New Jersey: John Wiley & Sons, Inc., 2015).\n Alan Alda, If I Understood You, Would I Have This Look on My Face? My Adventures in the Art and Science of Relating and Communicating (New York: Random House, 2017).\n Nancy Duarte, Resonate: Present Visual Stories That Transform Audiences (Hoboken, New Jersey: John Wiley & Sons, Inc., 2010).\n “Understanding the way scientists speak,” MSNBC Morning Joe, 2013-04-24\n “Improvisation for Scientists: Workshops by Alan Alda and the Center for Communicating Science,” Stony Brook Journalism, 2010-03-23"
  },
  {
    "objectID": "example/13-example.html",
    "href": "example/13-example.html",
    "title": "Text",
    "section": "",
    "text": "For this example, we’re going to use the text of Little Women by Louisa May Alcott and four Shakespearean tragedies (Romeo and Juliet, King Lear, Macbeth, and Hamlet) to explore how to do some basic text visualization.\nYou can follow along if you want, but don’t feel like you have too. This is mostly just to give you a taste of different methods for visualizing text. It’s by no means comprehensive, but it is well annotated and commented and should (hopefully) be easy to follow.\nIf you want to play with part-of-speech tagging, you can download an already-tagged version of Little Women here (you’ll likely need to right click and choose “Save Link As…”):\nIf you want to see other examples of text visualizations with the {tidytext} package, check out some of these:"
  },
  {
    "objectID": "example/13-example.html#live-coding-example",
    "href": "example/13-example.html#live-coding-example",
    "title": "Text",
    "section": "Live coding example",
    "text": "Live coding example\n\n\n\n\n\n\n\n\n\n\nBig differences from the video\n\n\n\nThis is a highly cleaned up version of the code from the video."
  },
  {
    "objectID": "example/13-example.html#get-data",
    "href": "example/13-example.html#get-data",
    "title": "Text",
    "section": "Get data",
    "text": "Get data\nFirst, as always, we’ll load the libraries we’ll be using:\n\n\nCode\nlibrary(tidyverse)   # For ggplot, dplyr, etc.\nlibrary(tidytext)    # For neat text things\nlibrary(gutenbergr)  # For downloading books from Project Gutenberg\n\n\nWe’re going to use the {gutenbergr} package to download some books directly from Project Gutenberg. The IDs for these books come from the URLs at their website. For instance, Little Women is book #514. We’ll store these books as *_raw and then clean them up later.\n\n\nCode\n# 514 Little Women\nlittle_women_raw &lt;- gutenberg_download(514, meta_fields = \"title\")\n\n# 1524 - Hamlet\n# 1532 - King Lear\n# 1533 - Macbeth\n# 1513 - Romeo and Juliet\ntragedies_raw &lt;- gutenberg_download(c(1524, 1532, 1533, 1513),\n                                    meta_fields = \"title\")\n\n\n\n\n\n\n\n\nDownloading errors\n\n\n\nSometimes the Project Gutenberg server gets too much traffic and goes down temporarily (it’s all run by volunteers!) and you’ll get an error like this:\nWarning: Could not download a book at http://aleph.gutenberg.org/...\nProject Gutenberg has multiple copies of itself on different servers around the world, called “mirrors.” You can see the full list of mirrors here. If you get an error about connecting to the main Project Gutenberg server (aleph.gutenberg.org), you can specify a different mirror with the mirror argument in gutenberg_download():\n\n\nCode\nlittle_women_raw &lt;- gutenberg_download(514, meta_fields = \"title\",\n                                       mirror = \"https://gutenberg.pglaf.org/\")\n\n\n\n\nIf you won’t want to redownload the books every time you knit (you don’t), you can do the same trick we’ve used for WDI and FRED data. Put the actual code for getting the books in a chunk with eval=FALSE on it and run it manually in RStudio when you want to get the data. Then you can write the downloaded data as a CSV file, and then load it invisibly from the CSV file when you knit:\nI first download data from Project Gutenberg:\n\n```{r get-book, eval=FALSE}\nbooks_raw &lt;- gutenberg_download(...)\n\nwrite_csv(books_raw, \"data/books_raw.csv\")\n```\n\n```{r load-book-data-real, include=FALSE}\nbooks_raw &lt;- read_csv(\"data/books_raw.csv\")\n```"
  },
  {
    "objectID": "example/13-example.html#clean-data",
    "href": "example/13-example.html#clean-data",
    "title": "Text",
    "section": "Clean data",
    "text": "Clean data\nThe data you get from Project Gutenberg comes in a tidy format, with a column for the book id, a column for the title, and a column for text. Sometimes this text column will be divided by lines in the book; sometimes it might be an entire page or paragraph or chapter. It all depends on how the book is formatted at Project Gutenberg.\nHere’s what the start of our little_women_raw data looks like:\n\n\nCode\nhead(little_women_raw)\n## # A tibble: 6 × 3\n##   gutenberg_id text                   title       \n##          &lt;int&gt; &lt;chr&gt;                  &lt;chr&gt;       \n## 1          514 \"Little Women\"         Little Women\n## 2          514 \"\"                     Little Women\n## 3          514 \"by Louisa May Alcott\" Little Women\n## 4          514 \"\"                     Little Women\n## 5          514 \"\"                     Little Women\n## 6          514 \"Contents\"             Little Women\n\n\nIf we look at the data in RStudio, we can see that the actual book doesn’t start until row 70 (the first 69 rows are the table of contents and other parts of the front matter).\n\n\n\n\n\n\nEvery book is different!\n\n\n\nIn this case, Little Women starts at row 67. That will not be true for all books! Every book is unique and has different amounts of front matter. You cannot assume that any book you work with starts at line 67.\n\n\nIt would be nice if we had a column that indicated what chapter each line is in, since we could then group by chapter and look at patterns within chapters. Since the data doesn’t come with a chapter column, we have to make one ourselves using a fun little trick. Each chapter in the book starts with “CHAPTER ONE” or “CHAPTER TWO”, with “chapter” in ALL CAPS. We can make a variable named chapter_start that will be true if a line starts with “CHAPTER” and false if not. Then we can use the cumsum() function to take the cumulative sum of this column, which will increment up one number ever time there’s a new chapter, thus creating a helpful chapter column.\n\n\nCode\n# Clean up Little Women\nlittle_women &lt;- little_women_raw %&gt;% \n  # The actual book doesn't start until line 67\n  slice(67:n()) %&gt;% \n  # Get rid of rows where text is missing\n  drop_na(text) %&gt;% \n  # Chapters start with CHAPTER X, so mark if each row is a chapter start\n  # cumsum() calculates the cumulative sum, so it'll increase every time there's\n  # a new chapter and automatically make chapter numbers\n  mutate(chapter_start = str_detect(text, \"^CHAPTER\"),\n         chapter_number = cumsum(chapter_start)) %&gt;% \n  # Get rid of these columns\n  select(-gutenberg_id, -title, -chapter_start)\n\nhead(little_women)\n## # A tibble: 6 × 2\n##   text                                                                      chapter_number\n##   &lt;chr&gt;                                                                              &lt;int&gt;\n## 1 \"CHAPTER ONE\"                                                                          1\n## 2 \"PLAYING PILGRIMS\"                                                                     1\n## 3 \"\"                                                                                     1\n## 4 \"\"                                                                                     1\n## 5 \"“Christmas won’t be Christmas without any presents,” grumbled Jo, lying\"              1\n## 6 \"on the rug.\"                                                                          1\n\n\nThe data from Shakespeare is similarly messy, with just three columns:\n\n\nCode\nhead(tragedies_raw)\n## # A tibble: 6 × 3\n##   gutenberg_id text                              title           \n##          &lt;int&gt; &lt;chr&gt;                             &lt;chr&gt;           \n## 1         1513 \"THE TRAGEDY OF ROMEO AND JULIET\" Romeo and Juliet\n## 2         1513 \"\"                                Romeo and Juliet\n## 3         1513 \"\"                                Romeo and Juliet\n## 4         1513 \"\"                                Romeo and Juliet\n## 5         1513 \"by William Shakespeare\"          Romeo and Juliet\n## 6         1513 \"\"                                Romeo and Juliet\n\n\nThe initial text sometimes isn’t the actual text of the book. If you look at the beginning of Hamlet, for instance, there’s a bunch of introductory stuff from editors and transcribers. In real life, we’d want to find a systematic way to get rid of that (perhaps by looking at how many introductory rows there are in each of the four plays and removing those rows), but for now, we’ll just live with it and pretend Shakespeare wrote these notes. 🤷\nWe could also figure out a systematic way to indicate acts and scenes, but that’s tricky, so we won’t for this example. (This guy did though!)\nNow that we have tidy text data, let’s do stuff with it!"
  },
  {
    "objectID": "example/13-example.html#tokens-and-word-counts",
    "href": "example/13-example.html#tokens-and-word-counts",
    "title": "Text",
    "section": "Tokens and word counts",
    "text": "Tokens and word counts\n\nSingle words\nOne way we can visualize text is to look at word frequencies and find the most common words. This is even more important when looking across documents.\nRight now the text we have is tidy, but it is based on lines of text, not words. In order to count words correctly, we need each token (or text element, whether it be a word or bigram or paragraph or whatever) to be in its own row. The unnest_tokens() functions from {tidytext} does this for us. The first argument is the name of the column we want to create; the second argument is the name of the column we want to split into tokens.\nLet’s just work with the Shakespeare tragedies:\n\n\nCode\ntragedies_words &lt;- tragedies_raw %&gt;% \n  drop_na(text) %&gt;% \n  unnest_tokens(word, text)\n\nhead(tragedies_words)\n## # A tibble: 6 × 3\n##   gutenberg_id title            word   \n##          &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;  \n## 1         1513 Romeo and Juliet the    \n## 2         1513 Romeo and Juliet tragedy\n## 3         1513 Romeo and Juliet of     \n## 4         1513 Romeo and Juliet romeo  \n## 5         1513 Romeo and Juliet and    \n## 6         1513 Romeo and Juliet juliet\n\n\nNow that we have words, we can filter and count the words. Here’s what’s happening in this next chunk:\n\nWe use anti_join() to remove all common stop words like “a” and “the” that are listed in the stop_words dataset that is loaded when you load {tidytext}\nWe count how many times each word appears in each title/play\nWe only keep the top 15 words\n\n\n\nCode\ntop_words_tragedies &lt;- tragedies_words %&gt;% \n  # Remove stop words\n  anti_join(stop_words) %&gt;% \n  # Get rid of old timey words and stage directions\n  filter(!(word %in% c(\"thou\", \"thy\", \"haue\", \"thee\", \n                      \"thine\", \"enter\", \"exeunt\", \"exit\"))) %&gt;% \n  # Count all the words in each play\n  count(title, word, sort = TRUE) %&gt;% \n  # Keep top 15 in each play\n  group_by(title) %&gt;% \n  top_n(15) %&gt;% \n  ungroup() %&gt;% \n  # Make the words an ordered factor so they plot in order\n  mutate(word = fct_inorder(word))\ntop_words_tragedies\n## # A tibble: 62 × 3\n##    title                     word           n\n##    &lt;chr&gt;                     &lt;fct&gt;      &lt;int&gt;\n##  1 Hamlet, Prince of Denmark hamlet       461\n##  2 Romeo and Juliet          romeo        300\n##  3 Macbeth                   macbeth      282\n##  4 The Tragedy of King Lear  lear         229\n##  5 Hamlet, Prince of Denmark lord         223\n##  6 Hamlet, Prince of Denmark king         196\n##  7 Romeo and Juliet          juliet       178\n##  8 The Tragedy of King Lear  kent         170\n##  9 The Tragedy of King Lear  gloucester   169\n## 10 Hamlet, Prince of Denmark horatio      156\n## # ℹ 52 more rows\n\n\nNow we can plot these results, facetting and filling by title:\n\n\nCode\nggplot(top_words_tragedies, aes(y = fct_rev(word), x = n, fill = title)) + \n  geom_col() + \n  guides(fill = \"none\") +\n  labs(y = \"Count\", x = NULL, \n       title = \"15 most frequent words in four Shakespearean tragedies\") +\n  facet_wrap(vars(title), scales = \"free_y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThese results aren’t terribly surprising. “lear” is the most common word in King Lear, “macbeth” is the most common word in Macbeth, and so on. But the results are still really neat! This is a wordcloud for grownups!\n(Sharp-eyed readers will notice that the words aren’t actually in perfect order! That’s because some common words are repeated across the plays, like “lord” and “sir”. However, each category in a factor can only have one possible position in the orer, so because “lord” is the second most common word in Hamlet it also appears as #2 in Macbeth and King Lear. You can fix this with the reorder_within() function in {tidytext}—see Julia Silge’s tutorial here for how to use it.)\n\n\nBigrams\nWe can also look at pairs of words instead of single words. To do this, we need to change a couple arguments in unnest_tokens(), but otherwise everything else stays the same. In order to remove stopwords, we need to split the bigram column into two columns (word1 and word2) with separate(), filter each of those columns, and then combine the word columns back together as bigram with unite()\n\n\nCode\ntragedies_bigrams &lt;- tragedies_raw %&gt;% \n  drop_na(text) %&gt;% \n  # n = 2 here means bigrams. We could also make trigrams (n = 3) or any type of n-gram\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %&gt;% \n  # Get rid of NAs in the new bigram column\n  drop_na(bigram) %&gt;% \n  # Split the bigrams into two words so we can remove stopwords\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %&gt;% \n  filter(!word1 %in% stop_words$word,\n         !word2 %in% stop_words$word) %&gt;% \n  filter(!word1 %in% c(\"thou\", \"thy\", \"thine\", \"enter\", \"exeunt\", \"exit\"),\n         !word2 %in% c(\"thou\", \"thy\", \"thine\", \"enter\", \"exeunt\", \"exit\")) %&gt;% \n  # Put the two word columns back together\n  unite(bigram, word1, word2, sep = \" \")\ntragedies_bigrams\n## # A tibble: 10,829 × 3\n##    gutenberg_id title            bigram             \n##           &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;              \n##  1         1513 Romeo and Juliet william shakespeare\n##  2         1513 Romeo and Juliet scene ii           \n##  3         1513 Romeo and Juliet scene iii          \n##  4         1513 Romeo and Juliet capulet’s house    \n##  5         1513 Romeo and Juliet scene iv           \n##  6         1513 Romeo and Juliet capulet’s house    \n##  7         1513 Romeo and Juliet act ii             \n##  8         1513 Romeo and Juliet adjoining capulet’s\n##  9         1513 Romeo and Juliet capulet’s garden   \n## 10         1513 Romeo and Juliet scene ii           \n## # ℹ 10,819 more rows\n\ntop_bigrams &lt;- tragedies_bigrams %&gt;% \n  # Count all the bigrams in each play\n  count(title, bigram, sort = TRUE) %&gt;% \n  # Keep top 15 in each play\n  group_by(title) %&gt;% \n  top_n(15) %&gt;% \n  ungroup() %&gt;% \n  # Make the bigrams an ordered factor so they plot in order\n  mutate(bigram = fct_inorder(bigram))\n## Selecting by n\n\nggplot(top_bigrams, aes(y = fct_rev(bigram), x = n, fill = title)) + \n  geom_col() + \n  guides(fill = \"none\") +\n  labs(y = \"Count\", x = NULL, \n       title = \"15 most frequent bigrams in four Shakespearean tragedies\") +\n  facet_wrap(vars(title), scales = \"free\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThere are some neat trends here. “Lord Hamlet” is the most common pair of words in Hamlet (not surprisingly), but in Macbeth the repeated “knock knock” (the first non-name repeated pair) is a well-known plot point and reoccurring symbolic theme throughout the play."
  },
  {
    "objectID": "example/13-example.html#bigrams-and-probability",
    "href": "example/13-example.html#bigrams-and-probability",
    "title": "Text",
    "section": "Bigrams and probability",
    "text": "Bigrams and probability\nWe can replicate the “She Giggles, He Gallops” idea by counting the bigrams that match “he X” and “she X”.\nThe log ratio idea shows how much more likely a word is compared to its counterpart (so “he that” is about 5 more likely to appear than “she that”. In this graph, I replaced the x-axis labels with “2x” and “4x”, but without those, you get numbers like 1, 2, and 3 (or -1, -2, -3)). To convert those logged ratio numbers into the multiplicative version (i.e. 2x instead of 1), raise 2 to the power of the log ratio. If the log ratio is 3, the human-readable version is \\(2^3\\), or 8 times.\n\n\nCode\n# Take the log of 8:\nlog2(8)\n## [1] 3\n\n# Reverse log of 3:\n2^3\n## [1] 8\n\n\nThe only text wizardry here is tokenizing the words. Pretty much the rest of all this code is just {dplyr} mutating, filtering, and counting:\n\n\nCode\npronouns &lt;- c(\"he\", \"she\")\n\nbigram_he_she_counts &lt;- tragedies_raw %&gt;%\n  drop_na(text) %&gt;% \n  # Split into bigrams\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %&gt;%\n  # Find counts of bigrams\n  count(bigram, sort = TRUE) %&gt;%\n  # Split the bigram column into two columns\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %&gt;%\n  # Only choose rows where the first word is he or she\n  filter(word1 %in% pronouns) %&gt;%\n  count(word1, word2, wt = n, sort = TRUE) %&gt;% \n  rename(total = n)\n\nword_ratios &lt;- bigram_he_she_counts %&gt;%\n  # Look at each of the second words\n  group_by(word2) %&gt;%\n  # Only choose rows where the second word appears more than 10 times\n  filter(sum(total) &gt; 10) %&gt;%\n  ungroup() %&gt;%\n  # Spread out the word1 column so that there's a column named \"he\" and one named \"she\"\n  spread(word1, total, fill = 0) %&gt;%\n  # Add 1 to each number so that logs work (just in case any are zero)\n  mutate_if(is.numeric, ~(. + 1) / sum(. + 1)) %&gt;%\n  # Create a new column that is the logged ratio of the she counts to he counts\n  mutate(logratio = log2(she / he)) %&gt;%\n  # Sort by that ratio\n  arrange(desc(logratio))\n\n# Rearrange this data so it's plottable\nplot_word_ratios &lt;- word_ratios %&gt;%\n  # This gets the words in the right order---we take the absolute value, select\n  # only rows where the log ratio is bigger than 0, and then take the top 15 words\n  mutate(abslogratio = abs(logratio)) %&gt;%\n  group_by(logratio &lt; 0) %&gt;%\n  top_n(15, abslogratio) %&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word2, logratio)) \n\n# Finally we plot this\nggplot(plot_word_ratios, aes(y = word, x = logratio, color = logratio &lt; 0)) +\n  geom_segment(aes(y = word, yend = word,\n                   x = 0, xend = logratio), \n               linewidth = 1.1, alpha = 0.6) +\n  geom_point(size = 3.5) +\n  labs(x = \"How much more/less likely\", y = NULL) +\n  scale_color_discrete(name = \"\", labels = c(\"More 'she'\", \"More 'he'\")) +\n  scale_x_continuous(breaks = seq(-3, 3),\n                     labels = c(\"8x\", \"4x\", \"2x\",\n                                \"Same\", \"2x\", \"4x\", \"8x\")) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nShakespeare doesn’t use a lot of fancy verbs in his plays, so we’re left with incredibly common verbs like “should” and “comes” and “was”. Oh well."
  },
  {
    "objectID": "example/13-example.html#term-frequency-inverse-document-frequency-tf-idf",
    "href": "example/13-example.html#term-frequency-inverse-document-frequency-tf-idf",
    "title": "Text",
    "section": "Term frequency-inverse document frequency (tf-idf)",
    "text": "Term frequency-inverse document frequency (tf-idf)\nWe can determine which words are the most unique for each book/document in our corpus using by calculating the tf-idf (term frequency-inverse document frequency) score for each term. The tf-idf is the product of the term frequency and the inverse document frequency:\n\\[\n\\begin{aligned}\n\\operatorname{tf}(\\text{term}) &= \\frac{n_{\\text{term}}}{n_{\\text{terms in document}}} \\\\\n\\operatorname{idf}(\\text{term}) &= \\ln{\\left(\\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}}\\right)} \\\\\n\\operatorname{tf-idf}(\\text{term}) &= \\operatorname{tf}(\\text{term}) \\times \\operatorname{idf}(\\text{term})\n\\end{aligned}\n\\]\nFortunately you don’t need to remember that formula. The bind_tf_idf() function will calculate this for you. Remember, the higher the tf-idf number, the more unique the term is in the document, but these numbers are meaningless and unitless—you can’t convert them to a percentage or anything.\nHere are the most unique words in these four tragedies, compared to all the tragedies:\n\n\nCode\ntragedy_words &lt;- tragedies_raw %&gt;% \n  drop_na() %&gt;% \n  # Split into word tokens\n  unnest_tokens(word, text) %&gt;% \n  # Remove stop words and old timey words\n  anti_join(stop_words) %&gt;% \n  filter(!word %in% c(\"thou\", \"thy\", \"haue\", \"thee\", \n                      \"thine\", \"enter\", \"exeunt\", \"exit\")) %&gt;% \n  count(title, word, sort = TRUE)\n\n# Add the tf-idf values to the counts\ntragedy_tf_idf &lt;- tragedy_words %&gt;% \n  bind_tf_idf(word, title, n)\n\n# Get the top 10 uniquest words\ntragedy_tf_idf_plot &lt;- tragedy_tf_idf %&gt;% \n  arrange(desc(tf_idf)) %&gt;% \n  group_by(title) %&gt;% \n  top_n(10) %&gt;% \n  ungroup() %&gt;% \n  mutate(word = fct_inorder(word))\n\nggplot(tragedy_tf_idf_plot, \n       aes(y = fct_rev(word), x = tf_idf, fill = title)) +\n  geom_col() +\n  guides(fill = \"none\") +\n  labs(x = \"tf-idf\", y = NULL) +\n  facet_wrap(~ title, scales = \"free\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nNot surprisingly, the most unique words for each play happen to be the names of the characters in those plays."
  },
  {
    "objectID": "example/13-example.html#sentiment-analysis",
    "href": "example/13-example.html#sentiment-analysis",
    "title": "Text",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\nIn the video, I plotted the sentiment of Little Women across the book, but it wasn’t a very interesting plot. We’ll try with Shakespeare here instead.\nAt its core, sentiment analysis involves looking at a big list of words for how negative or positive they are. Some sentiment dictionaries mark if a word is “negative” or “positive”; some give words a score from -3 to 3; some give different emotions like “sadness” or “anger”. You can see what the different dictionaries look like with get_sentiments()\n\n\nCode\nget_sentiments(\"afinn\")  # Scoring system\n## # A tibble: 2,477 × 2\n##    word       value\n##    &lt;chr&gt;      &lt;dbl&gt;\n##  1 abandon       -2\n##  2 abandoned     -2\n##  3 abandons      -2\n##  4 abducted      -2\n##  5 abduction     -2\n##  6 abductions    -2\n##  7 abhor         -3\n##  8 abhorred      -3\n##  9 abhorrent     -3\n## 10 abhors        -3\n## # ℹ 2,467 more rows\n# get_sentiments(\"bing\")  # Negative/positive\n# get_sentiments(\"nrc\")  # Specific emotions\n# get_sentiments(\"loughran\")  # Designed for financial statements; positive/negative\n\n\nHere we split the Shakespearean tragedies into words, join a sentiment dictionary to it, and use {dplyr} data wrangling to calculate the net number positive words in each chapter. Had we used the AFINN library, we could calculate the average sentiment per chapter, since AFINN uses a scoring system instead of negative/positive labels. Or we could’ve used the NRC library, which has specific emotions like trust and fear.\n\n\nCode\ntragedy_words &lt;- tragedies_raw %&gt;% \n  drop_na() %&gt;% \n  # Split into word tokens\n  unnest_tokens(word, text) %&gt;% \n  # Remove stop words and old timey words\n  anti_join(stop_words) %&gt;% \n  filter(!word %in% c(\"thou\", \"thy\", \"haue\", \"thee\", \n                      \"thine\", \"enter\", \"exeunt\", \"exit\"))\n\n# Join the sentiment dictionary \ntragedy_sentiment &lt;- tragedy_words %&gt;% \n  inner_join(get_sentiments(\"bing\"), relationship = \"many-to-many\")\ntragedy_sentiment\n## # A tibble: 7,637 × 4\n##    gutenberg_id title            word       sentiment\n##           &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;      &lt;chr&gt;    \n##  1         1513 Romeo and Juliet tragedy    negative \n##  2         1513 Romeo and Juliet dignity    positive \n##  3         1513 Romeo and Juliet fair       positive \n##  4         1513 Romeo and Juliet grudge     negative \n##  5         1513 Romeo and Juliet break      negative \n##  6         1513 Romeo and Juliet unclean    negative \n##  7         1513 Romeo and Juliet fatal      negative \n##  8         1513 Romeo and Juliet overthrows negative \n##  9         1513 Romeo and Juliet death      negative \n## 10         1513 Romeo and Juliet strife     negative \n## # ℹ 7,627 more rows\n\n\nWe can look at these sentiments a few different ways. First we can get a count of total positive and negative words in the four books. We can see that in all four, there are more negative words than positive ones (they’re tragdies, after all):\n\n\nCode\ntragedy_sentiment_plot &lt;- tragedy_sentiment %&gt;% \n  count(title, sentiment)\n\nggplot(tragedy_sentiment_plot, aes(x = sentiment, y = n, fill = title, alpha = sentiment)) +\n  geom_col(position = position_dodge()) +\n  scale_alpha_manual(values = c(0.5, 1)) +\n  facet_wrap(vars(title)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nPerhaps more usefully, we can divide each of the plays into groups of 100 lines, and then get the net sentiment of each group (number of positive words − number of negative words). By splitting the data into groups of lines, we can show a more granular view of the progression of the plot. To do this we make a column that indicates the row number, and then we use the special %/% operator to perform integer division, which essentially lops off the decimal point when dividing numbers: 150/100 normally is 1.5, but in integer divison, it is 1. This is a helpful trick for putting rows 1-99 in one group, then rows 100-199 in another group, etc.\n\n\nCode\ntragedies_split_into_lines &lt;- tragedy_sentiment %&gt;% \n  # Divide lines into groups of 100\n  mutate(line = row_number(),\n         line_chunk = line %/% 100) %&gt;% \n  # Get a count of postiive and negative words in each 100-line chunk in each play\n  count(title, line_chunk, sentiment) %&gt;% \n  # Convert the sentiment column into two columns named \"positive\" and \"negative\"\n  pivot_wider(names_from = sentiment, values_from = n) %&gt;% \n  # Calculate net sentiment\n  mutate(sentiment = positive - negative)\n\nggplot(tragedies_split_into_lines,\n       aes(x = line_chunk, y = sentiment, fill = sentiment)) +\n  geom_col() +\n  scale_fill_viridis_c(option = \"magma\", end = 0.9) +\n  facet_wrap(vars(title), scales = \"free_x\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nNeat. They’re all really sad and negative, except for the beginning of Romeo and Juliet where the two lovers meet and fall in love. Then everyone dies later."
  },
  {
    "objectID": "example/13-example.html#neat-extra-stuff",
    "href": "example/13-example.html#neat-extra-stuff",
    "title": "Text",
    "section": "Neat extra stuff",
    "text": "Neat extra stuff\nNone of this stuff was in the video, but it’s useful to know and see how to do it. It all generally comes from the Tidy Text Mining book by Julia Silge and David Robinson\n\nPart of speech tagging\nR has no way of knowing if words are nouns, verbs, or adjectives. You can algorithmically predict what part of speech each word is using a part-of-speech tagger, like spaCy or Stanford’s Natural Langauge Processing (NLP) library.\nThese are external programs that are not written in R and don’t naturally communicate with R (spaCy is written in Python; Stanford’s CoreNLP is written in Java). There is a helpful R package named {cleanNLP} that helps you interact with these programs from within R, whis is super helpful. {cleanNLP} also comes with its own R-only tagger so you don’t need to install anything with Python or Java (however, it’s not as powerful as either spaCy, which is faster, and doesn’t deal with foreign languages like Arabic and Chinese like Stanford’s NLP library).\nYou can see other examples of part-of-speech tagging (along with instructions for how to install spaCy and coreNLP) here:\n\n “Tidy text, parts of speech, and unique words in the Bible”\n “Tidy text, parts of speech, and unique words in the Qur’an”\n\nHere’s the general process for tagging (or “annotating”) text with the {cleanNLP} package:\n\nMake a dataset where one column is the id (line number, chapter number, book+chapter, etc.), and another column is the text itself.\nInitialize the NLP tagger. You can use any of these:\n\ncnlp_init_udpipe(): Use an R-only tagger that should work without installing anything extra (a little slower than the others, but requires no extra steps!)\ncnlp_init_spacy(): Use spaCy (if you’ve installed it on your computer with Python)\ncnlp_init_corenlp(): Use Stanford’s NLP library (if you’ve installed it on your computer with Java)\n\nFeed the data frame from step 1 into the cnlp_annotate() function and wait.\nSave the tagged data on your computer so you don’t have to re-tag it every time.\n\nHere’s an example using the Little Women data:\n\n\nCode\n# For the tagger to work, each row needs to be unique, which means we need to\n# combine all the text into individual chapter-based rows. This takes a little\n# bit of text-wrangling with dplyr:\nlittle_women_to_tag &lt;- little_women %&gt;% \n  # Group by chapter number\n  group_by(chapter_number) %&gt;% \n  # Take all the rows in each chapter and collapse them into a single cell\n  nest(data = c(text)) %&gt;% \n  ungroup() %&gt;% \n  # Look at each individual cell full of text lines and paste them together into\n  # one really long string of text per chapter\n  mutate(text = map_chr(data, ~paste(.$text, collapse = \" \"))) %&gt;% \n  # Get rid of this column\n  select(-data)\nlittle_women_to_tag\n## # A tibble: 47 × 2\n##    chapter_number text                                                                                                                                \n##             &lt;int&gt; &lt;chr&gt;                                                                                                                               \n##  1              1 \"CHAPTER ONE PLAYING PILGRIMS   “Christmas won’t be Christmas without any presents,” grumbled Jo, lying on the rug.  “It’s so dread…\n##  2              2 \"CHAPTER TWO A MERRY CHRISTMAS   Jo was the first to wake in the gray dawn of Christmas morning. No stockings hung at the fireplace…\n##  3              3 \"CHAPTER THREE THE LAURENCE BOY   “Jo! Jo! Where are you?” cried Meg at the foot of the garret stairs.  “Here!” answered a husky vo…\n##  4              4 \"CHAPTER FOUR BURDENS   “Oh, dear, how hard it does seem to take up our packs and go on,” sighed Meg the morning after the party, f…\n##  5              5 \"CHAPTER FIVE BEING NEIGHBORLY   “What in the world are you going to do now, Jo?” asked Meg one snowy afternoon, as her sister came…\n##  6              6 \"CHAPTER SIX BETH FINDS THE PALACE BEAUTIFUL   The big house did prove a Palace Beautiful, though it took some time for all to get …\n##  7              7 \"CHAPTER SEVEN AMY’S VALLEY OF HUMILIATION   “That boy is a perfect cyclops, isn’t he?” said Amy one day, as Laurie clattered by on…\n##  8              8 \"CHAPTER EIGHT JO MEETS APOLLYON   “Girls, where are you going?” asked Amy, coming into their room one Saturday afternoon, and find…\n##  9              9 \"CHAPTER NINE MEG GOES TO VANITY FAIR   “I do think it was the most fortunate thing in the world that those children should have th…\n## 10             10 \"CHAPTER TEN THE P.C. AND P.O.   As spring came on, a new set of amusements became the fashion, and the lengthening days gave long …\n## # ℹ 37 more rows\n\n\nNotice how there’s now a row for each chapter, and the whole chapter is contained in the text column. With the data in this format, we can annotate it. It takes 75 seconds to run this on my 2021 MacBook Pro with the R-only udpipe tagger (and only 30 seconds if I use the spaCy tagger). Notice how I immediately save the tagged tokens as a CSV file after so I don’t have to do it again.\n\n\nCode\nlibrary(cleanNLP)\n\n# Use the built-in R-based tagger\ncnlp_init_udpipe()\n\nlittle_women_tagged_raw &lt;- cnlp_annotate(little_women_to_tag, \n  text_name = \"text\", \n  doc_name = \"chapter_number\")\n\n# Save the tagged token dataframe so we don't have to run this again\nwrite_csv(little_women_tagged_raw$token, \"data/little_women_tagged.csv\")\n\n# Load the tagged tokens\nlittle_women_tagged &lt;- read_csv(\"data/little_women_tagged.csv\")\n\n\nHere’s what the tagged text looks like:\n\n\nCode\nlittle_women_tagged\n## # A tibble: 231,564 × 10\n##    doc_id   sid   tid token       token_with_ws lemma       upos  xpos  tid_source relation\n##     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;   \n##  1      1     1     1 \"CHAPTER\"   \"CHAPTER \"    \"chapter\"   NOUN  NN             4 nmod    \n##  2      1     1     2 \"ONE\"       \"ONE \"        \"one\"       NUM   CD             1 nummod  \n##  3      1     1     3 \"PLAYING\"   \"PLAYING \"    \"playing\"   NOUN  NN             4 compound\n##  4      1     1     4 \"PILGRIMS\"  \"PILGRIMS \"   \"PILGRIMS\"  PROPN NNP           10 npadvmod\n##  5      1     1     5 \"  \"        \"  \"          \"  \"        SPACE _SP            4 dep     \n##  6      1     1     6 \"“\"         \"“\"           \"\\\"\"        PUNCT ``            10 punct   \n##  7      1     1     7 \"Christmas\" \"Christmas \"  \"Christmas\" PROPN NNP           10 nsubj   \n##  8      1     1     8 \"wo\"        \"wo\"          \"will\"      AUX   MD            10 aux     \n##  9      1     1     9 \"n’t\"       \"n’t \"        \"not\"       PART  RB            10 neg     \n## 10      1     1    10 \"be\"        \"be \"         \"be\"        AUX   VB            17 ccomp   \n## # ℹ 231,554 more rows\n\n\nThere are a bunch of new columns like lemma (or the base stemmed word), and upos and pos for the different parts of speech. These use the Penn Treebank codes.\nNow that everything is tagged, we can do any grouping and summarizing and filtering we want. We could find the most common verbs, or the most common nouns or proper names, for instance. Here’s a fun plot that shows the proportion of mentions of the four main characters (Meg, Jo, Beth, and Amy) in each chapter.\n\n\nCode\n# Find all proper nouns\nproper_nouns &lt;- little_women_tagged %&gt;% \n  filter(upos == \"PROPN\")\n\nmain_characters_by_chapter &lt;- proper_nouns %&gt;% \n  # Find only Meg, Jo, Beth, and Amy\n  filter(lemma %in% c(\"Meg\", \"Jo\", \"Beth\", \"Amy\")) %&gt;% \n  # Group by chapter and character name\n  group_by(doc_id, lemma) %&gt;% \n  # Get the count of mentions\n  summarize(n = n()) %&gt;% \n  # Make a new column named \"name\" that is an ordered factor of the girls' names\n  mutate(name = factor(lemma, levels = c(\"Meg\", \"Jo\", \"Beth\", \"Amy\"), ordered = TRUE)) %&gt;% \n  # Rename this so it's called chapter\n  rename(chapter = doc_id) %&gt;% \n  # Group by chapter\n  group_by(chapter) %&gt;% \n  # Calculate the proportion of each girl's mentions in each chapter\n  mutate(prop = n / sum(n)) %&gt;% \n  ungroup() %&gt;% \n  # Make a cleaner chapter name column\n  mutate(chapter_name = paste(\"Chapter\", chapter)) %&gt;% \n  mutate(chapter_name = fct_inorder(chapter_name))\nmain_characters_by_chapter\n## # A tibble: 177 × 6\n##    chapter lemma     n name   prop chapter_name\n##      &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; &lt;fct&gt;       \n##  1       1 Amy      22 Amy   0.188 Chapter 1   \n##  2       1 Beth     26 Beth  0.222 Chapter 1   \n##  3       1 Jo       43 Jo    0.368 Chapter 1   \n##  4       1 Meg      26 Meg   0.222 Chapter 1   \n##  5       2 Amy      13 Amy   0.197 Chapter 2   \n##  6       2 Beth     12 Beth  0.182 Chapter 2   \n##  7       2 Jo       21 Jo    0.318 Chapter 2   \n##  8       2 Meg      20 Meg   0.303 Chapter 2   \n##  9       3 Amy       2 Amy   0.02  Chapter 3   \n## 10       3 Beth      2 Beth  0.02  Chapter 3   \n## # ℹ 167 more rows\n\n\nAnd here’s the polished plot:\n\n\nCode\nggplot(main_characters_by_chapter, aes(x = prop, y = \"1\", fill = fct_rev(name))) + \n  geom_col(position = position_stack()) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.9, name = NULL) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(x = NULL, y = NULL,\n       title = \"Proportion of mentions of each\\nLittle Woman per chapter\",\n       subtitle = \"Jo basically dominates the last third of the book\") +\n  facet_wrap(vars(chapter_name), nrow = 6) +\n  theme_bw(base_family = \"Roboto Condensed\") +\n  theme(legend.position = \"top\",\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        strip.background = element_rect(fill = \"white\"),\n        legend.text = element_text(face = \"bold\", size = rel(1)),\n        plot.title = element_text(face = \"bold\", hjust = 0.5, size = rel(1.7)),\n        plot.subtitle = element_text(hjust = 0.5, size = rel(1.1)))\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not found in Windows font database\n\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not found in Windows font database\n\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\nTopic modeling and fingerprinting\nIf you want to see some examples of topic modeling with Latent Dirichlet Allocation (LDA) or text fingerprinting based on sentence length and counts of hapax legomena (based on this article), see these examples from a previous version of this class: topic modeling and fingerprinting.\n\n\nText features\nFinally, you can use the {textfeatures} package to find all sorts of interesting numeric statistics about text, like the number of exclamation points, commas, digits, characters per word, uppercase letters, lowercase letters, and more!"
  },
  {
    "objectID": "example/11-example.html",
    "href": "example/11-example.html",
    "title": "Time",
    "section": "",
    "text": "For this example, we’re going to use economic data from the US Federal Reserve (the Fed). The St. Louis Fed is in charge of publishing Fed economic data, and they host it all at an online portal named FRED. Instead of downloading individual time series data from the FRED website, we’ll do what with did with the World Bank WDI data and download it directly from the internet with the {tidyquant} package, which includes a function for working with the FRED API/website.\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):"
  },
  {
    "objectID": "example/11-example.html#live-coding-example",
    "href": "example/11-example.html#live-coding-example",
    "title": "Time",
    "section": "Live coding example",
    "text": "Live coding example\n\n\n\n\n\n\n\n\n\n\nSlight differences from the video\n\n\n\nThis is a slightly cleaned up version of the code from the video."
  },
  {
    "objectID": "example/11-example.html#get-data",
    "href": "example/11-example.html#get-data",
    "title": "Time",
    "section": "Get data",
    "text": "Get data\nFirst, we load the libraries we’ll be using:\n\n\nCode\nlibrary(tidyverse)  # For ggplot, dplyr, and friends\nlibrary(tidyquant)  # For accessing FRED data\nlibrary(scales)     # For nicer labels\n\n\nThe US Federal Reserve provides thousands of economic datasets at FRED. We can use the {tidyquant} R package to access their servers and download the data directly into R.\nLike we did with the WDI indicators in session 8, we need to find the special internal code for the variables we want to get. We need to pay close attention to the details of each variable, since the same measure can be offered with different combinations of real (adjusted for inflation) or nominal (not adjusted for inflation); monthly, quarterly, or annually; and seasonally adjusted or not seasonally adjusted. For instance, if you want to see US GDP, here are some possibilities (all the possible GDP measures are listed here):\n\nGDPC1: Real (2012 dollars), quarterly, seasonally adjusted\nND000334Q: Real (2012 dollars), quarterly, not seasonally adjusted\nGDPCA: Real (2012 dollars), annual, not seasonally adjusted\nGDP: Nominal, quarterly, seasonally adjusted\nGDPA: Nominal, annual, not seasonally adjusted\n\nThe code for getting data from FRED works a little differently than WDI(), and the output is a little different too, but it’s hopefully not too complicated. We need to feed the tq_get() function (1) a list of indicators we want, (2) a source for those indicators, and (3) a starting and/or ending date.\ntq_get() can actually get data from a ton of different sources like stocks from Yahoo Finance and general financial data from Bloomberg, Quandl, and Tiingo. Most of those other sources require a subscription and a fancy API key that logs you into their servers when getting data, but FRED is free (yay public goods!).\nWe’ll first make a new dataset named fred_raw that gets a bunch of interesting variables from FRED from January 1, 1990 until today.\n\n\nCode\nfred_raw &lt;- tq_get(c(\"RSXFSN\",  # Advance retail sales\n                     \"GDPC1\",  # GDP\n                     \"ICSA\",  # Initial unemployment claims\n                     \"FPCPITOTLZGUSA\",  # Inflation\n                     \"UNRATE\",  # Unemployment rate\n                     \"USREC\"),  # Recessions\n                   get = \"economic.data\",  # Use FRED\n                   from = \"1990-01-01\")\n\n\nDownloading data from FRED every time you knit will get tedious and take a long time (plus if their servers are temporarily down, you won’t be able to get the data). As with the World Bank data we used, it’s good practice to save this raw data as a CSV file and then work with that.\nSince we care about reproducibility, we still want to include the code we used to get data from FRED, we just don’t want it to actually run. You can include chunks but not run them by setting eval=FALSE in the chunk options. In this little example, we show the code for downloading the data, but we don’t evaluate the chunk. We then include a chunk that loads the data from a CSV file with read_csv(), but we don’t include it (include=FALSE). That way, in the knitted file we see the WDI() code, but in reality it’s loading the data from CSV. Super tricky.\nI first download data from FRED:\n\n```{r get-fred-data, eval=FALSE}\nfred_raw &lt;- tq_get(...)\n\nwrite_csv(fred_raw, \"data/fred_raw.csv\")\n```\n\n```{r load-fred-data-real, include=FALSE}\nfred_raw &lt;- read_csv(\"data/fred_raw.csv\")\n```"
  },
  {
    "objectID": "example/11-example.html#look-at-and-clean-data",
    "href": "example/11-example.html#look-at-and-clean-data",
    "title": "Time",
    "section": "Look at and clean data",
    "text": "Look at and clean data\nThe data we get from FRED is in a slightly different format than we’re used to with WDI(), but with good reason. With World Bank data, you get data for every country and every year, so there are rows for Afghanistan 2000, Afghanistan 2001, etc. You then get a column for each of the variables you want (population, life expectancy, GDP/capita, etc.)\nWith FRED data, that kind of format doesn’t work for every possible time series variable because time is spaced differently. If you want to work with annual GDP, you should have a row for each year. If you want quarterly GDP, you should have a row for every quarter. If you put these in the same dataset, you’ll end up with all sorts of missing data issues:\n\n\n\ntime\nannual_gdp\nquarterly_gdp\n\n\n\n\n2019, Q1\nX\nX\n\n\n2019, Q2\n\nX\n\n\n2019, Q3\n\nX\n\n\n2019, Q4\n\nX\n\n\n2020, Q1\nX\nX\n\n\n2020, Q2\n\nX\n\n\n\nTo fix this, the {tidyquant} package gives you data in tidy (or long) form and only provides three columns:\n\n\nCode\nhead(fred_raw)\n## # A tibble: 6 × 3\n##   symbol date        price\n##   &lt;chr&gt;  &lt;date&gt;      &lt;dbl&gt;\n## 1 RSXFSN 1992-01-01 130683\n## 2 RSXFSN 1992-02-01 131244\n## 3 RSXFSN 1992-03-01 142488\n## 4 RSXFSN 1992-04-01 147175\n## 5 RSXFSN 1992-05-01 152420\n## 6 RSXFSN 1992-06-01 151849\n\n\nThe symbol column is the ID of the variable from FRED , date is… the date, and price is the value. These columns are called symbol and price because the {tidyquant} package was designed to get and process stock data, so you’d typically see stock symbols (like AAPL or MSFT) and stock prices. When working with FRED data, the price column shows the value of whatever you’re interested in—it’s not technically a price (so unemployment claims, inflation rates, and GDP values are still called price).\nRight now, our fred_raw dataset has only 3 columns, but nearly 3,000 rows since the six indicators we got from the server are all stacked on top of each other. To actually work with these, we need to filter the raw data so that it only includes the indicators we’re interested in. For instance, if we want to plot retail sales, we need to select only the rows where the symbol is RSXFSN. Make a smaller dataset with filter() to do that:\n\n\nCode\nretail_sales &lt;- fred_raw %&gt;% \n  filter(symbol == \"RSXFSN\")\n\nretail_sales\n## # A tibble: 391 × 3\n##    symbol date        price\n##    &lt;chr&gt;  &lt;date&gt;      &lt;dbl&gt;\n##  1 RSXFSN 1992-01-01 130683\n##  2 RSXFSN 1992-02-01 131244\n##  3 RSXFSN 1992-03-01 142488\n##  4 RSXFSN 1992-04-01 147175\n##  5 RSXFSN 1992-05-01 152420\n##  6 RSXFSN 1992-06-01 151849\n##  7 RSXFSN 1992-07-01 152586\n##  8 RSXFSN 1992-08-01 152476\n##  9 RSXFSN 1992-09-01 148158\n## 10 RSXFSN 1992-10-01 155987\n## # ℹ 381 more rows\n\n\nIf multiple variables have the same spacing (annual, quarterly, monthly, weekly), you can use filter to select all of them and then the use pivot_wider() or spread() to make separate columns for each. Inflation, unemployment, and retail sales are all monthly, so we can make a dataset for just those:\n\n\nCode\nfred_monthly_things &lt;- fred_raw %&gt;% \n  filter(symbol %in% c(\"FPCPITOTLZGUSA\", \"UNRATE\", \"RSXFSN\")) %&gt;% \n  # Convert the symbol column into multiple columns, using the \"prices\" for values\n  pivot_wider(names_from = symbol, values_from = price)\n\nfred_monthly_things\n## # A tibble: 415 × 4\n##    date       RSXFSN FPCPITOTLZGUSA UNRATE\n##    &lt;date&gt;      &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n##  1 1992-01-01 130683           3.03    7.3\n##  2 1992-02-01 131244          NA       7.4\n##  3 1992-03-01 142488          NA       7.4\n##  4 1992-04-01 147175          NA       7.4\n##  5 1992-05-01 152420          NA       7.6\n##  6 1992-06-01 151849          NA       7.8\n##  7 1992-07-01 152586          NA       7.7\n##  8 1992-08-01 152476          NA       7.6\n##  9 1992-09-01 148158          NA       7.6\n## 10 1992-10-01 155987          NA       7.3\n## # ℹ 405 more rows\n\n\nBut wait! There’s a problem! The inflation rate we got isn’t actually monthly—it seems to be annual, which explains all the NAs. Let’s fix it by not including it. We’ll also rename the columns so they’re easier to work with\n\n\nCode\nfred_monthly_things &lt;- fred_raw %&gt;% \n  filter(symbol %in% c(\"UNRATE\", \"RSXFSN\")) %&gt;% \n  # Convert the symbol column into multiple columns, using the \"prices\" for values\n  pivot_wider(names_from = symbol, values_from = price) %&gt;% \n  rename(unemployment = UNRATE, retail_sales = RSXFSN)\n\nfred_monthly_things\n## # A tibble: 415 × 3\n##    date       retail_sales unemployment\n##    &lt;date&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n##  1 1992-01-01       130683          7.3\n##  2 1992-02-01       131244          7.4\n##  3 1992-03-01       142488          7.4\n##  4 1992-04-01       147175          7.4\n##  5 1992-05-01       152420          7.6\n##  6 1992-06-01       151849          7.8\n##  7 1992-07-01       152586          7.7\n##  8 1992-08-01       152476          7.6\n##  9 1992-09-01       148158          7.6\n## 10 1992-10-01       155987          7.3\n## # ℹ 405 more rows\n\n\nAll better.\nWe can make as many subsets of the long, tidy, raw data as we want."
  },
  {
    "objectID": "example/11-example.html#plotting-time",
    "href": "example/11-example.html#plotting-time",
    "title": "Time",
    "section": "Plotting time",
    "text": "Plotting time\nLet’s plot some of these and see what the trends look like. We’ll just use geom_line().\nHere’s GDP:\n\n\nCode\n# Get just GDP data from the raw FRED data\ngdp_only &lt;- fred_raw %&gt;% \n  filter(symbol == \"GDPC1\")\n\nggplot(gdp_only, aes(x = date, y = price)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nHere’s retail sales:\n\n\nCode\n# Get just GDP data from the raw FRED data\nretail_sales_only &lt;- fred_raw %&gt;% \n  filter(symbol == \"RSXFSN\")\n\nggplot(retail_sales_only, aes(x = date, y = price)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nAnd here’s unemployment claims:\n\n\nCode\nunemployment_claims_only &lt;- fred_raw %&gt;% \n  filter(symbol == \"ICSA\")\n\nggplot(unemployment_claims_only, aes(x = date, y = price)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nYikes COVID-19.\nThere, we visualized time. ☑️"
  },
  {
    "objectID": "example/11-example.html#improving-graphics",
    "href": "example/11-example.html#improving-graphics",
    "title": "Time",
    "section": "Improving graphics",
    "text": "Improving graphics\nThese were simple graphs and they’re kind of helpful, but they’re not incredibly informative. We can clean these up a little. First we can change the labels and themes and colors:\n\n\nCode\nggplot(gdp_only, aes(x = date, y = price)) +\n  geom_line(color = \"#0074D9\", linewidth = 1) +\n  scale_y_continuous(labels = label_dollar()) +\n  labs(y = \"Billions of 2012 dollars\",\n       x = NULL,\n       title = \"US Gross Domestic Product\",\n       subtitle = \"Quarterly data; real 2012 dollars\",\n       caption = \"Source: US Bureau of Economic Analysis and FRED\") +\n  theme_bw(base_family = \"Roboto Condensed\") +\n  theme(plot.title = element_text(face = \"bold\"))\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not found in Windows font database\n\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not found in Windows font database\n\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n\n\n\n\n\n\n\n\nThat’s great and almost good enough to publish! We can add one additional layer of information onto the plot and highlight when recessions start and end. We included a recessions variable (USREC) when we got data from FRED, so let’s see what it looks like:\n\n\nCode\nfred_raw %&gt;% \n  filter(symbol == \"USREC\")\n## # A tibble: 415 × 3\n##    symbol date       price\n##    &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt;\n##  1 USREC  1990-01-01     0\n##  2 USREC  1990-02-01     0\n##  3 USREC  1990-03-01     0\n##  4 USREC  1990-04-01     0\n##  5 USREC  1990-05-01     0\n##  6 USREC  1990-06-01     0\n##  7 USREC  1990-07-01     0\n##  8 USREC  1990-08-01     1\n##  9 USREC  1990-09-01     1\n## 10 USREC  1990-10-01     1\n## # ℹ 405 more rows\n\n\nThis is monthly data that shows a 1 if we were in a recession that month and a 0 if we weren’t. The Fed doesn’t decide when recessions happen—the National Bureau of Economic Research (NBER) does, and they have specific guidelines for defining one. We’re probably in one right now, but there’s not enough data for NBER to formally declare it yet.\nThis data is long and tidy, but that makes it harder to work with given our GDP. We want the start and end dates for each recession so that we can shade those areas on the plot. To find those dates, we need to do a little data reshaping. First, we’ll create a temporary variable that marks if there was a switch from 0 to 1 or 1 to 0 in a given row by looking at the previous row\n\n\nCode\nrecessions_tidy &lt;- fred_raw %&gt;% \n  filter(symbol == \"USREC\") %&gt;% \n  mutate(recession_change = price - lag(price))\nrecessions_tidy\n## # A tibble: 415 × 4\n##    symbol date       price recession_change\n##    &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt;            &lt;dbl&gt;\n##  1 USREC  1990-01-01     0               NA\n##  2 USREC  1990-02-01     0                0\n##  3 USREC  1990-03-01     0                0\n##  4 USREC  1990-04-01     0                0\n##  5 USREC  1990-05-01     0                0\n##  6 USREC  1990-06-01     0                0\n##  7 USREC  1990-07-01     0                0\n##  8 USREC  1990-08-01     1                1\n##  9 USREC  1990-09-01     1                0\n## 10 USREC  1990-10-01     1                0\n## # ℹ 405 more rows\n\n\nNotice the new column we have that is mostly 0s, but 1 when there’s a switch, like in August 1990. 1 means we went from 0 to 1 (no recession → recession), while -1 means we went from 1 to 0 (recession → no recession).\nWe can see all the start and end dates if we filter:\n\n\nCode\nrecessions_start_end &lt;- fred_raw %&gt;% \n  filter(symbol == \"USREC\") %&gt;% \n  mutate(recession_change = price - lag(price)) %&gt;% \n  filter(recession_change != 0)\nrecessions_start_end\n## # A tibble: 8 × 4\n##   symbol date       price recession_change\n##   &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt;            &lt;dbl&gt;\n## 1 USREC  1990-08-01     1                1\n## 2 USREC  1991-04-01     0               -1\n## 3 USREC  2001-04-01     1                1\n## 4 USREC  2001-12-01     0               -1\n## 5 USREC  2008-01-01     1                1\n## 6 USREC  2009-07-01     0               -1\n## 7 USREC  2020-03-01     1                1\n## 8 USREC  2020-05-01     0               -1\n\n\nFinally, we can use tibble() to create a brand new little dataset row that includes columns for the start and end dates of each recession. We’ll use a combination of filter() and pull() to extract the start dates (where recession_change is 1) and the end dates (where recession_change is −1), and then we’ll stick those two vectors together in a data frame.\nIf you’re creating this tiny dataset during an actual recession, though, you need to do a little extra step. If you’re currently in a recession, the recession_ends vector will be one element shorted than the recession_starts vector, since the ongoing recession hasn’t ended yet. We can check for this with code. If the recession_ends vector is shorter than recesison_starts, will stick an end date to the recession as today() (today() by itself returns regular text like \"2023-11-01\"; we need to tell R that this is a date by feeding it to ymd())\n\n\nCode\n# Pull out the start dates\nrecession_starts &lt;- recessions_start_end %&gt;% \n  filter(recession_change == 1) %&gt;% \n  pull(date)\nrecession_starts\n## [1] \"1990-08-01\" \"2001-04-01\" \"2008-01-01\" \"2020-03-01\"\n\n# Pull out the end dates\nrecession_ends &lt;- recessions_start_end %&gt;% \n  filter(recession_change == -1) %&gt;% \n  pull(date)\nrecession_ends\n## [1] \"1991-04-01\" \"2001-12-01\" \"2009-07-01\" \"2020-05-01\"\n\n# Check the length of `recession_ends` and append `today()` if it doesn't \n# match `recession_starts`\n#\n# If you're running this code not during a recession, there's no need for this\n# intermediate step, but it's good practice to include it just in case you run\n# this code in the future and you *are* in a recession\nif (length(recession_ends) &lt; length(recession_starts)) {\n  recession_ends &lt;- c(recession_ends, ymd(today()))\n}\n\n# Make a dataframe with the two vectors of start and end dates\nrecessions &lt;- tibble(start = recession_starts,\n                     end = recession_ends)\nrecessions\n## # A tibble: 4 × 2\n##   start      end       \n##   &lt;date&gt;     &lt;date&gt;    \n## 1 1990-08-01 1991-04-01\n## 2 2001-04-01 2001-12-01\n## 3 2008-01-01 2009-07-01\n## 4 2020-03-01 2020-05-01\n\n\nWe can now add this tiny dataset to our plot using geom_rect(). Notice how we put geom_rect() before geom_line()—that’s so the recession rectangles go under the line instead of on top of it. Also notice that we have to specify 4 new aesthetics for geom_rect(): min and max values for both x and y. We use the recession start and end dates for xmin and xmax, and then use −∞ and ∞ for ymin and ymax to make the rectangles stretch from the bottom of the plot to the top.\nThe last odd/new thing here is that we also use inherit.aes = FALSE in geom_rect(). That’s because we specified a global x and y aesthetic in ggplot(), which applies to all the other layers we add. geom_rect() doesn’t use x or y, though, and it’ll complain that those columns are missing. The inherit.aes argument tells ggplot that the geom_rect() layer should not get any of the global aesthetics like x or y.\n\n\nCode\nggplot(gdp_only, aes(x = date, y = price)) +\n  geom_rect(data = recessions, \n            aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf),\n            inherit.aes = FALSE, fill = \"#B10DC9\", alpha = 0.3) +\n  geom_line(color = \"#0074D9\", linewidth = 1) +\n  scale_y_continuous(labels = label_dollar()) +\n  labs(y = \"Billions of 2012 dollars\",\n       x = NULL,\n       title = \"US Gross Domestic Product\",\n       subtitle = \"Quarterly data; real 2012 dollars\",\n       caption = \"Source: US Bureau of Economic Analysis and FRED\") +\n  theme_bw(base_family = \"Roboto Condensed\") +\n  theme(plot.title = element_text(face = \"bold\"))\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n\n\n\n\n\n\n\n\nAnd that’s it!\nNow that we have the tiny recessions data frame, we can add it to any plot we want. Here’s initial unemployment claims with some extra annotations for fun:\n\n\nCode\nggplot(unemployment_claims_only, aes(x = date, y = price)) +\n  geom_rect(data = recessions, \n            aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf),\n            inherit.aes = FALSE, fill = \"#B10DC9\", alpha = 0.3) +\n  geom_line(color = \"#FF4136\", linewidth = 0.5) +\n  annotate(geom = \"label\", x = as.Date(\"2010-01-01\"), y = 1000000, \n           label = \"The Great Recession\", size = 3, family = \"Roboto Condensed\") +\n  annotate(geom = \"label\", x = as.Date(\"2020-01-01\"), y = 6000000, \n           label = \"COVID-19\", size = 3, family = \"Roboto Condensed\", hjust = 1) +\n  scale_y_continuous(labels = label_comma()) +\n  labs(y = \"Initial unemployment claims\",\n       x = NULL,\n       title = \"Initial unemployment claims\",\n       subtitle = \"Weekly data\",\n       caption = \"Source: US Employment and Training Administration and FRED\") +\n  theme_bw(base_family = \"Roboto Condensed\") +\n  theme(plot.title = element_text(face = \"bold\"))\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database"
  },
  {
    "objectID": "example/11-example.html#decomposition",
    "href": "example/11-example.html#decomposition",
    "title": "Time",
    "section": "Decomposition",
    "text": "Decomposition\nThe mechanics of decomposing and forecasting time series goes beyond the scope of this class, but there are lots of resources you can use to learn more, including this phenomenal free textbook.\nThere’s a whole ecosystem of time-related packages that make working with time and decomposing trends easy (named {tidyverts}):\n\n{lubridate}: Helpful functions for manipulating dates (you’ve used this before)\n{tsibble}: Add fancy support for time variables to data frames\n{feasts}: Decompose time series and do other statistical things with time\n{fable}: Make forecasts\n\nHere’s a super short example of how these all work.\nThe retail sales data we got from FRED was not seasonally adjusted, so it looks like it has a heartbeat embedded in it:\n\n\nCode\nretail_sales &lt;- fred_raw %&gt;% \n  filter(symbol == \"RSXFSN\")\n\nggplot(retail_sales, aes(x = date, y = price)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nWe can divide this trend into its main components: the trend, the seasonality, and stuff that’s not explained by either the trend or the seasonality. To do that, we need to first modify our little dataset and tell it to be a time-enabled data frame (a tsibble) that is indexed by the year+month for each row. We’ll create a new column called year_month and then use as_tsibble() to tell R that this is really truly dealing with time:\n\n\nCode\nlibrary(tsibble)  # For embedding time things into data frames\n\nretail_sales &lt;- fred_raw %&gt;% \n  filter(symbol == \"RSXFSN\") %&gt;% \n  mutate(year_month = yearmonth(date)) %&gt;% \n  as_tsibble(index = year_month)\nretail_sales\n## # A tsibble: 391 x 4 [1M]\n##    symbol date        price year_month\n##    &lt;chr&gt;  &lt;date&gt;      &lt;dbl&gt;      &lt;mth&gt;\n##  1 RSXFSN 1992-01-01 130683   1992 Jan\n##  2 RSXFSN 1992-02-01 131244   1992 Feb\n##  3 RSXFSN 1992-03-01 142488   1992 Mar\n##  4 RSXFSN 1992-04-01 147175   1992 Apr\n##  5 RSXFSN 1992-05-01 152420   1992 May\n##  6 RSXFSN 1992-06-01 151849   1992 Jun\n##  7 RSXFSN 1992-07-01 152586   1992 Jul\n##  8 RSXFSN 1992-08-01 152476   1992 Aug\n##  9 RSXFSN 1992-09-01 148158   1992 Sep\n## 10 RSXFSN 1992-10-01 155987   1992 Oct\n## # ℹ 381 more rows\n\n\nNotice that the year_month column is now just the year+month. Neato.\nNext we need to create a time series model using that data. There are lots of different ways to model time series, and distinguishing between the different types is way beyond the scope of this class. Rob Hyndman’s free books covers them all. We’ll do this with STL decomposition (“Seasonal and Trend decomposition using Loess”) There are other models we can use, like ETS or ARIMA, but again, that’s all beyond this class.\n\n\nCode\nlibrary(feasts)  # For decomposition things like STL()\n\nretail_model &lt;- retail_sales %&gt;% \n  model(stl = STL(price))\nretail_model\n## # A mable: 1 x 1\n##       stl\n##   &lt;model&gt;\n## 1   &lt;STL&gt;\n\n\nThe decomposition model we create is kind of boring and useless—it’s all stored in a single cell.\nWe can extract the different components of the decomposition with the components() function:\n\n\nCode\nretail_components &lt;- components(retail_model)\nretail_components\n## # A dable: 391 x 7 [1M]\n## # Key:     .model [1]\n## # :        price = trend + season_year + remainder\n##    .model year_month  price   trend season_year remainder season_adjust\n##    &lt;chr&gt;       &lt;mth&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n##  1 stl      1992 Jan 130683 148390.     -21901.    4195.        152584.\n##  2 stl      1992 Feb 131244 148904.     -22686.    5026.        153930.\n##  3 stl      1992 Mar 142488 149418.      -1804.   -5126.        144292.\n##  4 stl      1992 Apr 147175 149932.      -2815.      57.9       149990.\n##  5 stl      1992 May 152420 150478.       5337.   -3394.        147083.\n##  6 stl      1992 Jun 151849 151023.       3011.   -2185.        148838.\n##  7 stl      1992 Jul 152586 151569.        350.     667.        152236.\n##  8 stl      1992 Aug 152476 152145.       3967.   -3635.        148509.\n##  9 stl      1992 Sep 148158 152720.      -5497.     935.        153655.\n## 10 stl      1992 Oct 155987 153296.        116.    2575.        155871.\n## # ℹ 381 more rows\n\n\nAnd we can use the autoplot() function from the {feasts} package to quickly plot all the components. The plot that autoplot() creates is made with ggplot, so any normal ggplot layers work with it:\n\n\nCode\nautoplot(retail_components) +\n  labs(x = NULL) +\n  theme_bw(base_family = \"Roboto Condensed\") +\n  theme(plot.title = element_text(face = \"bold\"))\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n\n\n\n\n\n\n\n\nWe can also plot individual components on their own using the retail_components dataset we made. Here’s seasonality by itself:\n\n\nCode\nggplot(retail_components, \n       aes(x = year_month, y = season_year)) +\n  geom_rect(data = recessions,\n            aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf),\n            inherit.aes = FALSE, fill = \"#B10DC9\", alpha = 0.3) +\n  geom_line() + \n  scale_y_continuous(labels = label_dollar()) +\n  # ggplot needs to know that the main data is a yearmonth column so that it'll\n  # deal with the recessions data correctly; without this, you'll get an error\n  scale_x_yearmonth() +\n  labs(x = NULL, y = \"Difference from trend, millions of dollars\",\n       title = \"Seasonal trends in retail sales\",\n       subtitle = \"Nominal US dollars\") +\n  theme_bw(base_family = \"Roboto Condensed\") +\n  theme(plot.title = element_text(face = \"bold\"))\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n\n\n\n\n\n\n\n\nAnd here’s the trend by itself:\n\n\nCode\nggplot(retail_components, \n       aes(x = year_month, y = trend)) +\n  geom_rect(data = recessions, \n            aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf),\n            inherit.aes = FALSE, fill = \"#B10DC9\", alpha = 0.3) +\n  geom_line() + \n  scale_y_continuous(labels = label_dollar()) +\n  scale_x_yearmonth() +\n  labs(x = NULL, y = \"Trend, millions of dollars\",\n       title = \"Seasonally adjusted trends in retail sales\",\n       subtitle = \"Nominal US dollars\") +\n  theme_bw(base_family = \"Roboto Condensed\") +\n  theme(plot.title = element_text(face = \"bold\"))\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n\n\n\n\n\n\n\n\nIf you want more control over the combined decomposed plot you can either (1) make individual plots for each of the components and then stitch them together with {patchwork}, or (2) make the components dataset tidy and facet by component. Here’s what that looks like:\n\n\nCode\nretail_components_tidy &lt;- retail_components %&gt;% \n  # Get rid of this column\n  select(-season_adjust) %&gt;% \n  # Take all these component columns and put them into a long column\n  pivot_longer(cols = c(price, trend, season_year, remainder),\n               names_to = \"component\", values_to = \"value\") %&gt;% \n  # Recode this values so they're nicer\n  mutate(component = recode(component, \n                            price = \"Actual data\",\n                            trend = \"Trend\",\n                            season_year = \"Seasonality\",\n                            remainder = \"Remainder\")) %&gt;% \n  # Make the component categories follow the order they're in in the data so\n  # that \"Actual data\" is first, etc.\n  mutate(component = fct_inorder(component))\n\nretail_components_tidy\n## # A tsibble: 1,564 x 4 [1M]\n## # Key:       .model, component [4]\n##    .model year_month component     value\n##    &lt;chr&gt;       &lt;mth&gt; &lt;fct&gt;         &lt;dbl&gt;\n##  1 stl      1992 Jan Actual data 130683 \n##  2 stl      1992 Jan Trend       148390.\n##  3 stl      1992 Jan Seasonality -21901.\n##  4 stl      1992 Jan Remainder     4195.\n##  5 stl      1992 Feb Actual data 131244 \n##  6 stl      1992 Feb Trend       148904.\n##  7 stl      1992 Feb Seasonality -22686.\n##  8 stl      1992 Feb Remainder     5026.\n##  9 stl      1992 Mar Actual data 142488 \n## 10 stl      1992 Mar Trend       149418.\n## # ℹ 1,554 more rows\n\n\nNow that we have a long dataset, we can facet by component:\n\n\nCode\nggplot(retail_components_tidy, \n       aes(x = year_month, y = value)) +\n  geom_rect(data = recessions, \n            aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf),\n            inherit.aes = FALSE, fill = \"#B10DC9\", alpha = 0.3) +\n  geom_line() + \n  scale_y_continuous(labels = label_dollar()) +\n  scale_x_yearmonth() +\n  labs(x = NULL, y = \"Millions of dollars\",\n       title = \"Decomposed US Advance Retail Sales\",\n       subtitle = \"Nominal US dollars\",\n       caption = \"Source: US Census Bureau and FRED (RSXFSN)\") +\n  facet_wrap(vars(component), ncol = 1, scales = \"free_y\") +\n  theme_minimal(base_family = \"Roboto Condensed\") +\n  theme(plot.title = element_text(face = \"bold\"),\n        plot.title.position = \"plot\",\n        strip.text = element_text(face = \"bold\", hjust = 0))\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n\n\n\n\n\n\n\n\nBeautiful!"
  },
  {
    "objectID": "example/09-example.html",
    "href": "example/09-example.html",
    "title": "Annotations",
    "section": "",
    "text": "For this example, we’re again going to use cross-national data from the World Bank’s Open Data portal. We’ll download the data with the {WDI} package.\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):"
  },
  {
    "objectID": "example/09-example.html#live-coding-example",
    "href": "example/09-example.html#live-coding-example",
    "title": "Annotations",
    "section": "Live coding example",
    "text": "Live coding example\n\n\n\n\n\n\n\n\n\n\nSlight differences from the video\n\n\n\nThis is a slightly cleaned up version of the code from the video."
  },
  {
    "objectID": "example/09-example.html#load-data",
    "href": "example/09-example.html#load-data",
    "title": "Annotations",
    "section": "Load data",
    "text": "Load data\nFirst, we load the libraries we’ll be using:\n\n\nCode\nlibrary(tidyverse)  # For ggplot, dplyr, and friends\nlibrary(WDI)        # Get data from the World Bank\nlibrary(ggrepel)    # For non-overlapping labels\nlibrary(ggtext)     # For fancier text handling\n\n\n\n\nCode\nindicators &lt;- c(population = \"SP.POP.TOTL\",  # Population\n                co2_emissions = \"EN.ATM.CO2E.PC\",  # CO2 emissions\n                gdp_per_cap = \"NY.GDP.PCAP.KD\")  # GDP per capita\n\nwdi_co2_raw &lt;- WDI(country = \"all\", indicators, extra = TRUE, \n                   start = 1995, end = 2015)\n\n\nThen we clean the data by removing non-country countries:\n\n\nCode\nwdi_clean &lt;- wdi_co2_raw %&gt;% \n  filter(region != \"Aggregates\")"
  },
  {
    "objectID": "example/09-example.html#clean-and-reshape-data",
    "href": "example/09-example.html#clean-and-reshape-data",
    "title": "Annotations",
    "section": "Clean and reshape data",
    "text": "Clean and reshape data\nNext we’ll do some substantial filtering and reshaping so that we can end up with the rankings of CO2 emissions in 1995 and 2014. I annotate as much as possible below so you can see what’s happening in each step.\n\n\nCode\nco2_rankings &lt;- wdi_clean %&gt;% \n  # Get rid of smaller countries\n  filter(population &gt; 200000) %&gt;% \n  # Only look at two years\n  filter(year %in% c(1995, 2014)) %&gt;% \n  # Get rid of all the rows that have missing values in co2_emissions\n  drop_na(co2_emissions) %&gt;% \n  # Look at each year individually and rank countries based on their emissions that year\n  group_by(year) %&gt;% \n  mutate(ranking = rank(co2_emissions)) %&gt;% \n  ungroup() %&gt;% \n  # Only select a handful of columns, mostly just the newly created \"ranking\"\n  # column and some country identifiers\n  select(iso3c, country, year, region, income, ranking) %&gt;% \n  # Right now the data is tidy and long, but we want to widen it and create\n  # separate columns for emissions in 1995 and in 2014. pivot_wider() will make\n  # new columns based on the existing \"year\" column (that's what `names_from`\n  # does), and it will add \"rank_\" as the prefix, so that the new columns will\n  # be \"rank_1995\" and \"rank_2014\". The values that go in those new columns will\n  # come from the existing \"ranking\" column\n  pivot_wider(names_from = year, names_prefix = \"rank_\", values_from = ranking) %&gt;% \n  # Find the difference in ranking between 2014 and 1995\n  mutate(rank_diff = rank_2014 - rank_1995) %&gt;% \n  # Remove all rows where there's a missing value in the rank_diff column\n  drop_na(rank_diff) %&gt;% \n  # Make an indicator variable that is true of the absolute value of the\n  # difference in rankings is greater than 25. 25 is arbitrary here—that just\n  # felt like a big change in rankings\n  mutate(big_change = ifelse(abs(rank_diff) &gt;= 25, TRUE, FALSE)) %&gt;% \n  # Make another indicator variable that indicates if the rank improved by a\n  # lot, worsened by a lot, or didn't change much. We use the case_when()\n  # function, which is like a fancy version of ifelse() that takes multiple\n  # conditions. This is how it generally works:\n  #\n  # case_when(\n  #  some_test ~ value_if_true,\n  #  some_other_test ~ value_if_true,\n  #  TRUE ~ value_otherwise\n  #)\n  mutate(better_big_change = case_when(\n    rank_diff &lt;= -25 ~ \"Rank improved\",\n    rank_diff &gt;= 25 ~ \"Rank worsened\",\n    TRUE ~ \"Rank changed a little\"\n  ))\n\n\nHere’s what that reshaped data looked like before:\n\n\nCode\nhead(wdi_clean)\n## # A tibble: 6 × 15\n##   country     iso2c iso3c  year status lastupdated population co2_emissions gdp_per_cap region     capital longitude latitude income     lending\n##   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt;  &lt;date&gt;           &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;  \n## 1 Afghanistan AF    AFG    1995 NA     2024-06-28    16418912        0.0888         NA  South Asia Kabul        69.2     34.5 Low income IDA    \n## 2 Afghanistan AF    AFG    2015 NA     2024-06-28    33753499        0.298         567. South Asia Kabul        69.2     34.5 Low income IDA    \n## 3 Afghanistan AF    AFG    2008 NA     2024-06-28    26427199        0.166         419. South Asia Kabul        69.2     34.5 Low income IDA    \n## 4 Afghanistan AF    AFG    1996 NA     2024-06-28    17106595        0.0823         NA  South Asia Kabul        69.2     34.5 Low income IDA    \n## 5 Afghanistan AF    AFG    2009 NA     2024-06-28    27385307        0.240         490. South Asia Kabul        69.2     34.5 Low income IDA    \n## 6 Afghanistan AF    AFG    2007 NA     2024-06-28    25903301        0.110         411. South Asia Kabul        69.2     34.5 Low income IDA\n\n\nAnd here’s what it looks like now:\n\n\nCode\nhead(co2_rankings)\n## # A tibble: 6 × 9\n##   iso3c country     region                     income              rank_1995 rank_2014 rank_diff big_change better_big_change    \n##   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;                      &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;      &lt;chr&gt;                \n## 1 AFG   Afghanistan South Asia                 Low income                 18        26         8 FALSE      Rank changed a little\n## 2 ALB   Albania     Europe & Central Asia      Upper middle income        50        77        27 TRUE       Rank worsened        \n## 3 DZA   Algeria     Middle East & North Africa Lower middle income        94       105        11 FALSE      Rank changed a little\n## 4 AGO   Angola      Sub-Saharan Africa         Lower middle income        62        62         0 FALSE      Rank changed a little\n## 5 ARG   Argentina   Latin America & Caribbean  Upper middle income        99       112        13 FALSE      Rank changed a little\n## 6 ARM   Armenia     Europe & Central Asia      Upper middle income        67        79        12 FALSE      Rank changed a little"
  },
  {
    "objectID": "example/09-example.html#plot-the-data-and-annotate",
    "href": "example/09-example.html#plot-the-data-and-annotate",
    "title": "Annotations",
    "section": "Plot the data and annotate",
    "text": "Plot the data and annotate\nI use IBM Plex Sans in this plot. You can download it from Google Fonts.\n\n\nCode\n# These three functions make it so all geoms that use text, label, and\n# label_repel will use IBM Plex Sans as the font. Those layers are *not*\n# influenced by whatever you include in the base_family argument in something\n# like theme_bw(), so ordinarily you'd need to specify the font in each\n# individual annotate(geom = \"text\") layer or geom_label() layer, and that's\n# tedious! This removes that tediousness.\nupdate_geom_defaults(\"text\", list(family = \"IBM Plex Sans\"))\nupdate_geom_defaults(\"label\", list(family = \"IBM Plex Sans\"))\nupdate_geom_defaults(\"label_repel\", list(family = \"IBM Plex Sans\"))\n\nggplot(co2_rankings,\n       aes(x = rank_1995, y = rank_2014)) +\n  # Add a reference line that goes from the bottom corner to the top corner\n  annotate(geom = \"segment\", x = 0, xend = 175, y = 0, yend = 175) +\n  # Add points and color them by the type of change in rankings\n  geom_point(aes(color = better_big_change)) +\n  # Add repelled labels. Only use data where big_change is TRUE. Fill them by\n  # the type of change (so they match the color in geom_point() above) and use\n  # white text\n  geom_label_repel(data = filter(co2_rankings, big_change == TRUE),\n                   aes(label = country, fill = better_big_change),\n                   color = \"white\") +\n  # Add notes about what the outliers mean in the bottom left and top right\n  # corners. These are italicized and light grey. The text in the bottom corner\n  # is justified to the right with hjust = 1, and the text in the top corner is\n  # justified to the left with hjust = 0\n  annotate(geom = \"text\", x = 170, y = 6, label = \"Outliers improving\", \n           fontface = \"italic\", hjust = 1, color = \"grey50\") +\n  annotate(geom = \"text\", x = 2, y = 170, label = \"Outliers worsening\", \n           fontface = \"italic\", hjust = 0, color = \"grey50\") +\n  # Add mostly transparent rectangles in the bottom right and top left corners\n  annotate(geom = \"rect\", xmin = 0, xmax = 25, ymin = 0, ymax = 25, \n           fill = \"#2ECC40\", alpha = 0.25) +\n  annotate(geom = \"rect\", xmin = 150, xmax = 175, ymin = 150, ymax = 175, \n           fill = \"#FF851B\", alpha = 0.25) +\n  # Add text to define what the rectangles abovee actually mean. The \\n in\n  # \"highest\\nemitters\" will put a line break in the label\n  annotate(geom = \"text\", x = 40, y = 6, label = \"Lowest emitters\", \n           hjust = 0, color = \"#2ECC40\") +\n  annotate(geom = \"text\", x = 162.5, y = 135, label = \"Highest\\nemitters\", \n           hjust = 0.5, vjust = 1, lineheight = 1, color = \"#FF851B\") +\n  # Add arrows between the text and the rectangles. These use the segment geom,\n  # and the arrows are added with the arrow() function, which lets us define the\n  # angle of the arrowhead and the length of the arrowhead pieces. Here we use\n  # 0.5 lines, which is a unit of measurement that ggplot uses internally (think\n  # of how many lines of text fit in the plot). We could also use unit(1, \"cm\")\n  # or unit(0.25, \"in\") or anything else\n  annotate(geom = \"segment\", x = 38, xend = 20, y = 6, yend = 6, color = \"#2ECC40\", \n           arrow = arrow(angle = 15, length = unit(0.5, \"lines\"))) +\n  annotate(geom = \"segment\", x = 162.5, xend = 162.5, y = 140, yend = 155, color = \"#FF851B\", \n           arrow = arrow(angle = 15, length = unit(0.5, \"lines\"))) +\n  # Use three different colors for the points\n  scale_color_manual(values = c(\"grey50\", \"#0074D9\", \"#FF4136\")) +\n  # Use two different colors for the filled labels. There are no grey labels, so\n  # we don't have to specify that color\n  scale_fill_manual(values = c(\"#0074D9\", \"#FF4136\")) +\n  # Make the x and y axes expand all the way to the edges of the plot area and\n  # add breaks every 25 units from 0 to 175\n  scale_x_continuous(expand = c(0, 0), breaks = seq(0, 175, 25)) +\n  scale_y_continuous(expand = c(0, 0), breaks = seq(0, 175, 25)) +\n  # Add labels! There are a couple fancy things here.\n  # 1. In the title we wrap the 2 of CO2 in the HTML &lt;sub&gt;&lt;/sub&gt; tag so that the\n  #    number gets subscripted. The only way this will actually get parsed as \n  #    HTML is if we tell the plot.title to use element_markdown() in the \n  #    theme() function, and element_markdown() comes from the ggtext package.\n  # 2. In the subtitle we bold the two words **improved** and **worsened** using\n  #    Markdown asterisks. We also wrap these words with HTML span tags with \n  #    inline CSS to specify the color of the text. Like the title, this will \n  #    only be processed and parsed as HTML and Markdown if we tell the p\n  #    lot.subtitle to use element_markdown() in the theme() function.\n  labs(x = \"Rank in 1995\", y = \"Rank in 2014\",\n       title = \"Changes in CO&lt;sub&gt;2&lt;/sub&gt; emission rankings between 1995 and 2014\",\n       subtitle = \"Countries that &lt;span style='color: #0074D9'&gt;**improved**&lt;/span&gt; or &lt;span style='color: #FF4136'&gt;**worsened**&lt;/span&gt; more than 25 positions in the rankings highlighted\",\n       caption = \"Source: The World Bank.\\nCountries with populations of less than 200,000 excluded.\") +\n  # Turn off the legends for color and fill, since the subtitle includes that\n  guides(color = \"none\", fill = \"none\") +\n  # Use theme_bw() with IBM Plex Sans\n  theme_bw(base_family = \"IBM Plex Sans\") +\n  # Tell the title and subtitle to be treated as Markdown/HTML, make the title\n  # 1.6x the size of the base font, and make the subtitle 1.3x the size of the\n  # base font. Also add a little larger margin on the right of the plot so that\n  # the 175 doesn't get cut off.\n  theme(plot.title = element_markdown(face = \"bold\", size = rel(1.6)),\n        plot.subtitle = element_markdown(size = rel(1.3)),\n        plot.margin = unit(c(0.5, 1, 0.5, 0.5), units = \"lines\"))\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not found in Windows font database\n\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not found in Windows font database"
  },
  {
    "objectID": "example/07-example.html",
    "href": "example/07-example.html",
    "title": "Relationships",
    "section": "",
    "text": "For this example, we’re going to use historical weather data from the now-defunct Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. I downloaded this data using Dark Sky’s (now-retired-because-they-were-bought-by-Apple) API using the {darksky} package. If you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):"
  },
  {
    "objectID": "example/07-example.html#live-coding-example",
    "href": "example/07-example.html#live-coding-example",
    "title": "Relationships",
    "section": "Live coding example",
    "text": "Live coding example\n\n\n\n\n\n\n\n\n\n\nSlight differences from the video\n\n\n\nThis is a slightly cleaned up version of the code from the video."
  },
  {
    "objectID": "example/07-example.html#load-and-clean-data",
    "href": "example/07-example.html#load-and-clean-data",
    "title": "Relationships",
    "section": "Load and clean data",
    "text": "Load and clean data\nFirst, we load the libraries we’ll be using:\n\n\nCode\nlibrary(tidyverse)  # For ggplot, dplyr, and friends\nlibrary(patchwork)  # For combining ggplot plots\nlibrary(GGally)     # For scatterplot matrices\nlibrary(broom)      # For converting model objects to data frames\n\n\nThen we load the data with read_csv(). Here I assume that the CSV file lives in a subfolder in my project named data:\n\n\nCode\nweather_atl &lt;- read_csv(\"data/atl-weather-2019.csv\")"
  },
  {
    "objectID": "example/07-example.html#legal-dual-y-axes",
    "href": "example/07-example.html#legal-dual-y-axes",
    "title": "Relationships",
    "section": "Legal dual y-axes",
    "text": "Legal dual y-axes\nIt is fine (and often helpful!) to use two y-axes if the two different scales measure the same thing, like counts and percentages, Fahrenheit and Celsius, pounds and kilograms, inches and centimeters, etc.\nTo do this, you need to add an argument (sec.axis) to scale_y_continuous() to tell it to use a second axis. This sec.axis argument takes a sec_axis() function that tells ggplot how to transform the scale. You need to specify a formula or function that defines how the original axis gets transformed. This formula uses a special syntax. It needs to start with a ~, which indicates that it’s a function, and it needs to use . to stand in for the original value in the original axis.\nSince the equation for converting Fahrenheit to Celsius is this…\n\\[\n\\text{C} = (32 - \\text{F}) \\times -\\frac{5}{9}\n\\]\n…we can specify this with code like so (where . stands for the Fahrenheit value):\n~ (32 - .) * -5 / 9\nHere’s a plot of daily high temperatures in Atlanta throughout 2019, with a second axis:\n\n\nCode\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) +\n  geom_line() +\n  scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9,\n                                         name = \"Celsius\")) +\n  labs(x = NULL, y = \"Fahrenheit\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFor fun, we could also convert it to Kelvin, which uses this formula:\n\\[\n\\text{K} = (\\text{F} - 32) \\times \\frac{5}{9} + 273.15\n\\]\n\n\nCode\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) +\n  geom_line() +\n  scale_y_continuous(sec.axis = sec_axis(trans = ~ (. - 32) * 5/9 + 273.15,\n                                         name = \"Kelvin\")) +\n  labs(x = NULL, y = \"Fahrenheit\") +\n  theme_minimal()"
  },
  {
    "objectID": "example/07-example.html#combining-plots",
    "href": "example/07-example.html#combining-plots",
    "title": "Relationships",
    "section": "Combining plots",
    "text": "Combining plots\nA good alternative to using two y-axes is to use two plots instead. The {patchwork} package makes this really easy to do with R. There are other similar packages that do this, like {cowplot} and {gridExtra}, but I’ve found that {patchwork} is the easiest to use and it actually aligns the different plot elements like axis lines and legends (yay alignment in CRAP!). The documentation for {patchwork} is really great and full of examples—you should check it out to see all the things you can do with it!\nTo use {patchwork}, we need to (1) save our plots as objects and (2) add them together with +.\nFor instance, is there a relationship between temperature and humidity in Atlanta? We can plot both:\n\n\nCode\n# Temperature in Atlanta\ntemp_plot &lt;- ggplot(weather_atl, aes(x = time, y = temperatureHigh)) +\n  geom_line() +\n  geom_smooth() +\n  scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9,\n                                         name = \"Celsius\")) +\n  labs(x = NULL, y = \"Fahrenheit\") +\n  theme_minimal()\ntemp_plot\n\n\n\n\n\n\n\n\n\nCode\n\n# Humidity in Atlanta\nhumidity_plot &lt;- ggplot(weather_atl, aes(x = time, y = humidity)) +\n  geom_line() +\n  geom_smooth() +\n  labs(x = NULL, y = \"Humidity\") +\n  theme_minimal()\nhumidity_plot\n\n\n\n\n\n\n\n\n\nRight now, these are two separate plots, but we can combine them with + if we load {patchwork}:\n\n\nCode\nlibrary(patchwork)\n\ntemp_plot + humidity_plot\n\n\n\n\n\n\n\n\n\nBy default, {patchwork} will put these side-by-side, but we can change that with the plot_layout() function:\n\n\nCode\ntemp_plot + humidity_plot +\n  plot_layout(ncol = 1)\n\n\n\n\n\n\n\n\n\nWe can also play with other arguments in plot_layout(). If we want to make the temperature plot taller and shrink the humidity section, we can specify the proportions for the plot heights. Here, the temperature plot is 70% of the height and the humidity plot is 30%:\n\n\nCode\ntemp_plot + humidity_plot +\n  plot_layout(ncol = 1, heights = c(0.7, 0.3))"
  },
  {
    "objectID": "example/07-example.html#scatterplot-matrices",
    "href": "example/07-example.html#scatterplot-matrices",
    "title": "Relationships",
    "section": "Scatterplot matrices",
    "text": "Scatterplot matrices\nWe can visualize the correlations between pairs of variables with the ggpairs() function in the {GGally} package. For instance, how correlated are high and low temperatures, humidity, wind speed, and the chance of precipitation? We first make a smaller dataset with just those columns, and then we feed that dataset into ggpairs() to see all the correlation information:\n\n\nCode\nlibrary(GGally)\n\nweather_correlations &lt;- weather_atl %&gt;% \n  select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability)\n\nggpairs(weather_correlations)\n\n\n\n\n\n\n\n\n\nIt looks like high and low temperatures are extremely highly positively correlated (r = 0.92). Wind spped and temperature are moderately negatively correlated, with low temperatures having a stronger negative correlation (r = -0.45). There’s no correlation whatsoever between low temperatures and the precipitation probability (r = -0.03) or humidity and high temperatures (r = -0.03).\nEven though ggpairs() doesn’t use the standard ggplot(...) + geom_whatever() syntax we’re familiar with, it does behind the scenes, so you can add regular ggplot layers to it:\n\n\nCode\nggpairs(weather_correlations) +\n  labs(title = \"Correlations!\") +\n  theme_dark()"
  },
  {
    "objectID": "example/07-example.html#correlograms",
    "href": "example/07-example.html#correlograms",
    "title": "Relationships",
    "section": "Correlograms",
    "text": "Correlograms\nScatterplot matrices typically include way too much information to be used in actual publications. I use them when doing my own analysis just to see how different variables are related, but I rarely polish them up for public consumption. In the readings for this week, Claus Wilke showed a type of plot called a correlogram which is more appropriate for publication.\nThese are essentially heatmaps of the different correlation coefficients. To make these with ggplot, we need to do a little bit of extra data processing, mostly to reshape data into a long, tidy format that we can plot. Here’s how.\nFirst we need to build a correlation matrix of the main variables we care about. Ordinarily the cor() function in R takes two arguments—x and y—and it will return a single correlation value. If you feed a data frame into cor() though, it’ll calculate the correlation between each pair of columns\n\n\nCode\n# Create a correlation matrix\nthings_to_correlate &lt;- weather_atl %&gt;% \n  select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability) %&gt;% \n  cor()\n\nthings_to_correlate\n##                   temperatureHigh temperatureLow humidity windSpeed precipProbability\n## temperatureHigh              1.00          0.920   -0.030    -0.377            -0.124\n## temperatureLow               0.92          1.000    0.112    -0.450            -0.026\n## humidity                    -0.03          0.112    1.000     0.011             0.722\n## windSpeed                   -0.38         -0.450    0.011     1.000             0.196\n## precipProbability           -0.12         -0.026    0.722     0.196             1.000\n\n\nThe two halves of this matrix (split along the diagonal line) are identical, so we can remove the lower triangle with this code (which will set all the cells in the lower triangle to NA):\n\n\nCode\n# Get rid of the lower triangle\nthings_to_correlate[lower.tri(things_to_correlate)] &lt;- NA\nthings_to_correlate\n##                   temperatureHigh temperatureLow humidity windSpeed precipProbability\n## temperatureHigh                 1           0.92    -0.03    -0.377            -0.124\n## temperatureLow                 NA           1.00     0.11    -0.450            -0.026\n## humidity                       NA             NA     1.00     0.011             0.722\n## windSpeed                      NA             NA       NA     1.000             0.196\n## precipProbability              NA             NA       NA        NA             1.000\n\n\nFinally, in order to plot this, the data needs to be in tidy (or long) format. Here we convert the things_to_correlate matrix into a data frame, add a column for the row names, take all the columns and put them into a single column named measure1, and take all the correlation numbers and put them in a column named cor In the end, we make sure the measure variables are ordered by their order of appearance (otherwise they plot alphabetically and don’t make a triangle)\n\n\nCode\nthings_to_correlate_long &lt;- things_to_correlate %&gt;% \n  # Convert from a matrix to a data frame\n  as.data.frame() %&gt;% \n  # Matrixes have column names that don't get converted to columns when using\n  # as.data.frame(), so this adds those names as a column\n  rownames_to_column(\"measure2\") %&gt;% \n  # Make this long. Take all the columns except measure2 and put their names in\n  # a column named measure1 and their values in a column named cor\n  pivot_longer(cols = -measure2,\n               names_to = \"measure1\",\n               values_to = \"cor\") %&gt;% \n  # Make a new column with the rounded version of the correlation value\n  mutate(nice_cor = round(cor, 2)) %&gt;% \n  # Remove rows where the two measures are the same (like the correlation\n  # between humidity and humidity)\n  filter(measure2 != measure1) %&gt;%\n  # Get rid of the empty triangle\n  filter(!is.na(cor)) %&gt;% \n  # Put these categories in order\n  mutate(measure1 = fct_inorder(measure1),\n         measure2 = fct_inorder(measure2))\n\nthings_to_correlate_long\n## # A tibble: 10 × 4\n##    measure2        measure1              cor nice_cor\n##    &lt;fct&gt;           &lt;fct&gt;               &lt;dbl&gt;    &lt;dbl&gt;\n##  1 temperatureHigh temperatureLow     0.920      0.92\n##  2 temperatureHigh humidity          -0.0301    -0.03\n##  3 temperatureHigh windSpeed         -0.377     -0.38\n##  4 temperatureHigh precipProbability -0.124     -0.12\n##  5 temperatureLow  humidity           0.112      0.11\n##  6 temperatureLow  windSpeed         -0.450     -0.45\n##  7 temperatureLow  precipProbability -0.0255    -0.03\n##  8 humidity        windSpeed          0.0108     0.01\n##  9 humidity        precipProbability  0.722      0.72\n## 10 windSpeed       precipProbability  0.196      0.2\n\n\nPhew. With the data all tidied like that, we can make a correlogram with a heatmap. This is just like the heatmap you made in session 4, but here we manipulate the fill scale a little so that it’s diverging with three colors: a high value, a midpoint value, and a low value.\n\n\nCode\nggplot(things_to_correlate_long, \n       aes(x = measure2, y = measure1, fill = cor)) +\n  geom_tile() +\n  geom_text(aes(label = nice_cor)) +\n  scale_fill_gradient2(low = \"#E16462\", mid = \"white\", high = \"#0D0887\",\n                       limits = c(-1, 1)) +\n  labs(x = NULL, y = NULL) +\n  coord_equal() +\n  theme_minimal() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\nInstead of using a heatmap, we can also use points, which encode the correlation information both as color and as size. To do that, we just need to switch geom_tile() to geom_point() and set the size = cor mapping:\n\n\nCode\nggplot(things_to_correlate_long, \n       aes(x = measure2, y = measure1, color = cor)) +\n  # Size by the absolute value so that -0.7 and 0.7 are the same size\n  geom_point(aes(size = abs(cor))) +\n  scale_color_gradient2(low = \"#E16462\", mid = \"white\", high = \"#0D0887\",\n                        limits = c(-1, 1)) +\n  scale_size_area(max_size = 15, limits = c(-1, 1), guide = \"none\") +\n  labs(x = NULL, y = NULL) +\n  coord_equal() +\n  theme_minimal() +\n  theme(panel.grid = element_blank())"
  },
  {
    "objectID": "example/07-example.html#simple-regression",
    "href": "example/07-example.html#simple-regression",
    "title": "Relationships",
    "section": "Simple regression",
    "text": "Simple regression\nWe can also visualize the relationships between variables using regression. Simple regression is easy to visualize, since you’re only working with an X and a Y. For instance, what’s the relationship between humidity and high temperatures during the summer?\nFirst, let’s filter the data to only look at the summer. We also add a column to scale up the humidity value—right now it’s on a scale of 0-1 (for percentages), but when interpreting regression we talk about increases in whole units, so we’d talk about moving from 0% humidity to 100% humidity, which isn’t helpful, so instead we multiply everything by 100 so we can talk about moving from 50% humidity to 51% humidity. We also scale up a couple other variables that we’ll use in the larger model later.\n\n\nCode\nweather_atl_summer &lt;- weather_atl %&gt;% \n  filter(time &gt;= \"2019-05-01\", time &lt;= \"2019-09-30\") %&gt;% \n  mutate(humidity_scaled = humidity * 100,\n         moonPhase_scaled = moonPhase * 100,\n         precipProbability_scaled = precipProbability * 100,\n         cloudCover_scaled = cloudCover * 100)\n\n\nThen we can build a simple regression model:\n\n\nCode\nmodel_simple &lt;- lm(temperatureHigh ~ humidity_scaled, \n                   data = weather_atl_summer)\n\ntidy(model_simple, conf.int = TRUE)\n## # A tibble: 2 × 7\n##   term            estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)      104.       2.35       44.3  1.88e-88   99.5     109.   \n## 2 humidity_scaled   -0.241    0.0358     -6.74 3.21e-10   -0.312    -0.170\n\n\nWe can interpret these coefficients like so:\n\nThe intercept shows that the average temperature when there’s 0% humidity is 104°. There are no days with 0% humidity though, so we can ignore the intercept—it’s really just here so that we can draw the line.\nThe coefficient for humidity_scaled shows that a one percent increase in humidity is associated with a 0.241° decrease in temperature, on average.\n\nVisualizing this model is simple, since there are only two variables:\n\n\nCode\nggplot(weather_atl_summer,\n       aes(x = humidity_scaled, y = temperatureHigh)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n## `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAnd indeed, as humidity increases, temperatures decrease."
  },
  {
    "objectID": "example/07-example.html#coefficient-plots",
    "href": "example/07-example.html#coefficient-plots",
    "title": "Relationships",
    "section": "Coefficient plots",
    "text": "Coefficient plots\nBut if we use multiple variables in the model, it gets really hard to visualize the results since we’re working with multiple dimensions. Instead, we can use coefficient plots to see the individual coefficients in the model.\nFirst, let’s build a more complex model:\n\n\nCode\nmodel_complex &lt;- lm(temperatureHigh ~ humidity_scaled + moonPhase_scaled + \n                      precipProbability_scaled + windSpeed + pressure + cloudCover_scaled,\n                    data = weather_atl_summer)\ntidy(model_complex, conf.int = TRUE)\n## # A tibble: 7 × 7\n##   term                     estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)              262.      125.         2.09  0.0380    14.8      510.    \n## 2 humidity_scaled           -0.111     0.0757    -1.47  0.143     -0.261      0.0381\n## 3 moonPhase_scaled           0.0116    0.0126     0.917 0.360     -0.0134     0.0366\n## 4 precipProbability_scaled   0.0356    0.0203     1.75  0.0820    -0.00458    0.0758\n## 5 windSpeed                 -1.78      0.414     -4.29  0.0000326 -2.59      -0.958 \n## 6 pressure                  -0.157     0.122     -1.28  0.203     -0.398      0.0854\n## 7 cloudCover_scaled         -0.0952    0.0304    -3.14  0.00207   -0.155     -0.0352\n\n\nWe can interpret these coefficients like so:\n\nHolding everything else constant, a 1% increase in humidity is associated with a 0.11° decrease in the high temperature, on average, but the effect is not statistically significant\nHolding everything else constant, a 1% increase in moon visibility is associated with a 0.01° increase in the high temperature, on average, and the effect is not statistically significant\nHolding everything else constant, a 1% increase in the probability of precipitation is associated with a 0.04° increase in the high temperature, on average, and the effect is not statistically significant\nHolding everything else constant, a 1 mph increase in the wind speed is associated with a 1.8° decrease in the high temperature, on average, and the effect is statistically significant\nHolding everything else constant, a 1 unit increase in barometric pressure is associated with a 0.15° decrease in the high temperature, on average, and the effect is not statistically significant\nHolding everything else constant, a 1% increase in cloud cover is associated with a 0.01° decrease in the high temperature, on average, and the effect is statistically significant\nThe intercept is pretty useless. It shows that the predicted temperature will be 262° when humidity is 0%, the moon is invisible, there’s no chance of precipitation, no wind, no barometric pressure, and no cloud cover. Yikes.\n\nTo plot all these things at once, we’ll store the results of tidy(model_complex) as a data frame, remove the useless intercept, and plot it using geom_pointrange():\n\n\nCode\nmodel_tidied &lt;- tidy(model_complex, conf.int = TRUE) %&gt;% \n  filter(term != \"(Intercept)\")\n\nggplot(model_tidied,\n       aes(x = estimate, y = term)) +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dotted\") +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) + \n  labs(x = \"Coefficient estimate\", y = NULL) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNeat! Now we can see how big these different coefficients are and how close they are to zero. Wind speed has a big significant effect on temperature. The others are all very close to zero."
  },
  {
    "objectID": "example/07-example.html#marginal-effects-plots",
    "href": "example/07-example.html#marginal-effects-plots",
    "title": "Relationships",
    "section": "Marginal effects plots",
    "text": "Marginal effects plots\n\n\n\n\n\n\n2023 update!\n\n\n\nSince recording the video for this section, lots of things have changed in the R world to make finding predicted values and marginal effects a lot easier. (This is why I had you read my guide to marginal things as part of the readings for this session.) The {marginaleffects} R package makes it really nice and easy to get predicted values of an outcome while holding everything else constant—you don’t need to plug values in manually anymore like this section shows.\nThere’s an example of using {marginaleffects} down below.\n\n\nInstead of just looking at the coefficients, we can also see the effect of moving different variables up and down like sliders and switches. Remember that regression coefficients allow us to build actual mathematical formulas that predict the value of Y. The coefficients from model_compex yield the following big hairy ugly equation:\n\\[\n\\begin{aligned}\n\\hat{\\text{High temperature}} =& 262 - 0.11 \\times \\text{humidity\\_scaled } \\\\\n& + 0.01 \\times \\text{moonPhase\\_scaled } + 0.04 \\times \\text{precipProbability\\_scaled } \\\\\n& - 1.78 \\times \\text{windSpeed} - 0.16 \\times \\text{pressure} - 0.095 \\times \\text{cloudCover\\_scaled}\n\\end{aligned}\n\\]\nIf we plug in values for each of the explanatory variables, we can get the predicted value of high temperature, or \\(\\hat{y}\\).\nThe augment() function in the {broom} library allows us to take a data frame of explanatory variable values, plug them into the model equation, and get predictions out. For example, let’s set each of the variables to some arbitrary values (50% for humidity, moon phase, chance of rain, and cloud cover; 1000 for pressure, and 1 MPH for wind speed).\n\n\nCode\nnewdata_example &lt;- tibble(humidity_scaled = 50, moonPhase_scaled = 50, \n                          precipProbability_scaled = 50, windSpeed = 1, \n                          pressure = 1000, cloudCover_scaled = 50)\nnewdata_example\n## # A tibble: 1 × 6\n##   humidity_scaled moonPhase_scaled precipProbability_scaled windSpeed pressure cloudCover_scaled\n##             &lt;dbl&gt;            &lt;dbl&gt;                    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt;\n## 1              50               50                       50         1     1000                50\n\n\nWe can plug these values into the model with augment(). The se_fit argument gives us standard errors for each prediction:\n\n\nCode\n# I use select() here because augment() returns columns for all the explanatory\n# variables, and the .fitted column with the predicted value is on the far right\n# and gets cut off\naugment(model_complex, newdata = newdata_example, se_fit = TRUE) %&gt;% \n  select(.fitted, .se.fit)\n## # A tibble: 1 × 2\n##   .fitted .se.fit\n##     &lt;dbl&gt;   &lt;dbl&gt;\n## 1    96.2    3.19\n\n\nGiven these weather conditions, the predicted high temperature is 96.2°. Now you’re an armchair meteorologist!\nWe can follow the same pattern to show how the predicted temperature changes as specific variables change across a whole range. Here, we create a data frame of possible wind speeds and keep all the other explanatory variables at their means:\n\n\nCode\nnewdata &lt;- tibble(windSpeed = seq(0, 8, 0.5),\n                  pressure = mean(weather_atl_summer$pressure),\n                  precipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled),\n                  moonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled),\n                  humidity_scaled = mean(weather_atl_summer$humidity_scaled),\n                  cloudCover_scaled = mean(weather_atl_summer$cloudCover_scaled))\nnewdata\n## # A tibble: 17 × 6\n##    windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled\n##        &lt;dbl&gt;    &lt;dbl&gt;                    &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;             &lt;dbl&gt;\n##  1       0      1016.                     40.2             50.7            64.8              29.5\n##  2       0.5    1016.                     40.2             50.7            64.8              29.5\n##  3       1      1016.                     40.2             50.7            64.8              29.5\n##  4       1.5    1016.                     40.2             50.7            64.8              29.5\n##  5       2      1016.                     40.2             50.7            64.8              29.5\n##  6       2.5    1016.                     40.2             50.7            64.8              29.5\n##  7       3      1016.                     40.2             50.7            64.8              29.5\n##  8       3.5    1016.                     40.2             50.7            64.8              29.5\n##  9       4      1016.                     40.2             50.7            64.8              29.5\n## 10       4.5    1016.                     40.2             50.7            64.8              29.5\n## 11       5      1016.                     40.2             50.7            64.8              29.5\n## 12       5.5    1016.                     40.2             50.7            64.8              29.5\n## 13       6      1016.                     40.2             50.7            64.8              29.5\n## 14       6.5    1016.                     40.2             50.7            64.8              29.5\n## 15       7      1016.                     40.2             50.7            64.8              29.5\n## 16       7.5    1016.                     40.2             50.7            64.8              29.5\n## 17       8      1016.                     40.2             50.7            64.8              29.5\n\n\nIf we feed this big data frame into augment(), we can get the predicted high temperature for each row. We can also use the .se.fit column to calculate the 95% confidence interval for each predicted value. We take the standard error, multiply it by -1.96 and 1.96 (or the quantile of the normal distribution at 2.5% and 97.5%), and add that value to the estimate.\n\n\nCode\npredicted_values &lt;- augment(model_complex, \n                            newdata = newdata,\n                            se_fit = TRUE) %&gt;% \n  mutate(conf.low = .fitted + (-1.96 * .se.fit),\n         conf.high = .fitted + (1.96 * .se.fit))\n\npredicted_values %&gt;% \n  select(windSpeed, .fitted, .se.fit, conf.low, conf.high) %&gt;% \n  head()\n## # A tibble: 6 × 5\n##   windSpeed .fitted .se.fit conf.low conf.high\n##       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1       0      95.3   1.63      92.2      98.5\n## 2       0.5    94.5   1.42      91.7      97.2\n## 3       1      93.6   1.22      91.2      96.0\n## 4       1.5    92.7   1.03      90.7      94.7\n## 5       2      91.8   0.836     90.1      93.4\n## 6       2.5    90.9   0.653     89.6      92.2\n\n\nCool! Just looking at the data in the table, we can see that predicted temperature drops as windspeed increases. We can plot this to visualize the effect:\n\n\nCode\nggplot(predicted_values, aes(x = windSpeed, y = .fitted)) +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high),\n              fill = \"#BF3984\", alpha = 0.5) + \n  geom_line(linewidth = 1, color = \"#BF3984\") +\n  labs(x = \"Wind speed (MPH)\", y = \"Predicted high temperature (F)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe just manipulated one of the model coefficients and held everything else at its mean. We can manipulate multiple variables too and encode them all on the graph. For instance, what is the effect of windspeed and cloud cover on the temperature?\nWe’ll follow the same process, but vary both windSpeed and cloudCover_scaled. Instead of using tibble(), we use exapnd_grid(), which creates every combination of the variables we specify. Instead of varying cloud cover by every possible value (like from 0 to 100), we’ll choose four possible cloud cover types: 0%, 33%, 66%, and 100%. Everything else will be at its mean.\n\n\nCode\nnewdata_fancy &lt;- expand_grid(windSpeed = seq(0, 8, 0.5),\n                             pressure = mean(weather_atl_summer$pressure),\n                             precipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled),\n                             moonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled),\n                             humidity_scaled = mean(weather_atl_summer$humidity_scaled),\n                             cloudCover_scaled = c(0, 33, 66, 100))\nnewdata_fancy\n## # A tibble: 68 × 6\n##    windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled\n##        &lt;dbl&gt;    &lt;dbl&gt;                    &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;             &lt;dbl&gt;\n##  1       0      1016.                     40.2             50.7            64.8                 0\n##  2       0      1016.                     40.2             50.7            64.8                33\n##  3       0      1016.                     40.2             50.7            64.8                66\n##  4       0      1016.                     40.2             50.7            64.8               100\n##  5       0.5    1016.                     40.2             50.7            64.8                 0\n##  6       0.5    1016.                     40.2             50.7            64.8                33\n##  7       0.5    1016.                     40.2             50.7            64.8                66\n##  8       0.5    1016.                     40.2             50.7            64.8               100\n##  9       1      1016.                     40.2             50.7            64.8                 0\n## 10       1      1016.                     40.2             50.7            64.8                33\n## # ℹ 58 more rows\n\n\nNotice now that windSpeed repeats four times (0, 0, 0, 0, 0.5, 0.5, etc.), since there are four possible cloudCover_scaled values (0, 33, 66, 100).\nWe can plot this now, just like before, with wind speed on the x-axis, the predicted temperature on the y-axis, and colored and faceted by cloud cover:\n\n\nCode\npredicted_values_fancy &lt;- augment(model_complex, \n                                  newdata = newdata_fancy, \n                                  se_fit = TRUE) %&gt;% \n  mutate(conf.low = .fitted + (-1.96 * .se.fit),\n         conf.high = .fitted + (1.96 * .se.fit)) %&gt;% \n  # Make cloud cover a categorical variable so we can facet with it\n  mutate(cloudCover_scaled = factor(cloudCover_scaled))\n\nggplot(predicted_values_fancy, aes(x = windSpeed, y = .fitted)) +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = cloudCover_scaled),\n              alpha = 0.5) + \n  geom_line(aes(color = cloudCover_scaled), linewidth = 1) +\n  labs(x = \"Wind speed (MPH)\", y = \"Predicted high temperature (F)\") +\n  theme_minimal() +\n  guides(fill = \"none\", color = \"none\") +\n  facet_wrap(vars(cloudCover_scaled), nrow = 1)\n\n\n\n\n\n\n\n\n\nThat’s so neat! Temperatures go down slightly as cloud cover increases. If we wanted to improve the model, we’d add an interaction term between cloud cover and windspeed so that each line would have a different slope in addition to a different intercept, but that’s beyond the scope of this class."
  },
  {
    "objectID": "example/07-example.html#predicted-values-and-marginal-effects-in-2023",
    "href": "example/07-example.html#predicted-values-and-marginal-effects-in-2023",
    "title": "Relationships",
    "section": "Predicted values and marginal effects in 2023",
    "text": "Predicted values and marginal effects in 2023\nInstead of using expand_grid() and augment() to create and plug in a mini dataset of variables to move up and down, we can use the {marginaleffects} package to simplify life!\nRemember above where we wanted to see the effect of wind speed on temperature while holding all other variables in the model constant. We had to create a small data frame (newdata) with columns for each of the variables in the model, with everything except windSpeed set to their averages. We then plugged newdata into the model with augment() and calculated the confidence interval around each predicted value using mutate(). It’s a fairly involved process, but it works:\n\n\nCode\n# Make model\nmodel_complex &lt;- lm(temperatureHigh ~ humidity_scaled + moonPhase_scaled + \n                      precipProbability_scaled + windSpeed + pressure + cloudCover_scaled,\n                    data = weather_atl_summer)\n\n# Make mini dataset\nnewdata &lt;- tibble(windSpeed = seq(0, 8, 0.5),\n                  pressure = mean(weather_atl_summer$pressure),\n                  precipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled),\n                  moonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled),\n                  humidity_scaled = mean(weather_atl_summer$humidity_scaled),\n                  cloudCover_scaled = mean(weather_atl_summer$cloudCover_scaled))\n\n# Plug mini dataset into model\npredicted_values &lt;- augment(model_complex, \n                            newdata = newdata,\n                            se_fit = TRUE) %&gt;% \n  mutate(conf.low = .fitted + (-1.96 * .se.fit),\n         conf.high = .fitted + (1.96 * .se.fit))\n\n# Look at predicted values\npredicted_values %&gt;% \n  select(windSpeed, .fitted, .se.fit, conf.low, conf.high) %&gt;% \n  head()\n## # A tibble: 6 × 5\n##   windSpeed .fitted .se.fit conf.low conf.high\n##       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1       0      95.3   1.63      92.2      98.5\n## 2       0.5    94.5   1.42      91.7      97.2\n## 3       1      93.6   1.22      91.2      96.0\n## 4       1.5    92.7   1.03      90.7      94.7\n## 5       2      91.8   0.836     90.1      93.4\n## 6       2.5    90.9   0.653     89.6      92.2\n\n\nThe {marginaleffects} package makes this far easier. We can use the predictions() function to generate, um, predictions. We still need to feed it a smaller dataset of variables to manipulate, but if we use the datagrid() function, we only have to specify the variables we want to move. It will automatically use the averages or typical values for all other variables in the model. It will also automatically create confidence intervals for each prediction—no need for the mutate(conf.low = .fitted + (-1.96 * .se.fit)) math that we did previously.\n\n\nCode\nlibrary(marginaleffects)\n## Warning: package 'marginaleffects' was built under R version 4.2.3\n\n# Calculate predictions across a range of windSpeed\npredicted_values_easy &lt;- predictions(\n  model_complex,\n  newdata = datagrid(windSpeed = seq(0, 8, 0.5))\n)\n\n# Look at predicted values\npredicted_values_easy %&gt;%\n  select(windSpeed, estimate, std.error, conf.low, conf.high)\n## \n##  Estimate Std. Error CI low CI high\n##      95.3      1.626   92.2    98.5\n##      94.5      1.425   91.7    97.2\n##      93.6      1.225   91.2    96.0\n##      92.7      1.028   90.7    94.7\n##      91.8      0.836   90.1    93.4\n##      90.9      0.653   89.6    92.2\n##      90.0      0.490   89.0    91.0\n##      89.1      0.374   88.4    89.9\n##      88.2      0.354   87.5    88.9\n##      87.3      0.443   86.5    88.2\n##      86.5      0.594   85.3    87.6\n##      85.6      0.772   84.1    87.1\n##      84.7      0.961   82.8    86.6\n##      83.8      1.157   81.5    86.1\n##      82.9      1.356   80.2    85.6\n##      82.0      1.557   79.0    85.1\n##      81.1      1.759   77.7    84.6\n## \n## Columns: windSpeed, estimate, std.error, conf.low, conf.high\n\n\nWe can then plot this really easily too:\n\n\nCode\nggplot(predicted_values_easy, aes(x = windSpeed, y = estimate)) +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high),\n              fill = \"#BF3984\", alpha = 0.5) + \n  geom_line(linewidth = 1, color = \"#BF3984\") +\n  labs(x = \"Wind speed (MPH)\", y = \"Predicted high temperature (F)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis works when moving multiple variables at the same time, too. Earlier we used expand_grid() to create a mini dataset of different values for both windSpeed and cloudCover, while holding all the other variables at their means. Here’s how to do that with the much easier predictions() function:\n\n\nCode\npredicted_values_fancy_easy &lt;- predictions(\n  model_complex,\n  newdata = datagrid(windSpeed = seq(0, 8, 0.5),\n                     cloudCover_scaled = c(0, 33, 66, 100))) %&gt;%\n  # Make cloud cover a categorical variable so we can facet with it\n  mutate(cloudCover_scaled = factor(cloudCover_scaled))\n\nggplot(predicted_values_fancy_easy, aes(x = windSpeed, y = estimate)) +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = cloudCover_scaled),\n              alpha = 0.5) + \n  geom_line(aes(color = cloudCover_scaled), linewidth = 1) +\n  labs(x = \"Wind speed (MPH)\", y = \"Predicted high temperature (F)\") +\n  theme_minimal() +\n  guides(fill = \"none\", color = \"none\") +\n  facet_wrap(vars(cloudCover_scaled), nrow = 1)\n\n\n\n\n\n\n\n\n\nThat’s it! predictions() makes this so easy!\nIf we’re interested in the slopes (or marginal effects) of these lines, we can calculate these really easily too with the slopes() function. For instance, here are the predicted temperatures when just manipulating wind speed:\n\n\nCode\nggplot(predicted_values_easy, aes(x = windSpeed, y = estimate)) +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high),\n              fill = \"#BF3984\", alpha = 0.5) + \n  geom_line(linewidth = 1, color = \"#BF3984\") +\n  labs(x = \"Wind speed (MPH)\", y = \"Predicted high temperature (F)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nIf we want to see what the slope of that line is when wind speed is 2, 4 and 6, we can use slopes():\n\n\nCode\nslopes(model_complex, \n  newdata = datagrid(windSpeed = c(2, 4, 6)), \n  variables = \"windSpeed\") %&gt;%\n  # This creates a ton of extra columns so we'll just look at a few\n  select(term, windSpeed, estimate, std.error, p.value, conf.low, conf.high, predicted)\n## \n##       Term Estimate Std. Error Pr(&gt;|z|) CI low CI high\n##  windSpeed    -1.78      0.414   &lt;0.001  -2.59  -0.964\n##  windSpeed    -1.78      0.415   &lt;0.001  -2.59  -0.962\n##  windSpeed    -1.78      0.414   &lt;0.001  -2.59  -0.966\n## \n## Columns: term, windSpeed, estimate, std.error, p.value, conf.low, conf.high, predicted\n\n\nThe estimate column here shows that slope at each of those values of windSpeed is -1.8, meaning a 1-MPH increase in wind speed is associated with a nearly 2° decrease in predicted high temperature, on average. That’s not super exciting though, since the predicted values create a nice straight line, with the same slope across the whole range of the line. It’s also the same number we get from the model coefficient—run tidy(model_complex) and you’ll see that the coefficient for windSpeed is -1.78. Since everything is linear here, using slopes() isn’t that important.\nFor bonus fun and excitement, let’s make an even more complex model with some non-linear curvy stuff and some interaction terms. We’ll include both wind speed and wind speed squared (since maybe higher wind speeds have a larger effect on predicted temperatures), and the interaction between wind speed and cloud cover (since maybe temperature behaves differently at different combinations of wind speed and cloud cover). Again, I’m not a meteorologist so this model is definitely wrong, but it gives us some neat moving parts we can play with.\n\n\nCode\n# Make model\n# We square windSpeed with I(windSpeed^2). The I() function lets you do math\n# with regression terms.\n# We make an interaction term with *\nmodel_wild &lt;- lm(temperatureHigh ~ humidity_scaled + moonPhase_scaled + \n                   precipProbability_scaled + windSpeed + I(windSpeed^2) + \n                   pressure + cloudCover_scaled + (windSpeed * cloudCover_scaled),\n                 data = weather_atl_summer)\n\ntidy(model_wild)\n## # A tibble: 9 × 5\n##   term                         estimate std.error statistic p.value\n##   &lt;chr&gt;                           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 (Intercept)                 264.       126.        2.10    0.0374\n## 2 humidity_scaled              -0.122      0.0767   -1.58    0.115 \n## 3 moonPhase_scaled              0.0109     0.0128    0.849   0.397 \n## 4 precipProbability_scaled      0.0390     0.0207    1.88    0.0620\n## 5 windSpeed                     0.113      2.58      0.0438  0.965 \n## 6 I(windSpeed^2)               -0.198      0.330    -0.601   0.549 \n## 7 pressure                     -0.162      0.123    -1.32    0.190 \n## 8 cloudCover_scaled            -0.0691     0.0829   -0.833   0.406 \n## 9 windSpeed:cloudCover_scaled  -0.00688    0.0196   -0.351   0.726\n\n\nWe have some strange new regression coefficients now. We have two coefficients for wind speed: windSpeed and I(windSpeed^2). We also have a coefficient for the interaction term windspeed:cloudCover_scaled. We cannot interpret these individually. If we want to know the effect of wind speed on high temperatures, we have to incorporate all three of these new coefficients simultaneously. Fortunately predictions() and slopes() both handle that for us automatically.\nLet’s plot the predictions to see that everything is more curvy now (and differently curved across different levels of cloud cover).\n\n\nCode\npredicted_values_wild &lt;- predictions(\n  model_wild, \n  newdata = datagrid(windSpeed = seq(0, 8, 0.5),\n                     cloudCover_scaled = c(0, 33, 66, 100))) %&gt;%\n  # Make cloud cover a categorical variable so we can facet with it\n  mutate(cloudCover_scaled = factor(cloudCover_scaled))\n\nggplot(predicted_values_wild, aes(x = windSpeed, y = estimate)) +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = cloudCover_scaled),\n              alpha = 0.5) + \n  geom_line(aes(color = cloudCover_scaled), linewidth = 1) +\n  labs(x = \"Wind speed (MPH)\", y = \"Predicted high temperature (F)\") +\n  theme_minimal() +\n  guides(fill = \"none\", color = \"none\") +\n  facet_wrap(vars(cloudCover_scaled), nrow = 1)\n\n\n\n\n\n\n\n\n\nThat’s neat! At all the different levels of cloud cover, the wind speed trend is fairly shallow (and even pretty flat when cloud cover is 0 or 33) at low levels of wind speed. The line drops fairly quickly as wind speed increases though. Let’s get some exact numbers with marginaleffects():\n\n\nCode\nslopes(model_wild, \n  newdata = datagrid(windSpeed = c(2, 4, 6),\n    cloudCover_scaled = c(0, 33, 66, 100)), \n  variables = \"windSpeed\") %&gt;%\n  # This creates a ton of extra columns so we'll just look at a few\n  select(term, windSpeed, estimate, std.error, p.value, conf.low, conf.high, predicted)\n## \n##       Term Estimate Std. Error Pr(&gt;|z|) CI low CI high\n##  windSpeed   -0.681      1.477   0.6449  -3.58  2.2145\n##  windSpeed   -0.908      1.669   0.5864  -4.18  2.3627\n##  windSpeed   -1.135      2.050   0.5800  -5.15  2.8838\n##  windSpeed   -1.368      2.561   0.5931  -6.39  3.6515\n##  windSpeed   -1.475      0.694   0.0335  -2.83 -0.1150\n##  windSpeed   -1.702      0.421   &lt;0.001  -2.53 -0.8755\n##  windSpeed   -1.928      0.837   0.0212  -3.57 -0.2880\n##  windSpeed   -2.162      1.462   0.1391  -5.03  0.7030\n##  windSpeed   -2.268      1.801   0.2078  -5.80  1.2613\n##  windSpeed   -2.495      1.434   0.0817  -5.31  0.3144\n##  windSpeed   -2.722      1.305   0.0369  -5.28 -0.1651\n##  windSpeed   -2.956      1.488   0.0470  -5.87 -0.0394\n## \n## Columns: term, windSpeed, estimate, std.error, p.value, conf.low, conf.high, predicted\n\n\nPhew, we have 12 different slopes here. Let’s talk through a few of them to get the intuition. If cloud cover is 0 and wind speed is 2 MPH, moving from 2 to 3 MPH is associated with a -0.68° decrease in high temperature on average (see the estimate column in the first row in the table). If the existing wind speed is 6 MPH, moving from 6 to 7 is associated with a -2.27° decrease in high temperature on average (see the estimate column in the 9th row of the table). The slope is steeper and more negative when the wind is faster, so changes in temperature are more dramatic. Because we have an interaction with cloud cover, the slope also changes at different levels of cloud cover. At 2 MPH of wind, the slope is -0.68° when cloud cover is 0 (first row), but -1.37° when cloud cover is 100 (4th row).\nFinally, we can visualize how these slopes change across wind speed and cloud cover by plotting them:\n\n\nCode\nslopes_wild &lt;- slopes(\n  model_wild, \n  newdata = datagrid(windSpeed = seq(0, 6, by = 0.1),\n                     cloudCover_scaled = c(0, 33, 66, 100)), \n  variables = \"windSpeed\") %&gt;%\n  mutate(cloudCover_scaled = factor(cloudCover_scaled))\n\nggplot(slopes_wild, aes(x = windSpeed, y = estimate)) +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = cloudCover_scaled),\n              alpha = 0.5) +\n  geom_line(aes(color = cloudCover_scaled), linewidth = 1) +\n  labs(x = \"Wind speed (MPH)\", y = \"Slope (marginal effect) of wind speed\",\n       title = \"Marginal effect of wind speed across levels of cloud cover\",\n       subtitle = \"These are *slopes*, not predicted values\") +\n  theme_minimal() +\n  guides(fill = \"none\", color = \"none\") +\n  facet_wrap(vars(cloudCover_scaled), nrow = 1)"
  },
  {
    "objectID": "example/05-example.html",
    "href": "example/05-example.html",
    "title": "Themes",
    "section": "",
    "text": "The lesson for this week’s session is a fairly comprehensive introduction to using the theme() function in ggplot, and this page by Henry Wang is a good cheat sheet for remembering which theme elements are which on a plot—and I like this PDF cheatsheet by Clara Granell even better.\nFor your exercise, you’re going to create the world’s ugliest plot. For this example, we’ll use the principles of CRAP to make a great theme.\nI’m going to build the theme semi-incrementally here. Instead of showing how the plot updates with each change in setting, I do most of the updates all at once, with tons of comments explaining what each line does. Importantly, I did not write this all at once. When you’re tinkering with themes, you generally start with something like theme_minimal() or theme_bw() and then gradually add new things to theme(), like modifying plot.title, then plot.subtitle, etc. It’s a very iterative process with lots of tinkering. Because of this, there is no live-coding video for this example—it would be incredibly long and boring. Instead, look through each of the lines and see what they’re doing.\nFor this example, I’m going to use the gapminder dataset that we’ve been using throughout this week. Instead of using the CSV file like we did before, we’ll load the data from the {gapminder} package. Once you run library(gapminder), you’ll automatically have access to a dataset named gapminder.\nI’m also going to use the Roboto Condensed font in the theme. Download and install it on your computer if you don’t have it."
  },
  {
    "objectID": "example/05-example.html#basic-plot",
    "href": "example/05-example.html#basic-plot",
    "title": "Themes",
    "section": "Basic plot",
    "text": "Basic plot\nWhen I’m creating a theme, I like to use a basic plot with everything that might show up, complete with a title, subtitle, caption, legend, facets, and other elements.\n\n\nCode\nlibrary(tidyverse)  # For ggplot, dplyr, and friends\nlibrary(gapminder)  # For gapminder data\nlibrary(scales)     # For nice axis labels\n\ngapminder_filtered &lt;- gapminder %&gt;% \n  filter(year &gt; 2000)\n\nbase_plot &lt;- ggplot(data = gapminder_filtered,\n                    mapping = aes(x = gdpPercap, y = lifeExp, \n                                  color = continent, size = pop)) +\n  geom_point() +\n  # Use dollars, and get rid of the cents part (i.e. $300 instead of $300.00)\n  scale_x_log10(labels = label_dollar(accuracy = 1)) +\n  # Format with commas\n  scale_size_continuous(labels = label_comma()) +\n  # Use viridis\n  scale_color_viridis_d(option = \"plasma\", end = 0.9) +\n  labs(x = \"GDP per capita\", y = \"Life expectancy\",\n       color = \"Continent\", size = \"Population\",\n       title = \"Here's a cool title\",\n       subtitle = \"And here's a neat subtitle\",\n       caption = \"Source: The Gapminder Project\") +\n  facet_wrap(vars(year))\n\nbase_plot\n\n\n\n\n\n\n\n\n\nNow we have base_plot to work with. Here’s what it looks like with theme_minimal() applied to it:\n\n\nCode\nbase_plot +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThat gets rid of the grey background and is a good start, but we can make lots of improvements. First let’s deal with the gridlines. There are too many. We can get rid of the minor gridlines with by setting them to element_blank():\n\n\nCode\nbase_plot +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\n\nNext let’s add some typographic contrast. We’ll use Roboto Condensed Regular as the base font. Before trying this, make sure you do the following:\nOn macOS:\n\nRun capabilities() in your console and verify that TRUE shows up under cairo\nIf not, download and install XQuartz\n\nOn Windows:\n\nRun windowsFonts() in your console and you’ll see a list of all the fonts you can use with R. It’s not a very big list.\n#&gt; $serif\n#&gt; [1] \"TT Times New Roman\"\n#&gt;\n#&gt; $sans\n#&gt; [1] \"TT Arial\"\n#&gt; \n#&gt; $mono\n#&gt; [1] \"TT Courier New\"\nYou can add Roboto Condensed to your current R session by running this in your console:\n\n\nCode\nwindowsFonts(`Roboto Condensed` = windowsFont(\"Roboto Condensed\"))\n\n\nNow if you run windowsFonts(), you’ll see it in the list:\n#&gt; $serif\n#&gt; [1] \"TT Times New Roman\"\n#&gt;\n#&gt; $sans\n#&gt; [1] \"TT Arial\"\n#&gt; \n#&gt; $mono\n#&gt; [1] \"TT Courier New\"\n#&gt;\n#&gt; $`Roboto Condensed`\n#&gt; [1] \"Roboto Condensed\"\nThis only takes effect for your current R session, so if you are knitting a document or if you ever plan on closing RStudio, you’ll need to incorporate this font creation code into your script.\n\nWe’ll use the font as the base_family argument. Note how I make it bold with face and change the size with rel(). Instead of manually setting some arbitrary size, I use rel() to resize the text in relation to the base_size argument. Using rel(1.7) means 1.7 × base_size, or 20.4 That will rescale according to whatever base_size is—if I shrink it to base_size = 8, the title will scale down accordingly.\n\n\nCode\nplot_with_good_typography &lt;- base_plot +\n  theme_minimal(base_family = \"Roboto Condensed\", base_size = 12) +\n  theme(panel.grid.minor = element_blank(),\n        # Bold, bigger title\n        plot.title = element_text(face = \"bold\", size = rel(1.7)),\n        # Plain, slightly bigger subtitle that is grey\n        plot.subtitle = element_text(face = \"plain\", size = rel(1.3), color = \"grey70\"),\n        # Italic, smaller, grey caption that is left-aligned\n        plot.caption = element_text(face = \"italic\", size = rel(0.7), \n                                    color = \"grey70\", hjust = 0),\n        # Bold legend titles\n        legend.title = element_text(face = \"bold\"),\n        # Bold, slightly larger facet titles that are left-aligned for the sake of repetition\n        strip.text = element_text(face = \"bold\", size = rel(1.1), hjust = 0),\n        # Bold axis titles\n        axis.title = element_text(face = \"bold\"),\n        # Add some space above the x-axis title and make it left-aligned\n        axis.title.x = element_text(margin = margin(t = 10), hjust = 0),\n        # Add some space to the right of the y-axis title and make it top-aligned\n        axis.title.y = element_text(margin = margin(r = 10), hjust = 1))\nplot_with_good_typography\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n## not found in Windows font database\n\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n## not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n## not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n## not found in Windows font database\n\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n## not found in Windows font database\n\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n## not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n## font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n## font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n\n\n\n\n\n\n\n\nWhoa. That gets us most of the way there! We have good contrast with the typography, with the strong bold and the lighter regular font (✓ contrast). Everything is aligned left (✓ alignment and ✓ repetition). By moving the axis titles a little bit away from the labels, we’ve enhanced proximity, since they were too close together (✓ proximity). We repeat grey in both the caption and the subtitle (✓ repetition).\nThe only thing I don’t like is that the 2002 isn’t quite aligned with the title and subtitle. This is because the facet labels are in boxes along the top of each plot, and in some themes (like theme_grey() and theme_bw()) those facet labels have grey backgrounds. We can turn off the margin in those boxes, or we can add a background, which will then be perfectly aligned with the title and subtitle.\n\n\nCode\nplot_with_good_typography +\n  # Add a light grey background to the facet titles, with no borders\n  theme(strip.background = element_rect(fill = \"grey90\", color = NA),\n        # Add a thin grey border around all the plots to tie in the facet titles\n        panel.border = element_rect(color = \"grey90\", fill = NA))\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n## font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n## font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n\n\n\n\n\n\n\n\n👨‍🍳 💋! That looks great!\nTo save ourselves time in the future, we can store this whole thing as an object that we can then reuse on other plots:\n\n\nCode\nmy_pretty_theme &lt;- theme_minimal(base_family = \"Roboto Condensed\", base_size = 12) +\n  theme(panel.grid.minor = element_blank(),\n        # Bold, bigger title\n        plot.title = element_text(face = \"bold\", size = rel(1.7)),\n        # Plain, slightly bigger subtitle that is grey\n        plot.subtitle = element_text(face = \"plain\", size = rel(1.3), color = \"grey70\"),\n        # Italic, smaller, grey caption that is left-aligned\n        plot.caption = element_text(face = \"italic\", size = rel(0.7), \n                                    color = \"grey70\", hjust = 0),\n        # Bold legend titles\n        legend.title = element_text(face = \"bold\"),\n        # Bold, slightly larger facet titles that are left-aligned for the sake of repetition\n        strip.text = element_text(face = \"bold\", size = rel(1.1), hjust = 0),\n        # Bold axis titles\n        axis.title = element_text(face = \"bold\"),\n        # Add some space above the x-axis title and make it left-aligned\n        axis.title.x = element_text(margin = margin(t = 10), hjust = 0),\n        # Add some space to the right of the y-axis title and make it top-aligned\n        axis.title.y = element_text(margin = margin(r = 10), hjust = 1),\n        # Add a light grey background to the facet titles, with no borders\n        strip.background = element_rect(fill = \"grey90\", color = NA),\n        # Add a thin grey border around all the plots to tie in the facet titles\n        panel.border = element_rect(color = \"grey90\", fill = NA))\n\n\nNow we can use it on any plot. Remember that first plot you made in your exercise from session 1 with the cars dataset? Let’s throw this theme on it! (only here the dataset is named mpg instead of cars; the mpg dataset is loaded invisibly whenever you load ggplot)\n\n\nCode\nmpg_example &lt;- ggplot(data = mpg, \n                      mapping = aes(x = displ, y = hwy, color = class)) +\n  geom_point(size = 3) + \n  scale_color_viridis_d() +\n  facet_wrap(vars(drv)) +\n  labs(x = \"Displacement\", y = \"Highway MPG\", color = \"Car class\",\n       title = \"Heavier cars get worse mileage\",\n       subtitle = \"Except two-seaters?\",\n       caption = \"Here's a caption\") +\n  my_pretty_theme\n\nmpg_example\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n## font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n## font family not found in Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n## family not found in Windows font database\n\n\n\n\n\n\n\n\n\nSuper neat!"
  },
  {
    "objectID": "example/05-example.html#nice-pre-built-themes",
    "href": "example/05-example.html#nice-pre-built-themes",
    "title": "Themes",
    "section": "Nice pre-built themes",
    "text": "Nice pre-built themes\nThis custom theme we just made is just one iteration of a theme. There are countless ways to tinker with a theme and have it meet the different CRAP principles. People have even published their own themes in different R packages. Check these out to see lots of different examples:\n\n{hrbrthemes}\n{ggthemes}\n{ggthemr}\n{ggtech}\n{tvthemes}\n{ggpomological} (this one is incredible!)\n\nCheck this blog post for examples of a bunch of others"
  },
  {
    "objectID": "example/05-example.html#bonus-ggthemeassist",
    "href": "example/05-example.html#bonus-ggthemeassist",
    "title": "Themes",
    "section": "Bonus: {ggthemeassist}",
    "text": "Bonus: {ggthemeassist}\nIf you’re intimidated by constantly referring to the documentation and figuring out what little line of code affects which part of the graph, install and check out the {ggthemeassist} package. It provides an interactive menu for manipulating different theme elements, and then generates all the corresponding code, which is really magical.\nHere’s a brief example of how to use it."
  },
  {
    "objectID": "example/05-example.html#saving-plots",
    "href": "example/05-example.html#saving-plots",
    "title": "Themes",
    "section": "Saving plots",
    "text": "Saving plots\nIf we want to save these plots, we can use ggsave(). For that to work, we need to store the plot as an object, which I already did in the examples above:\n\n\nCode\nname_of_plot_object &lt;- ggplot(...)\n\n\nWe then feed our saved plot object to ggsave() and specify the filename and dimensions we want to use. If we’re using PNG, we don’t need to worry about any extra options. If we’re using PDF, we need to tell R to use the Cairo PDF writing engine instead of R’s normal one, since R’s normal one can’t deal with custom fonts.\n\n\nCode\n# Add my_pretty_theme to the gapminder base_plot and save as an object\nfinal_gampinder_plot &lt;- base_plot +\n  my_pretty_theme\n\n# Save as PNG and PDF\nggsave(\"fancy_gapminder.png\", final_gampinder_plot,\n       width = 8, height = 5, units = \"in\")\nggsave(\"fancy_gapminder.pdf\", final_gampinder_plot,\n       width = 8, height = 5, units = \"in\", device = cairo_pdf)\n\n# Save the mpg plot as PNG and PDF\nggsave(\"fancy_mpg.png\", mpg_example,\n       width = 8, height = 5, units = \"in\")\nggsave(\"fancy_mpg.pdf\", mpg_example,\n       width = 8, height = 5, units = \"in\", device = cairo_pdf)"
  },
  {
    "objectID": "example/03-example.html",
    "href": "example/03-example.html",
    "title": "Mapping data to graphics",
    "section": "",
    "text": "For this example, I’m going to use real world data to demonstrate the typical process for loading data, cleaning it up a bit, and mapping specific columns of the data onto the parts of a graph using the grammar of graphics and ggplot().\nThe data I’ll use comes from the BBC’s corporate charity, BBC Children in Need, which makes grants to smaller UK nonprofit organizations that work on issues related to childhood poverty. An organization in the UK named 360Giving helps nonprofits and foundations publish data about their grant giving activities in an open and standardized way, and (as of May 2020) they list data from 126 different charities, including BBC Children in Need.\nIf you want to follow along with this example (highly recommended!), you can download the data directly from 360Giving or by using this link:"
  },
  {
    "objectID": "example/03-example.html#live-coding-example",
    "href": "example/03-example.html#live-coding-example",
    "title": "Mapping data to graphics",
    "section": "Live coding example",
    "text": "Live coding example\n\n\n\n\n\n\nWarning\n\n\n\nI got carried away with this because I wanted to make it as comprehensive and detailed as possible, so it starts off with nothing and walks through the process of downloading data, creating a new project, and getting everything started. As such, it is ridiculously long (1 hour 😱 😱). Remember that there’s no requirement that you watch these things—they’re simply for your reference so you can see what doing this R stuff looks like in real time. The content all below the video is roughly the same (more polished even).\nThat said, it is a useful demonstration of how to get everything started and what it looks like to do an entire analysis, so there is value in it. Watch just the first part, or watch it on 2x or something.\nAnd I promise future examples will not be this long!\n\n\n\n\n\n\n\n\n\n\n\n\nSlight differences from the video\n\n\n\nThis is a slightly cleaned up version of the code from the video."
  },
  {
    "objectID": "example/03-example.html#load-and-clean-data",
    "href": "example/03-example.html#load-and-clean-data",
    "title": "Mapping data to graphics",
    "section": "Load and clean data",
    "text": "Load and clean data\nFirst, we need to load a few libraries: {tidyverse} (as always) and {readxl} for reading Excel files:\n\n\nCode\n# Load libraries\nlibrary(tidyverse)  # For ggplot, dplyr, and friends\nlibrary(readxl)     # For reading Excel files\n\n\nWe’ll then load the original Excel file. I placed this file in a folder named data in my RStudio Project folder for this example. I like to read original data into an object named whatever_raw just in case it takes a long time to load (that way I don’t have to keep reloading it every time I add a new column or do anything else with it). It’s also good practice to keep a pristine, untouched copy of your data.\n\n\nCode\n# Load the original Excel file\nbbc_raw &lt;- read_excel(\"data/360-giving-data.xlsx\")\n\n\nThere may be some errors reading the file—you can ignore those in this case.\nNext we’ll add a couple columns and clean up the data a little. In the video I did this non-linearly—I came back to the top of the document to add columns when I needed them and then reran the chunk to create the data.\nWe’ll extract the year from the Award Date column, rename some of the longer-named columns, and make a new column that shows the duration of grants. We’ll also get rid of 2015 since there are so few observations then.\nNote the strange use of `s around column names like `Award Date`. This is because R technically doesn’t allow special characters like spaces in column names. If there are spaces, you have to wrap the column names in backticks. Because typing backticks all the time gets tedious, we’ll use rename() to rename some of the columns:\n\n\nCode\nbbc &lt;- bbc_raw %&gt;% \n  # Extract the year from the award date\n  mutate(grant_year = year(`Award Date`)) %&gt;% \n  # Rename some columns\n  rename(grant_amount = `Amount Awarded`,\n         grant_program = `Grant Programme:Title`,\n         grant_duration = `Planned Dates:Duration (months)`) %&gt;% \n  # Make a new text-based version of the duration column, recoding months\n  # between 12-23, 23-35, and 36+. The case_when() function here lets us use\n  # multiple if/else conditions at the same time.\n  mutate(grant_duration_text = case_when(\n    grant_duration &gt;= 12 & grant_duration &lt; 24 ~ \"1 year\",\n    grant_duration &gt;= 24 & grant_duration &lt; 36 ~ \"2 years\",\n    grant_duration &gt;= 36 ~ \"3 years\"\n  )) %&gt;% \n  # Get rid of anything before 2016\n  filter(grant_year &gt; 2015) %&gt;% \n  # Make a categorical version of the year column\n  mutate(grant_year_category = factor(grant_year))"
  },
  {
    "objectID": "example/03-example.html#histograms",
    "href": "example/03-example.html#histograms",
    "title": "Mapping data to graphics",
    "section": "Histograms",
    "text": "Histograms\nFirst let’s look at the distribution of grant amounts with a histogram. Map grant_amount to the x-axis and don’t map anything to the y-axis, since geom_histogram() will calculate the y-axis values for us:\n\n\nCode\nggplot(data = bbc, mapping = aes(x = grant_amount)) +\n  geom_histogram()\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNotice that ggplot warns you about bin widths. By default it will divide the data into 30 equally spaced bins, which will most likely not be the best for your data. You should always set your own bin width to something more appropriate. There are no rules for correct bin widths. Just don’t have them be too wide:\n\n\nCode\nggplot(data = bbc, mapping = aes(x = grant_amount)) +\n  geom_histogram(binwidth = 100000)\n\n\n\n\n\n\n\n\n\nOr too small:\n\n\nCode\nggplot(data = bbc, mapping = aes(x = grant_amount)) +\n  geom_histogram(binwidth = 500)\n\n\n\n\n\n\n\n\n\n£10,000 seems to fit well. It’s often helpful to add a white border to the histogram bars, too:\n\n\nCode\nggplot(data = bbc, mapping = aes(x = grant_amount)) +\n  geom_histogram(binwidth = 10000, color = \"white\")\n\n\n\n\n\n\n\n\n\nWe can map other variables onto the plot, like mapping grant_year_category to the fill aesthetic:\n\n\nCode\nggplot(bbc, aes(x = grant_amount, fill = grant_year_category)) +\n  geom_histogram(binwidth = 10000, color = \"white\")\n\n\n\n\n\n\n\n\n\nThat gets really hard to interpret though, so we can facet by year with facet_wrap():\n\n\nCode\nggplot(bbc, aes(x = grant_amount, fill = grant_year_category)) +\n  geom_histogram(binwidth = 10000, color = \"white\") +\n  facet_wrap(vars(grant_year))\n\n\n\n\n\n\n\n\n\nNeat!"
  },
  {
    "objectID": "example/03-example.html#points",
    "href": "example/03-example.html#points",
    "title": "Mapping data to graphics",
    "section": "Points",
    "text": "Points\nNext let’s look at the data using points, mapping year to the x-axis and grant amount to the y-axis:\n\n\nCode\nggplot(bbc, aes(x = grant_year_category, y = grant_amount)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nWe have some serious overplotting here, with dots so thick that it looks like lines. We can fix this a couple different ways. First, we can make the points semi-transparent using alpha, which ranges from 0 (completely invisible) to 1 (completely solid).\n\n\nCode\nggplot(bbc, aes(x = grant_year_category, y = grant_amount)) +\n  geom_point(alpha = 0.1)\n\n\n\n\n\n\n\n\n\nWe can also randomly space the points to spread them out using position_jitter():\n\n\nCode\nggplot(bbc, aes(x = grant_year_category, y = grant_amount)) +\n  geom_point(position = position_jitter())\n\n\n\n\n\n\n\n\n\nOne issue with this, though, is that the points are jittered along the x-axis (which is fine, since they’re all within the same year) and the y-axis (which is bad, since the amounts are actual numbers). We can tell ggplot to only jitter in one direction by specifying the height argument—we don’t want any up-and-down jittering:\n\n\nCode\nggplot(bbc, aes(x = grant_year_category, y = grant_amount)) +\n  geom_point(position = position_jitter(height = 0))\n\n\n\n\n\n\n\n\n\nThere are some weird clusters around £30,000 and below. Let’s map grant_program to the color aesthetic, which has two categories—regular grants and small grants—and see if that helps explain why:\n\n\nCode\nggplot(bbc, aes(x = grant_year_category, y = grant_amount, color = grant_program)) +\n  geom_point(position = position_jitter(height = 0))\n\n\n\n\n\n\n\n\n\nIt does! We appear to have two different distributions of grants: small grants have a limit of £30,000, while regular grants have a much higher average amount."
  },
  {
    "objectID": "example/03-example.html#boxplots",
    "href": "example/03-example.html#boxplots",
    "title": "Mapping data to graphics",
    "section": "Boxplots",
    "text": "Boxplots\nWe can add summary information to the plot by only changing the geom we’re using. Switch from geom_point() to geom_boxplot():\n\n\nCode\nggplot(bbc, aes(x = grant_year_category, y = grant_amount, color = grant_program)) +\n  geom_boxplot()"
  },
  {
    "objectID": "example/03-example.html#summaries",
    "href": "example/03-example.html#summaries",
    "title": "Mapping data to graphics",
    "section": "Summaries",
    "text": "Summaries\nWe can also make smaller summarized datasets with {dplyr} functions like group_by() and summarize() and plot those. First let’s look at grant totals, averages, and counts over time:\n\n\nCode\nbbc_by_year &lt;- bbc %&gt;% \n  group_by(grant_year) %&gt;%  # Make invisible subgroups for each year\n  summarize(total = sum(grant_amount),  # Find the total awarded in each group\n            avg = mean(grant_amount),  # Find the average awarded in each group\n            number = n())  # n() is a special function that shows the number of rows in each group\n\n# Look at our summarized data\nbbc_by_year\n## # A tibble: 4 × 4\n##   grant_year    total    avg number\n##        &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;int&gt;\n## 1       2016 17290488 78238.    221\n## 2       2017 62394278 59765.   1044\n## 3       2018 61349392 60205.   1019\n## 4       2019 41388816 61136.    677\n\n\nBecause we used summarize(), R shrank our data down significantly. We now only have a row for each of the subgroups we made: one for each year. We can plot this smaller data. We’ll use geom_col() for now (but in the next session you’ll learn why this is actually bad for averages!)\n\n\nCode\n# Plot our summarized data\nggplot(bbc_by_year, aes(x = grant_year, y = avg)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\nCode\nggplot(bbc_by_year, aes(x = grant_year, y = total)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\nCode\nggplot(bbc_by_year, aes(x = grant_year, y = number)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\nBased on these charts, it looks like 2016 saw the largest average grant amount. In all other years, grants averaged around £60,000, but in 2016 it jumped up to £80,000. If we look at total grants, though, we can see that there were far fewer grants awarded in 2016—only 221! 2017 and 2018 were much bigger years with far more money awarded.\nWe can also use multiple aesthetics to reveal more information from the data. First we’ll make a new small summary dataset and group by both year and grant program. With those groups, we’ll again calculate the total, average, and number.\n\n\nCode\nbbc_year_size &lt;- bbc %&gt;% \n  group_by(grant_year, grant_program) %&gt;% \n  summarize(total = sum(grant_amount),\n            avg = mean(grant_amount),\n            number = n())\n\nbbc_year_size\n## # A tibble: 8 × 5\n## # Groups:   grant_year [4]\n##   grant_year grant_program    total    avg number\n##        &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;int&gt;\n## 1       2016 Main Grants   16405586 86345.    190\n## 2       2016 Small Grants    884902 28545.     31\n## 3       2017 Main Grants   48502923 90154.    538\n## 4       2017 Small Grants  13891355 27453.    506\n## 5       2018 Main Grants   47347789 95652.    495\n## 6       2018 Small Grants  14001603 26721.    524\n## 7       2019 Main Grants   33019492 96267.    343\n## 8       2019 Small Grants   8369324 25058.    334\n\n\nNext we’ll plot the data, mapping the grant_program column to the fill aesthetic:\n\n\nCode\nggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\nBy default, ggplot will stack the different fill colors within the same bar, but this makes it a little hard to make comparisons. While we can see that the average small grant amount was a little bigger in 2017 than in 2019, it’s harder to compare average main grant amount, since the bottoms of those sections don’t align.\nTo fix this, we can use position_dodge() to tell the columns to fit side-by-side:\n\n\nCode\nggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) +\n  geom_col(position = position_dodge())\n\n\n\n\n\n\n\n\n\nInstead of dodging, we can also facet by grant_program to separate the bars:\n\n\nCode\nggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) +\n  geom_col() +\n  facet_wrap(vars(grant_program))\n\n\n\n\n\n\n\n\n\nWe can put these in one column if we want:\n\n\nCode\nggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) +\n  geom_col() +\n  facet_wrap(vars(grant_program), ncol = 1)\n\n\n\n\n\n\n\n\n\nFinally, we can include even more variables! We have a lot of aesthetics we can work with (size, alpha, color, fill, linetype, etc.), as well as facets, so let’s add one more to show the duration of the awarded grant.\nFirst we’ll make another smaller summarized dataset, grouping by year, program, and duration and summarizing the total, average, and number of awards.\n\n\nCode\nbbc_year_size_duration &lt;- bbc %&gt;% \n  group_by(grant_year, grant_program, grant_duration_text) %&gt;% \n  summarize(total = sum(grant_amount),\n            avg = mean(grant_amount),\n            number = n())\n\nbbc_year_size_duration\n## # A tibble: 21 × 6\n## # Groups:   grant_year, grant_program [8]\n##    grant_year grant_program grant_duration_text    total    avg number\n##         &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;                  &lt;dbl&gt;  &lt;dbl&gt;  &lt;int&gt;\n##  1       2016 Main Grants   2 years                97355 48678.      2\n##  2       2016 Main Grants   3 years             16308231 86746.    188\n##  3       2016 Small Grants  3 years               884902 28545.     31\n##  4       2017 Main Grants   1 year                 59586 29793       2\n##  5       2017 Main Grants   2 years               825732 82573.     10\n##  6       2017 Main Grants   3 years             47617605 90528.    526\n##  7       2017 Small Grants  1 year                 10000 10000       1\n##  8       2017 Small Grants  2 years               245227 18864.     13\n##  9       2017 Small Grants  3 years             13636128 27716.    492\n## 10       2018 Main Grants   1 year                118134 59067       2\n## # ℹ 11 more rows\n\n\nNext, we’ll fill by grant program and facet by duration and show the total number of grants awarded\n\n\nCode\nggplot(bbc_year_size_duration, aes(x = grant_year, y = number, fill = grant_program)) +\n  geom_col(position = position_dodge(preserve = \"single\")) +\n  facet_wrap(vars(grant_duration_text), ncol = 1)\n\n\n\n\n\n\n\n\n\nThe vast majority of BBC Children in Need’s grants last for 3 years. Super neat."
  },
  {
    "objectID": "example/01-example.html",
    "href": "example/01-example.html",
    "title": "Introduction to R and the tidyverse",
    "section": "",
    "text": "For this example, I’m going to create a new RStudio project, download some data, put the data in the project, and make a graph of it using R Markdown. You’ll follow this same process any time you start a new project or exercise.\nTo follow along, download this CSV file here (you may need to right click on it and select “Save As…”):\n\n gapminder.csv\n\nHere’s a video walkthrough of how to get started:"
  },
  {
    "objectID": "example/01-example.html#basic-process-for-working-with-rstudio",
    "href": "example/01-example.html#basic-process-for-working-with-rstudio",
    "title": "Introduction to R and the tidyverse",
    "section": "",
    "text": "For this example, I’m going to create a new RStudio project, download some data, put the data in the project, and make a graph of it using R Markdown. You’ll follow this same process any time you start a new project or exercise.\nTo follow along, download this CSV file here (you may need to right click on it and select “Save As…”):\n\n gapminder.csv\n\nHere’s a video walkthrough of how to get started:"
  },
  {
    "objectID": "content/15-content.html",
    "href": "content/15-content.html",
    "title": "Truth, beauty, and data revisited",
    "section": "",
    "text": "Chapter 12 in The Truthful Art (Cairo 2016)\n Chapter 26 in Fundamentals of Data Visualization (Wilke 2018)\n Martin Krzywinski and Alberto Cairo, “Storytelling” (Krzywinski and Cairo 2013)\n Ben Wellington, “Making data mean more through storytelling”\n Jonathan Schwabish, “Better Data Communication,” National Bureau of Economic Research\n Why People Make Bad Charts (and What to Do When it Happens)\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nWhy are stories so powerful?\nHow are stories related to truth?\nIs it ethical to emphasize certain aspects of the facts in data more than others? How do you decide which facts to use to convince audiences?\nWhen you’re telling a story about data, you’re inherently manipulating audience emotions. Is that okay?"
  },
  {
    "objectID": "content/15-content.html#readings",
    "href": "content/15-content.html#readings",
    "title": "Truth, beauty, and data revisited",
    "section": "",
    "text": "Chapter 12 in The Truthful Art (Cairo 2016)\n Chapter 26 in Fundamentals of Data Visualization (Wilke 2018)\n Martin Krzywinski and Alberto Cairo, “Storytelling” (Krzywinski and Cairo 2013)\n Ben Wellington, “Making data mean more through storytelling”\n Jonathan Schwabish, “Better Data Communication,” National Bureau of Economic Research\n Why People Make Bad Charts (and What to Do When it Happens)\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nWhy are stories so powerful?\nHow are stories related to truth?\nIs it ethical to emphasize certain aspects of the facts in data more than others? How do you decide which facts to use to convince audiences?\nWhen you’re telling a story about data, you’re inherently manipulating audience emotions. Is that okay?"
  },
  {
    "objectID": "content/15-content.html#slides",
    "href": "content/15-content.html#slides",
    "title": "Truth, beauty, and data revisited",
    "section": "Slides",
    "text": "Slides\nThe slides for this week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands."
  },
  {
    "objectID": "content/15-content.html#videos",
    "href": "content/15-content.html#videos",
    "title": "Truth, beauty, and data revisited",
    "section": "Videos",
    "text": "Videos\nVideos for each section of the lecture are available at this YouTube playlist.\n\nIntroduction\nTelling stories with data\nCuriosity\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "content/13-content.html",
    "href": "content/13-content.html",
    "title": "Text",
    "section": "",
    "text": "Rohan Alexander, “Text as data,” chapter 16 in Telling Stories with Data\n Look through some of the chapters in Tidy Text Mining (Silge and Robinson 2017) (but definitely don’t read them all! You just need to get a taste of what modern text analysis looks like.)\nA bunch of (really) short things:\n\n Evangeline Reynolds, “Federalist Papers”\n Julia Silge, “She Giggles, He Gallops”\n Abby Ohlheiser, “These are the words most associated with men and women, according to Facebook status updates”\n Six Ideas for Displaying Qualitative Data\n Word clouds considered harmful\n Word clouds cause death… or something\n When It’s Ok to Use Word Clouds\n The Class of 2011\n Every time Ford and Kavanaugh dodged a question, in one chart\n Tweet by @s_soroka\n\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nWhy is qualitative data difficult to visualize?\nWhy are word clouds so problematic? When is (not) okay to use them?"
  },
  {
    "objectID": "content/13-content.html#readings",
    "href": "content/13-content.html#readings",
    "title": "Text",
    "section": "",
    "text": "Rohan Alexander, “Text as data,” chapter 16 in Telling Stories with Data\n Look through some of the chapters in Tidy Text Mining (Silge and Robinson 2017) (but definitely don’t read them all! You just need to get a taste of what modern text analysis looks like.)\nA bunch of (really) short things:\n\n Evangeline Reynolds, “Federalist Papers”\n Julia Silge, “She Giggles, He Gallops”\n Abby Ohlheiser, “These are the words most associated with men and women, according to Facebook status updates”\n Six Ideas for Displaying Qualitative Data\n Word clouds considered harmful\n Word clouds cause death… or something\n When It’s Ok to Use Word Clouds\n The Class of 2011\n Every time Ford and Kavanaugh dodged a question, in one chart\n Tweet by @s_soroka\n\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nWhy is qualitative data difficult to visualize?\nWhy are word clouds so problematic? When is (not) okay to use them?"
  },
  {
    "objectID": "content/13-content.html#other-resources",
    "href": "content/13-content.html#other-resources",
    "title": "Text",
    "section": "Other resources",
    "text": "Other resources\nCheck out this slide show demonstrating a bunch of different ways to visualize text, as well as this collection of 400+ different text visualization techniques (!!)"
  },
  {
    "objectID": "content/13-content.html#slides",
    "href": "content/13-content.html#slides",
    "title": "Text",
    "section": "Slides",
    "text": "Slides\nThe slides for this week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands."
  },
  {
    "objectID": "content/13-content.html#videos",
    "href": "content/13-content.html#videos",
    "title": "Text",
    "section": "Videos",
    "text": "Videos\nVideos for each section of the lecture are available at this YouTube playlist.\n\nIntroduction\nQualitative text-based data\nCrash course in computational linguistics\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "content/11-content.html",
    "href": "content/11-content.html",
    "title": "Time",
    "section": "",
    "text": "Chapter 8 in The Truthful Art (Cairo 2016)\n The Nuclear Threat—The Shadow Peace, part 1\n 11 Ways to Visualize Changes Over Time – A Guide\nA bunch of (really) short blog posts:\n\n What a Hundred Million Calls to 311 Reveal About New York (just look at the picture; you don’t need to read this unless you’re really curious about trends in 311 calls)\n A century of ocean shipping animated\n What is seasonal adjustment and why is it used?\n The start-at-zero rule\n Keeping one’s appetite after touring the sausage factory\n How Common is Your Birthday? This Visualization Might Surprise You\n\n\n\n\n\n The Fallen of World War II\n Visualizing Statistical Mix Effects and Simpson’s Paradox (Armstrong and Wattenberg 2014)\n How To Fix a Toilet (And Other Things We Couldn’t Do Without Search)\n\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nWhen is it okay (or not) to truncate the y-axis?\nIt is remarkably easy to mislead people with many of these chart types. Why? How can you avoid the same mistakes?\nAll these types of charts are good at communicating change over time, but some are more appropriate in different situations. When is it best to use these different types (e.g. line graphs vs. area graphs vs. horizon charts vs. heatmaps, etc.)?"
  },
  {
    "objectID": "content/11-content.html#readings",
    "href": "content/11-content.html#readings",
    "title": "Time",
    "section": "",
    "text": "Chapter 8 in The Truthful Art (Cairo 2016)\n The Nuclear Threat—The Shadow Peace, part 1\n 11 Ways to Visualize Changes Over Time – A Guide\nA bunch of (really) short blog posts:\n\n What a Hundred Million Calls to 311 Reveal About New York (just look at the picture; you don’t need to read this unless you’re really curious about trends in 311 calls)\n A century of ocean shipping animated\n What is seasonal adjustment and why is it used?\n The start-at-zero rule\n Keeping one’s appetite after touring the sausage factory\n How Common is Your Birthday? This Visualization Might Surprise You\n\n\n\n\n\n The Fallen of World War II\n Visualizing Statistical Mix Effects and Simpson’s Paradox (Armstrong and Wattenberg 2014)\n How To Fix a Toilet (And Other Things We Couldn’t Do Without Search)\n\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nWhen is it okay (or not) to truncate the y-axis?\nIt is remarkably easy to mislead people with many of these chart types. Why? How can you avoid the same mistakes?\nAll these types of charts are good at communicating change over time, but some are more appropriate in different situations. When is it best to use these different types (e.g. line graphs vs. area graphs vs. horizon charts vs. heatmaps, etc.)?"
  },
  {
    "objectID": "content/11-content.html#slides",
    "href": "content/11-content.html#slides",
    "title": "Time",
    "section": "Slides",
    "text": "Slides\nThe slides for this week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands."
  },
  {
    "objectID": "content/11-content.html#videos",
    "href": "content/11-content.html#videos",
    "title": "Time",
    "section": "Videos",
    "text": "Videos\nVideos for each section of the lecture are available at this YouTube playlist.\n\nIntroduction\nAxis issues\nVisualizing time\nStarting, ending, and decomposing time\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "content/09-content.html",
    "href": "content/09-content.html",
    "title": "Annotations",
    "section": "",
    "text": "Chapter 17, Chapter 18, Chapter 19, and Chapter 21 in Fundamentals of Data Visualization (Wilke 2018)\n Chapter 5 in Data Visualization (Healy 2018)\n Cara Thompson, “Level Up Your Labels: Tips and Tricks for Annotating Plots”"
  },
  {
    "objectID": "content/09-content.html#readings",
    "href": "content/09-content.html#readings",
    "title": "Annotations",
    "section": "",
    "text": "Chapter 17, Chapter 18, Chapter 19, and Chapter 21 in Fundamentals of Data Visualization (Wilke 2018)\n Chapter 5 in Data Visualization (Healy 2018)\n Cara Thompson, “Level Up Your Labels: Tips and Tricks for Annotating Plots”"
  },
  {
    "objectID": "content/09-content.html#slides",
    "href": "content/09-content.html#slides",
    "title": "Annotations",
    "section": "Slides",
    "text": "Slides\nThe slides for this week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands."
  },
  {
    "objectID": "content/09-content.html#videos",
    "href": "content/09-content.html#videos",
    "title": "Annotations",
    "section": "Videos",
    "text": "Videos\nVideos for each section of the lecture are available at this YouTube playlist.\n\nIntroduction\nFretting the little things\nText in plots\nSeeds\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "content/07-content.html",
    "href": "content/07-content.html",
    "title": "Relationships",
    "section": "",
    "text": "Chapter 9 in The Truthful Art (Cairo 2016)\n Andrew Heiss, “Marginalia”\n Chapter 12 in Fundamentals of Data Visualization (Wilke 2018)\n Kieran Healy, “Two y-axes”\n Two Alternatives to Using a Second Y-Axis & Illusion of success & Dissecting two axes\n Dual-Scaled Axes in Graphs: Are They Ever the Best Solution? (Few 2008)\n\n\n\n\n “A Study on Dual-Scale Data Charts” (Isenberg et al. 2011)\n\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nHow can you correctly and honestly communicate relationships between variables? How can you communicate the uncertainty in those relationships?\nWhat are the dangers of visualizing two variables?\nWhen is it appropriate to use two y-axes?"
  },
  {
    "objectID": "content/07-content.html#readings",
    "href": "content/07-content.html#readings",
    "title": "Relationships",
    "section": "",
    "text": "Chapter 9 in The Truthful Art (Cairo 2016)\n Andrew Heiss, “Marginalia”\n Chapter 12 in Fundamentals of Data Visualization (Wilke 2018)\n Kieran Healy, “Two y-axes”\n Two Alternatives to Using a Second Y-Axis & Illusion of success & Dissecting two axes\n Dual-Scaled Axes in Graphs: Are They Ever the Best Solution? (Few 2008)\n\n\n\n\n “A Study on Dual-Scale Data Charts” (Isenberg et al. 2011)\n\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nHow can you correctly and honestly communicate relationships between variables? How can you communicate the uncertainty in those relationships?\nWhat are the dangers of visualizing two variables?\nWhen is it appropriate to use two y-axes?"
  },
  {
    "objectID": "content/07-content.html#slides",
    "href": "content/07-content.html#slides",
    "title": "Relationships",
    "section": "Slides",
    "text": "Slides\nThe slides for this week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands."
  },
  {
    "objectID": "content/07-content.html#videos",
    "href": "content/07-content.html#videos",
    "title": "Relationships",
    "section": "Videos",
    "text": "Videos\nVideos for each section of the lecture are available at this YouTube playlist.\n\nIntroduction\nThe dangers of dual y-axes\nVisualizing correlations\nVisualizing regressions\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "content/05-content.html",
    "href": "content/05-content.html",
    "title": "Themes",
    "section": "",
    "text": "Chapter 22 in Fundamentals of Data Visualization (Wilke 2018)\n Clara Granell, “ggplot2 Theme System Cheatsheet”\n Henry Wang, “ggplot2 Theme Elements Demonstration”\n Andrew Heiss, “Quick and easy ways to deal with long labels in ggplot2”\n Glance through the documentation for ggplot’s complete themes and theme(), especially the examples near the bottom\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nHow do the principles of CRAP apply to graph design and other theme elements?"
  },
  {
    "objectID": "content/05-content.html#readings",
    "href": "content/05-content.html#readings",
    "title": "Themes",
    "section": "",
    "text": "Chapter 22 in Fundamentals of Data Visualization (Wilke 2018)\n Clara Granell, “ggplot2 Theme System Cheatsheet”\n Henry Wang, “ggplot2 Theme Elements Demonstration”\n Andrew Heiss, “Quick and easy ways to deal with long labels in ggplot2”\n Glance through the documentation for ggplot’s complete themes and theme(), especially the examples near the bottom\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nHow do the principles of CRAP apply to graph design and other theme elements?"
  },
  {
    "objectID": "content/05-content.html#slides",
    "href": "content/05-content.html#slides",
    "title": "Themes",
    "section": "Slides",
    "text": "Slides\nThe slides for this week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands."
  },
  {
    "objectID": "content/05-content.html#videos",
    "href": "content/05-content.html#videos",
    "title": "Themes",
    "section": "Videos",
    "text": "Videos\nVideos for each section of the lecture are available at this YouTube playlist.\n\nIntroduction\nCRAP and ggplot\nThe anatomy of a ggplot theme\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "content/03-content.html",
    "href": "content/03-content.html",
    "title": "Mapping data to graphics",
    "section": "",
    "text": "Hans Rosling, “200 Countries, 200 Years, 4 Minutes”\n Chapter 2 in Fundamentals of Data Visualization (Wilke 2018)\n Chapter 3 in Data Visualization (Healy 2018)\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nWhy is it important to visualize variables and data?\nWhat does it mean to map data to graph aesthetics?\nWhat data was mapped to which aesthetics in Rosling’s video?"
  },
  {
    "objectID": "content/03-content.html#readings",
    "href": "content/03-content.html#readings",
    "title": "Mapping data to graphics",
    "section": "",
    "text": "Hans Rosling, “200 Countries, 200 Years, 4 Minutes”\n Chapter 2 in Fundamentals of Data Visualization (Wilke 2018)\n Chapter 3 in Data Visualization (Healy 2018)\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nWhy is it important to visualize variables and data?\nWhat does it mean to map data to graph aesthetics?\nWhat data was mapped to which aesthetics in Rosling’s video?"
  },
  {
    "objectID": "content/03-content.html#other-resources",
    "href": "content/03-content.html#other-resources",
    "title": "Mapping data to graphics",
    "section": "Other resources",
    "text": "Other resources\nThis tutorial by Cédric Scherer is an excellent demonstration of the grammar of graphics and the sequential nature of building up a plot layer-by-layer."
  },
  {
    "objectID": "content/03-content.html#slides",
    "href": "content/03-content.html#slides",
    "title": "Mapping data to graphics",
    "section": "Slides",
    "text": "Slides\nThe slides for this week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands."
  },
  {
    "objectID": "content/03-content.html#videos",
    "href": "content/03-content.html#videos",
    "title": "Mapping data to graphics",
    "section": "Videos",
    "text": "Videos\nVideos for each section of the lecture are available at this YouTube playlist.\n\nIntroduction\nData, aesthetics, and the grammar of graphics\nGrammatical layers\nAesthetics in extra dimensions\nTidy data\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "Truth, beauty, and data + R and tidyverse",
    "section": "",
    "text": "The syllabus, content, lessons, examples, and assignments pages for this class\n Tim Harford, “Florence Nightingale: Data Viz Pioneer,” 99% Invisible, episode 433, March 2, 2021 (this is a podcast; listen to it in your browser or use an app like Overcast or Spotify)\n Chapter 1 in Data Visualization (Healy 2018)\n Chapters 2 and 3 in The Truthful Art (Cairo 2016) (skim the introduction and chapter 1)\n Study: Charts change hearts and minds better than words do\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nHow do we know what is true?\nAre facts truth?\nWhy do we visualize data?\nWhat makes a great visualization?\nHow do you choose which kind of visualization to use?"
  },
  {
    "objectID": "content/01-content.html#readings",
    "href": "content/01-content.html#readings",
    "title": "Truth, beauty, and data + R and tidyverse",
    "section": "",
    "text": "The syllabus, content, lessons, examples, and assignments pages for this class\n Tim Harford, “Florence Nightingale: Data Viz Pioneer,” 99% Invisible, episode 433, March 2, 2021 (this is a podcast; listen to it in your browser or use an app like Overcast or Spotify)\n Chapter 1 in Data Visualization (Healy 2018)\n Chapters 2 and 3 in The Truthful Art (Cairo 2016) (skim the introduction and chapter 1)\n Study: Charts change hearts and minds better than words do\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nHow do we know what is true?\nAre facts truth?\nWhy do we visualize data?\nWhat makes a great visualization?\nHow do you choose which kind of visualization to use?"
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "Truth, beauty, and data + R and tidyverse",
    "section": "Slides",
    "text": "Slides\nThe slides for this week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands."
  },
  {
    "objectID": "content/01-content.html#videos",
    "href": "content/01-content.html#videos",
    "title": "Truth, beauty, and data + R and tidyverse",
    "section": "Videos",
    "text": "Videos\nVideos for each section of the lecture are available at this YouTube playlist.\n\nIntroduction\nFacts, truth, and beauty\nData, truth, and beauty\nBeautiful visualizations\nClass details\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "assignment/final-project.html",
    "href": "assignment/final-project.html",
    "title": "Final project",
    "section": "",
    "text": "You made it to the end of our whirlwind tour of data visualization principles! Congratulations!\nNow you get to show off all the tools you learned with a beautiful, truthful, narrative visualization.\nFor your final project, you will take a dataset, explore it, tinker with it, and tell a nuanced story about it using at least three graphs.\nI want this project to be as useful for you and your future career as possible—you’ll hopefully want to show off your final project in a portfolio or during job interviews.\nAccordingly, you have some choice in what data you can use for this project. I’ve found several different high-quality datasets online related to the core MPA/MPP tracks. You do not have to choose a dataset in your given field (especially if you’re not an MPA or MPP student!) Choose whatever one you are most interested in or will have the most fun with."
  },
  {
    "objectID": "assignment/final-project.html#data-from-the-internet",
    "href": "assignment/final-project.html#data-from-the-internet",
    "title": "Final project",
    "section": "Data from the internet",
    "text": "Data from the internet\nGo to this list of data sources and find something interesting! The things in the “Data is Plural” newsletter are often especially interesting and fun. Here are some different high-quality datasets that students have worked with before:\n\nNonprofit management\n\nU.S. Charities and Non-profits: All of the charities and nonprofits registered with the IRS. This is actually split into six separate files. You can combine them all into one massive national database with bind_rows(), or filter the data to include specific states (or a single state). It all depends on the story you’re telling. Source: IRS.\nNonprofit Grants 2010 to 2016: Nonprofit grants made in the US as listed in Schedule I of the IRS 990 tax form between 2010 to 2016. Source: IRS.\n\n\n\nFederal, state, and local government management\n\nDeadly traffic accidents in the UK (2015): List of all traffic-related deaths in the UK in 2015. Source: data.gov.uk.\nFirefighter Fatalities in the United States: Name, rank, and cause of death for all firefighters killed since 2000. Source: FEMA.\nFederal Emergencies and Disasters, 1953–Present: Every federal emergency or disaster declared by the President of the United States since 1953. Source: FEMA.\nGlobal Terrorism Database (1970–2016): 170,000 terrorist attacks worldwide, 1970-2016. Source: National Consortium for the Study of Terrorism and Responses to Terrorism (START), University of Maryland.\nCity of Austin 311 Unified Data: All 311 calls to the City of Austin since 2014. Source: City of Austin.\n\n\n\nBusiness management\n\n515K Hotel Reviews Data in Europe: 515,000 customer reviews and scoring of 1,493 luxury hotels across Europe. Source: Booking.com.\nChase Bank Branch Deposits, 2010–2016: Records for every branch of Chase Bank in the United States. This dataset is not quite tidy and will require a little bit of reshaping with pivot_longer(), since there are individual columns of deposits per year. Source: Chase Bank."
  },
  {
    "objectID": "assignment/final-project.html#instructions",
    "href": "assignment/final-project.html#instructions",
    "title": "Final project",
    "section": "Instructions",
    "text": "Instructions\nHere’s what you’ll need to do:\n\nDownload a dataset and explore it. Many of these datasets are large and will not open (well) in Excel, so you’ll need to load the CSV file into R with read_csv(). Most of these datasets have nice categorical variables that you can use for grouping and summarizing, and many have time components too, so you can look at trends. Your past problem sets and in-class examples will come in handy here.\nFind a story in the data. Explore that story and make sure it’s true and insightful.\nUse R to create multiple graphs to tell the story. You can make as many graphs as you want, but you must use at least three different chart types (i.e. don’t just make three scatterplots or three maps).\nExport these figures as PDF files, place them in a vector editing program, and make one combined graphic or handout where you tell the complete story. You have a lot of latitude in how you do this. You can make a graphic-heavy one-page handout. You can make something along the lines of the this, with one big graphic + smaller subgraphics + explanatory text. Just don’t make a goofy infographic. Whatever you do, the final figure must include all the graphics, must have some explanatory text to help summarize the narrative, and must be well designed.\nExport the final graphic from Illustrator as a PDF and a PNG.\nWrite a memo using R Markdown to introduce, frame, and describe your story and figure. Use this template to get started. You should include the following in the memo:\n\nExecutive summary\nBackground information and summary of the data\nExplanation, description, and code for each individual figure\nExplanation and description for the final figure\nFinal figure should be included as an image (remember ![Caption goes here](path/to/file.png))\n\n\nRemember to follow R Markdown etiquette rules and style—don’t have it output extraneous messages or warnings, include summary tables in nice tables, adjust the dimensions for your figures, and remove the placeholder text that’s in the template already (i.e. I don’t want to see stuff like “Describe and show how you cleaned and reshaped the data” in the final report.)\nYou should download a full example of what a final project might look like (but don’t make your final combined visualization look exactly like this—show some creativity!)"
  },
  {
    "objectID": "assignment/final-project.html#final-deliverables",
    "href": "assignment/final-project.html#final-deliverables",
    "title": "Final project",
    "section": "Final deliverables",
    "text": "Final deliverables\nUpload the following files to Canvas:\n\nA memo introducing and describing your final graphic (see full instructions above)\nA standalone PDF of your graphic exported from Illustrator\nA standalone PNG of your graphic exported from Illustrator\n\nNo late work will be accepted for this project since it’s the last project and it counts as your final.\nI will use this rubric to grade the final product:\n\n final-project-rubric.xlsx\n\nI am happy to give feedback and help along the way—please don’t hesitate to get help! My goal is for you to have a beautiful graphic in the end that you’ll want to show off to all your friends, family, neighbors, employers, and strangers on the street—I’m not trying to trip you up or give you trick questions!\nAnd that’s it. You’re done! Go out into the world now and make beautiful, insightful, and truthful graphics.\nGo forth and make awesomeness."
  },
  {
    "objectID": "assignment/final-project.html#past-examples",
    "href": "assignment/final-project.html#past-examples",
    "title": "Final project",
    "section": "Past examples",
    "text": "Past examples\nDownload a full example of what a final project might look like.\nHere are some great examples of student projects from past versions of this class.\n\nTravel runs in Yellowstone\n Project description   Final PDF   Final PNG\n\n\n\nYellowstone travel final project\n\n\n\n\nFirefighter fatalities\n Project description   Final PDF   Final PNG\n\n\n\nFirefighter fatalities final project\n\n\n\n\nScripture use by The Killers\n Project description   Final PDF   Final PNG\n\n\n\nFirefighter fatalities final project\n\n\n\n\nUtah nonprofits\n Project description   Final PDF   Final PNG\n\n\n\nFirefighter fatalities final project\n\n\n\n\nBuckethead\n Project description   Final PDF   Final PNG\n\n\n\nFirefighter fatalities final project"
  },
  {
    "objectID": "assignment/14-exercise.html",
    "href": "assignment/14-exercise.html",
    "title": "Enhancing graphics",
    "section": "",
    "text": "For this exercise, you’ll export a PDF and/or an SVG from R, open it in a vector edting program, add annotations and make minor edits, and then export a final polished version.\nI have given you 100% of the R code you need to use. All you have to do is run it. I’ve also given you all the text and pieces of the final plot—see those below.\nThis example page will be incredibly useful for you:\nYou’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\nYou don’t need to make your plots super fancy (except for these annotations), but you should play around with the plots and try to go beyond the basics—experiment with changing colors or modifying themes and theme elements.\nAnd as always, if you’re struggling, please talk to me and use Slack and talk to your classmates! Don’t suffer in silence!"
  },
  {
    "objectID": "assignment/14-exercise.html#instructions",
    "href": "assignment/14-exercise.html#instructions",
    "title": "Enhancing graphics",
    "section": "Instructions",
    "text": "Instructions\n\nIf you’re using R on your own computer, download this file, unzip it, and double click on the file named exercise-14.Rproj:  14-exercise.zip\nYou’ll need to make sure you have these packages installed on your computer: tidyverse, svglite. If you try to load one of those packages with library(tidyverse) or library(svglite), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Exercise 14” on Posit.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Exercise 14”.)\nRename the R Markdown file named your-name_exercise-14.Rmd to something that matches your name and open it in RStudio.\nComplete the tasks given in the R Markdown file.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\n\n\n\n\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n\nWhen you’re all done, click on the “Knit” button at the top of the editing window and create a Word or PDF version (if you’ve installed tinytex) of your document. Upload that file to Canvas. Do not upload a knitted HTML file (they don’t work on Canvas)."
  },
  {
    "objectID": "assignment/14-exercise.html#task-2-details-hot-dog-eating-contest-winners",
    "href": "assignment/14-exercise.html#task-2-details-hot-dog-eating-contest-winners",
    "title": "Enhancing graphics",
    "section": "Task 2 details: Hot dog eating contest winners",
    "text": "Task 2 details: Hot dog eating contest winners\nRecreate this plot (or something like it):\n\n\n\nCreate and save a basic bar chart of hot dog eating contest winners using the code provided. Open the resulting file in a vector editing program.\nBe sure that you save your file with a different name. You don’t want to accidentally overwrite all your enhancements and updates when you knit this document. That would be so sad.\nYou don’t have data prior to 1980, so don’t worry about recreating that half of the graph. You don’t have to put all the text boxes in exactly the same locations—you can even do a completely different design and add different annotations if you want.\nThe point of this assignment is to help you get familiar with vector editing software, so don’t stress out about R issues or graphic design issues (though try to follow CRAP where possible).\n\n\n\n\n\n\nPlot text\n\n\n\nTo save you some typing, here’s all the text from the original plot. Copy and paste it into your enhanced version (or change the text if you want—again, do whatever you want):\n\nWinners from Nathan’s Hot Dog Eating Contest\nIt’s that time of year again. Since 1916, the annual eating competition has grown substantially attracting competitors from around the world\nFrank Dellarosa eats 21 and a half HDBs over 12 minutes, breaking the previous record of 19 and a half\nThrough 2001-2005, Takeru Kobayashi wins by no less than 12 HDBs. In 2006 he only wins by 1.75. After winning 6 years in a row and setting the world record 4 times, Kobayashi places second in 2007.\nFor the first time since 1999, an American reclaims the title when Joey Chestnut consumes 66 HDBs, a new world record. Chestnut repeats in 2008.\nSource: Wikipedia and Nathan’s Famous"
  },
  {
    "objectID": "assignment/12-exercise.html",
    "href": "assignment/12-exercise.html",
    "title": "Space",
    "section": "",
    "text": "For this exercise, you’ll visualize the proportion of the world that uses the internet. You’ll use data from Max Roser’s Our World in Data project, which collects all sorts of interesting cross-national data. You’ll also use national shapefiles from Natural Earth.\nYou’ll also make a map showing any kind of data you want.\nThis example page will be incredibly useful for you:\nYou’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\nThis can be as simple or as complex as you want. You don’t need to make your map super fancy, but if you’re feeling brave, experiment with changing colors or modifying themes and theme elements.\nAnd as always, if you’re struggling, please talk to me and use Slack and talk to your classmates! Don’t suffer in silence!"
  },
  {
    "objectID": "assignment/12-exercise.html#instructions",
    "href": "assignment/12-exercise.html#instructions",
    "title": "Space",
    "section": "Instructions",
    "text": "Instructions\n\nIf you’re using R on your own computer, download this file, unzip it, and double click on the file named exercise-12.Rproj:  12-exercise.zip\nYou’ll need to make sure you have these packages installed on your computer: tidyverse, sf. If you try to load one of those packages with library(tidyverse) or library(sf), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Exercise 12” on Posit.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Exercise 12”.)\nRename the R Markdown file named your-name_exercise-12.Rmd to something that matches your name and open it in RStudio.\nComplete the tasks given in the R Markdown file.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\n\n\n\n\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n\nWhen you’re all done, click on the “Knit” button at the top of the editing window and create a Word or PDF version (if you’ve installed tinytex) of your document. Upload that file to Canvas. Do not upload a knitted HTML file (they don’t work on Canvas)."
  },
  {
    "objectID": "assignment/10-exercise.html",
    "href": "assignment/10-exercise.html",
    "title": "Interactivity",
    "section": "",
    "text": "For this exercise, you’ll use whatever data you want to make a plot and make it interactive. Use a dataset from a past exercise, use one of the built-in datasets like mpg or gapminder from the {gapminder} package, download stuff from the World Bank using the {WDI} package, or use something from this list of datasets. Whatever you want.\nThe example for this week’s session will be helpful as you tinker with ggplotly(), and the resources listed at the bottom of the example will be helpful for making a dashboard.\nYou’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\nAnd as always, if you’re struggling, please talk to me and use Slack and talk to your classmates! Don’t suffer in silence!"
  },
  {
    "objectID": "assignment/10-exercise.html#instructions",
    "href": "assignment/10-exercise.html#instructions",
    "title": "Interactivity",
    "section": "Instructions",
    "text": "Instructions\n\nIf you’re using R on your own computer, download this file, unzip it, and double click on the file named exercise-10.Rproj:  10-exercise.zip\nYou’ll need to make sure you have these packages installed on your computer: tidyverse, plotly, flexdashboard. If you try to load one of those packages with library(tidyverse) or library(plotly), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Exercise 10” on Posit.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Exercise 10”.)\nRename the R Markdown file named your-name_exercise-10.Rmd to something that matches your name and open it in RStudio.\nComplete the tasks given in the R Markdown file.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page—don’t try to write everything from scratch!).\nHere’s where this is all different this time. You will not upload a knitted PDF or Word file to Canvas, since those can’t handle interactivity. Instead, do this:\n\nKnit the document with Tasks 1 and 2 in it to HTML and publish it to RPubs using the “Publish document” menu in the preview of the knitted file. Take note of the URL.\n\n\n\n\n\n\nKnit the dashboard from Task 3 to HTML and publish it to RPubs using the same menu. Take note of the URL.\nIn Canvas, paste the two URLs into the submission form for exercise 10 following this template:\nTask 1 and 2: URL HERE\nTask 3 dashboard: URL HERE"
  },
  {
    "objectID": "assignment/08-exercise.html",
    "href": "assignment/08-exercise.html",
    "title": "Comparisons",
    "section": "",
    "text": "For this exercise you’ll use state-level unemployment data from 2006 to 2016 that comes from the US Bureau of Labor Statistics. This example page will be incredibly useful for you:\nYou’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\nYou don’t need to make your plots super fancy, but you should play around with the plots and try to go beyond the basics—experiment with adding a labs() layer or changing colors or modifying themes and theme elements.\nAnd as always, if you’re struggling, please talk to me and use Slack and talk to your classmates! Don’t suffer in silence!"
  },
  {
    "objectID": "assignment/08-exercise.html#instructions",
    "href": "assignment/08-exercise.html#instructions",
    "title": "Comparisons",
    "section": "Instructions",
    "text": "Instructions\n\nIf you’re using R on your own computer, download this file, unzip it, and double click on the file named exercise-8.Rproj:  08-exercise.zip\nYou’ll need to make sure you have these packages installed on your computer: tidyverse, broom. If you try to load one of those packages with library(tidyverse) or library(broom), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Exercise 8” on Posit.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Exercise 8”.)\nRename the R Markdown file named your-name_exercise-8.Rmd to something that matches your name and open it in RStudio.\nComplete the tasks given in the R Markdown file.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\n\n\n\n\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n\nWhen you’re all done, click on the “Knit” button at the top of the editing window and create a Word or PDF version (if you’ve installed tinytex) of your document. Upload that file to Canvas. Do not upload a knitted HTML file (they don’t work on Canvas)."
  },
  {
    "objectID": "assignment/06-exercise.html",
    "href": "assignment/06-exercise.html",
    "title": "Uncertainty",
    "section": "",
    "text": "For this exercise you’ll revisit Hans Rosling’s gapminder data on health and wealth. This example page will be incredibly useful for you:\nYou’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\nAnd as always, if you’re struggling, please talk to me and use Slack and talk to your classmates! Don’t suffer in silence!"
  },
  {
    "objectID": "assignment/06-exercise.html#instructions",
    "href": "assignment/06-exercise.html#instructions",
    "title": "Uncertainty",
    "section": "Instructions",
    "text": "Instructions\n\nIf you’re using R on your own computer, download this file, unzip it, and double click on the file named exercise-6.Rproj:  06-exercise.zip\nYou’ll need to make sure you have these packages installed on your computer: tidyverse, gapminder, ggridges, gghalves. If you try to load one of those packages with library(tidyverse) or library(gapminder), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Exercise 6” on Posit.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Exercise 6”.)\nRename the R Markdown file named your-name_exercise-6.Rmd to something that matches your name and open it in RStudio.\nComplete the tasks given in the R Markdown file.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\n\n\n\n\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n\nWhen you’re all done, click on the “Knit” button at the top of the editing window and create a Word or PDF version (if you’ve installed tinytex) of your document. Upload that file to Canvas. Do not upload a knitted HTML file (they don’t work on Canvas)."
  },
  {
    "objectID": "assignment/04-exercise.html",
    "href": "assignment/04-exercise.html",
    "title": "Amounts and proportions",
    "section": "",
    "text": "The New York City Department of Buildings (DOB) maintains a list of construction sites that have been categorized as “essential” during the city’s shelter-in-place pandemic order. They’ve provided an interactive map here where you can see the different projects. There’s also a link there to download the complete dataset.\nFor this exercise, you’re going to use this data to visualize the amounts or proportions of different types of essential projects in the five boroughs of New York City (Brooklyn, Manhattan, the Bronx, Queens, and Staten Island).\nYou’ll be doing all your R work in R Markdown this time (and from now on). You can download a zipped file of a pre-made project here:\nYou don’t need to make your plots super fancy, but if you’re feeling brave, experiment with adding a labs() layer or changing fill colors with scale_fill_manual() or with viridis palettes.\nAnd as always, if you’re struggling, please talk to me and use Slack and talk to your classmates! Don’t suffer in silence!"
  },
  {
    "objectID": "assignment/04-exercise.html#instructions",
    "href": "assignment/04-exercise.html#instructions",
    "title": "Amounts and proportions",
    "section": "Instructions",
    "text": "Instructions\n\nIf you’re using R on your own computer, download this file, unzip it, and double click on the file named exercise-4.Rproj:  04-exercise.zip\nYou’ll need to make sure you have the tidyverse package installed on your computer. If you try to load it with library(tidyverse) and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Exercise 4” on Posit.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Exercise 4”.)\nRename the R Markdown file named your-name_exercise-4.Rmd to something that matches your name and open it in RStudio.\nComplete the tasks given in the R Markdown file.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\n\n\n\n\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n\nWhen you’re all done, click on the “Knit” button at the top of the editing window and create a Word or PDF version (if you’ve installed tinytex) of your document. Upload that file to Canvas. Do not upload a knitted HTML file (they don’t work on Canvas)."
  },
  {
    "objectID": "assignment/02-mini-project.html",
    "href": "assignment/02-mini-project.html",
    "title": "Mini project 2",
    "section": "",
    "text": "The United States has resettled more than 600,000 refugees from 60 different countries since 2006.\nRefugees welcome, by Attila Szervác\nIn this mini project, you will use R, ggplot, and a vector editing program to explore where these refugees have come from."
  },
  {
    "objectID": "assignment/02-mini-project.html#instructions",
    "href": "assignment/02-mini-project.html#instructions",
    "title": "Mini project 2",
    "section": "Instructions",
    "text": "Instructions\nHere’s what you need to do:\n\nCreate a new RStudio project and place it on your computer somewhere. Open that new folder in Windows File Explorer or macOS Finder (however you navigate around the files on your computer), and create two subfolders there named data and output.\nDownload the Department of Homeland Security’s annual count of people granted refugee status between 2006-2015:\n DHS refugees, 2006-2015\nPlace this in the data subfolder you created in step 1. You might need to right click on this link and choose “Save link as…”, since your browser may try to display it as text. This data was originally uploaded by the Department of Homeland Security to Kaggle, and is provided with a public domain license.\nCreate a new R Markdown file and save it in your project. In RStudio go to File &gt; New File &gt; R Markdown…, choose the default options, and delete all the placeholder text in the new file except for the metadata at the top, which is between --- and ---.\nVerify that your project folder is structured like this:\nyour-project-name/\n  your-analysis.Rmd\n  your-project-name.Rproj\n  data/\n    refugee_status.csv\n  output/\n    NOTHING\nClean the data using the code I’ve given you below.\nSummarize the data somehow. There is data for 60 countries over 10 years, so you’ll probably need to aggregate or reshape the data somehow (unless you do a 60-country sparkline).\nCreate an appropriate time-based visualization based on the data. I’ve shown a few different ways to summarize the data so that it’s plottable down below. Don’t just calculate overall averages or totals per country—the visualization needs to deal with change over time. Do as much polishing and refining in R—make adjustments to the colors, scales, labels, grid lines, and even fonts, etc.\nSave the figure as a PDF. Use this:\nggsave(plot_name, filename = \"output/blah.pdf\", width = XX, height = XX)\nRefine and polish the saved PDF in a vector editing program, adding annotations, changing colors, and otherwise enhancing it.\nExport the polished image as a PDF and a PNG file.\nWrite a memo (no word limit) explaining your process. I’m specifically looking for the following:\n\nWhat story are you telling with your graphic?\nHow did you apply the principles of CRAP?\nHow did you apply Kieran Healy’s principles of great visualizations or Alberto Cairo’s five qualities of great visualizations?\n\nUpload the following outputs to Canvas:\n\nA PDF or Word file of your memo with your final code, intermediate graphic (the one you create in R), and final graphic (the one you enhance) in it. Remember to use ![Caption](path/to/figure/here) to place external images in Markdown.\nA standalone PNG version of your graphic. You’ll export this from Illustrator or Inkscape.\nA standalone PDF version of your graphic. You’ll export this from Illustrator or Inkscape.\n\n\nYou will be graded based on completion using the standard ✓ system, but I’ll provide comments on how you use R and {ggplot2}, how well you apply the principles of CRAP, The Truthful Art, and Effective Data Visualization, and how appropriate the graph is for the data and the story you’re telling. I will use this rubric to make comments and provide you with a simulated grade.\n\n mini-project-2-rubric.pdf\n\nFor this assignment, I am less concerned with the code (that’s why I gave most of it to you), and more concerned with the design. Choose good colors based on palettes. Choose good, clean fonts. Use the heck out of theme(). Add informative design elements in Illustrator/Inkscape/Affinity Designer. Make it look beautiful and CRAPpy. Refer to the design resources here.\nPlease seek out help when you need it! You know enough R (and have enough examples of code from class and your readings) to be able to do this. Your project has to be turned in individually, and your visualization should be your own (i.e. if you work with others, don’t all turn in the same graph), but you should work with others! Reach out to me for help too—I’m here to help!\nYou can do this."
  },
  {
    "objectID": "assignment/02-mini-project.html#data-cleaning-code",
    "href": "assignment/02-mini-project.html#data-cleaning-code",
    "title": "Mini project 2",
    "section": "Data cleaning code",
    "text": "Data cleaning code\nThe data isn’t perfectly clean and tidy, but it’s real world data, so this is normal. Because the emphasis for this assignment is on design, not code, I’ve provided code to help you clean up the data.\nThese are the main issues with the data:\n\nThere are non-numeric values in the data, like -, X, and D. The data isn’t very well documented; I’m assuming - indicates a missing value, but I’m not sure what X and D mean, so for this assignment, we’ll just assume they’re also missing.\nThe data generally includes rows for dozens of countries, but there are also rows for some continents, “unknown,” “other,” and a total row. Because Africa is not a country, and neither are the other continents, we want to exclude all non-countries.\nMaintaining consistent country names across different datasets is literally the woooooooorst. Countries have different formal official names and datasets are never consistent in how they use those names.1 It’s such a tricky problem that social scientists have spent their careers just figuring out how to properly name and code countries. Really.2 There are international standards for country codes, though, like ISO 3166-1 alpha 3 (my favorite), also known as ISO3. It’s not perfect—it omits microstates (some Polynesian countries) and gray area states (Palestine, Kosovo)—but it’s an international standard, so it has that going for it.\nTo ensure that country names are consistent in this data, we use the {countrycode} package (install it if you don’t have it), which is amazing. The countrycode() function will take a country name in a given coding scheme and convert it to a different coding scheme using this syntax:\ncountrycode(variable, \"current-coding-scheme\", \"new-coding-scheme\")\nIt also does a farily good job at guessing and parsing inconsistent country names (e.g. it will recognize “Congo, Democratic Republic”, even though it should technically be “Democratic Republic of the Congo”). Here, we use countrycode() to convert the inconsistent country names into ISO3 codes. We then create a cleaner version of the origin_country column by converting the ISO3 codes back into country names. Note that the function chokes on North Korea initially, since it’s included as “Korea, North”—we use the custom_match argument to help the function out.\nThe data isn’t tidy—there are individual columns for each year. gather() takes every column and changes it to a row. We exclude the country, region, continent, and ISO3 code from the change-into-rows transformation with -origin_country, -iso3, -origin_region, -origin_continent.\nCurrently, the year is being treated as a number, but it’s helpful to also treat it as an actual date. We create a new variable named year_date that converts the raw number (e.g. 2009) into a date. The date needs to have at least a month, day, and year, so we actually convert it to January 1, 2009 with ymd(paste0(year, \"-01-01\")).\n\n\nlibrary(tidyverse)    # For ggplot, dplyr, and friends\nlibrary(countrycode)  # For dealing with country names, abbreviations, and codes\n\nrefugees_raw &lt;- read_csv(\"data/refugee_status.csv\", na = c(\"-\", \"X\", \"D\"))\n\nnon_countries &lt;- c(\"Africa\", \"Asia\", \"Europe\", \"North America\", \"Oceania\", \n                   \"South America\", \"Unknown\", \"Other\", \"Total\")\n\nrefugees_clean &lt;- refugees_raw %&gt;%\n  # Make this column name easier to work with\n  rename(origin_country = `Continent/Country of Nationality`) %&gt;%\n  # Get rid of non-countries\n  filter(!(origin_country %in% non_countries)) %&gt;%\n  # Convert country names to ISO3 codes\n  mutate(iso3 = countrycode(origin_country, \"country.name\", \"iso3c\",\n                            custom_match = c(\"Korea, North\" = \"PRK\"))) %&gt;%\n  # Convert ISO3 codes to country names, regions, and continents\n  mutate(origin_country = countrycode(iso3, \"iso3c\", \"country.name\"),\n         origin_region = countrycode(iso3, \"iso3c\", \"region\"),\n         origin_continent = countrycode(iso3, \"iso3c\", \"continent\")) %&gt;%\n  # Make this data tidy\n  gather(year, number, -origin_country, -iso3, -origin_region, -origin_continent) %&gt;%\n  # Make sure the year column is numeric + make an actual date column for years\n  mutate(year = as.numeric(year),\n         year_date = ymd(paste0(year, \"-01-01\")))"
  },
  {
    "objectID": "assignment/02-mini-project.html#data-to-possibly-use-in-your-plot",
    "href": "assignment/02-mini-project.html#data-to-possibly-use-in-your-plot",
    "title": "Mini project 2",
    "section": "Data to possibly use in your plot",
    "text": "Data to possibly use in your plot\nHere are some possible summaries of the data you might use…\n\nCountry totals over time\nThis is just the refugees_clean data frame I gave you. You’ll want to filter it and select specific countries, though—you won’t really be able to plot 60 countries all at once unless you use sparklines.\n\nrefugees_clean\n\n\n\n# A tibble: 6 × 7\n  origin_country iso3  origin_region    origin_continent  year number year_date \n  &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;    \n1 Afghanistan    AFG   South Asia       Asia              2006    651 2006-01-01\n2 Angola         AGO   Sub-Saharan Afr… Africa            2006     13 2006-01-01\n3 Armenia        ARM   Europe & Centra… Asia              2006     87 2006-01-01\n4 Azerbaijan     AZE   Europe & Centra… Asia              2006     77 2006-01-01\n5 Belarus        BLR   Europe & Centra… Europe            2006    350 2006-01-01\n6 Bhutan         BTN   South Asia       Asia              2006      3 2006-01-01\n\n\n\n\nCumulative country totals over time\nNote the cumsum() function—it calculates the cumulative sum of a column.\n\nrefugees_countries_cumulative &lt;- refugees_clean %&gt;%\n  arrange(year_date) %&gt;%\n  group_by(origin_country) %&gt;%\n  mutate(cumulative_total = cumsum(number))\n\n\n\n# A tibble: 6 × 7\n# Groups:   origin_country [1]\n  origin_country iso3  origin_continent  year number year_date  cumulative_total\n  &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;                &lt;dbl&gt;\n1 Afghanistan    AFG   Asia              2006    651 2006-01-01              651\n2 Afghanistan    AFG   Asia              2007    441 2007-01-01             1092\n3 Afghanistan    AFG   Asia              2008    576 2008-01-01             1668\n4 Afghanistan    AFG   Asia              2009    349 2009-01-01             2017\n5 Afghanistan    AFG   Asia              2010    515 2010-01-01             2532\n6 Afghanistan    AFG   Asia              2011    428 2011-01-01             2960\n\n\n\n\nContinent totals over time\nNote the na.rm = TRUE argument in sum(). This makes R ignore any missing data when calculating the total. Without it, if R finds a missing value in the column, it will mark the final sum as NA too, which we don’t want.\n\nrefugees_continents &lt;- refugees_clean %&gt;%\n  group_by(origin_continent, year_date) %&gt;%\n  summarize(total = sum(number, na.rm = TRUE))\n\n\n\n# A tibble: 6 × 3\n# Groups:   origin_continent [1]\n  origin_continent year_date  total\n  &lt;chr&gt;            &lt;date&gt;     &lt;dbl&gt;\n1 Africa           2006-01-01 18116\n2 Africa           2007-01-01 17473\n3 Africa           2008-01-01  8931\n4 Africa           2009-01-01  9664\n5 Africa           2010-01-01 13303\n6 Africa           2011-01-01  7677\n\n\n\n\nCumulative continent totals over time\nNote that there are two group_by() functions here. First we get the total number of refugees per continent per year, then we group by continent only to get the cumulative sum of refugees across continents.\n\nrefugees_continents_cumulative &lt;- refugees_clean %&gt;%\n  group_by(origin_continent, year_date) %&gt;%\n  summarize(total = sum(number, na.rm = TRUE)) %&gt;%\n  arrange(year_date) %&gt;%\n  group_by(origin_continent) %&gt;%\n  mutate(cumulative_total = cumsum(total))\n\n\n\n# A tibble: 6 × 4\n# Groups:   origin_continent [1]\n  origin_continent year_date  total cumulative_total\n  &lt;chr&gt;            &lt;date&gt;     &lt;dbl&gt;            &lt;dbl&gt;\n1 Africa           2006-01-01 18116            18116\n2 Africa           2007-01-01 17473            35589\n3 Africa           2008-01-01  8931            44520\n4 Africa           2009-01-01  9664            54184\n5 Africa           2010-01-01 13303            67487\n6 Africa           2011-01-01  7677            75164"
  },
  {
    "objectID": "assignment/02-mini-project.html#footnotes",
    "href": "assignment/02-mini-project.html#footnotes",
    "title": "Mini project 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor instance, “North Korea”, “Korea, North”, “DPRK”, “Korea, Democratic People’s Republic of”, and “Democratic People’s Republic of Korea”, and “Korea (DPRK)” are all perfectly normal versions of the country’s name and you’ll find them all in the wild.↩︎\nSee Gleditsch, Kristian S. & Michael D. Ward. 1999. “Interstate System Membership: A Revised List of the Independent States since 1816.” International Interactions 25: 393-413; or the “ICOW Historical State Names Data Set”.↩︎"
  },
  {
    "objectID": "assignment/01-mini-project.html",
    "href": "assignment/01-mini-project.html",
    "title": "Mini project 1",
    "section": "",
    "text": "New York City is full of urban wildlife, and rats are one of the city’s most infamous animal mascots. Rats in NYC are plentiful, but they also deliver food, so they’re useful too.\nNYC keeps incredibly detailed data regarding animal sightings, including rats, and it makes this data publicly available. For this first mini project, you will use R {ggplot2} to tell an interesting story hidden in that data.\n(Don’t worry! You’re not completely on your own! I’ve given you some starter code below.)"
  },
  {
    "objectID": "assignment/01-mini-project.html#instructions",
    "href": "assignment/01-mini-project.html#instructions",
    "title": "Mini project 1",
    "section": "Instructions",
    "text": "Instructions\nHere’s what you need to do:\n\nCreate a new RStudio project and place it on your computer somewhere. Open that new folder in Windows File Explorer or macOS Finder (however you navigate around the files on your computer), and create two subfolders there named data and output.\nDownload New York City’s database of rat sightings since 2010:\n\n Rat_Sightings.csv\nPlace this in the data subfolder you created in step 1. You might need to right click on this link and choose “Save link as…”, since your browser may try to display it as text. The data was originally uploaded by the City of New York to Kaggle, and is provided with a public domain license.\n\nCreate a new R Markdown file and save it in your project. In RStudio go to File &gt; New File &gt; R Markdown…, choose the default options, and delete all the placeholder text in the new file except for the metadata at the top, which is between --- and ---.\nVerify that your project folder is structured like this:\nyour-project-name/\n  your-analysis.Rmd\n  your-project-name.Rproj\n  data/\n    Rat_Sightings.csv\n  output/\n    NOTHING\nSummarize the data somehow. The raw data has more than 100,000 rows, which means you’ll need to aggregate the data (filter(), group_by(), and summarize() will be your friends). Consider looking at the number of sightings per borough, per year, per dwelling type, etc., or a combination of these, like the change in the number sightings across the 5 boroughs between 2010 and 2016.\nCreate an appropriate visualization based on the data you summarized.\nWrite a memo (no word limit) explaining your process. I’m specifically looking for a discussion of the following:\n\nWhat was wrong with the original graphic (if you’re fixing one of the original figures)?\nWhat story are you telling with your new graphic?\nHow did you apply the principles of CRAP?\nHow did you apply Kieran Healy’s principles of great visualizations or Alberto Cairo’s five qualities of great visualizations?\n\nUpload the following outputs to Canvas:\n\nA PDF or Word file of your memo with your final code and graphic embedded in it. This means you’ll need to do all your coding in an R Markdown file and embed your code in chunks.\n\n\n\n\n\n\nAssignment structure\n\n\n\nYou can approach this in a couple different ways—you can write the memo and then include the full figure and code at the end, similar to this blog post, or you can write the memo in an incremental way, describing the different steps of creating the figure, ultimately arriving at a clean final figure, like this blog post.\n\n\nA standalone PNG version of your graphic. Use this:\nggsave(plot_name, filename = \"output/blah.png\", width = XX, height = XX)\nA standalone PDF version of your graphic. Use this:\nggsave(plot_name, filename = \"output/blah.pdf\", width = XX, height = XX)\n\n\nYou will be graded based on completion using the standard ✓ system, but I’ll provide comments on how you use R and {ggplot2}, how well you apply the principles of CRAP, The Truthful Art, and Effective Data Visualization, and how appropriate the graph is for the data and the story you’re telling. I will use this rubric to make comments and provide you with a simulated grade.\n\n mini-project-1-rubric.pdf\n\nFor this assignment, I am less concerned with detailed graphic design principles—select appropriate colors, change fonts if you’re brave, and choose a nice ggplot theme and make some adjustments like moving the legend around (theme(legend.position = \"bottom\")).\nPlease seek out help when you need it! You know enough R (and have enough examples of code from class and your readings) to be able to do this. Your project has to be turned in individually, and your visualization should be your own (i.e. if you work with others, don’t all turn in the same graph), but you should work with others! Reach out to me for help too—I’m here to help!\nYou can do this, and you’ll feel like a budding dataviz witch/wizard when you’re done."
  },
  {
    "objectID": "assignment/01-mini-project.html#starter-code",
    "href": "assignment/01-mini-project.html#starter-code",
    "title": "Mini project 1",
    "section": "Starter code",
    "text": "Starter code\nI’ve provided some starter code below. A couple comments about it:\n\nBy default, read_csv() treats cells that are empty or “NA” as missing values. This rat dataset uses “N/A” to mark missing values, so we need to add that as a possible marker of missingness (hence na = c(\"\", \"NA\", \"N/A\"))\nTo make life easier, I’ve renamed some of the key variables you might work with. You can rename others if you want.\nI’ve also created a few date-related variables (sighting_year, sighting_month, sighting_day, and sighting_weekday). You don’t have to use them, but they’re there if you need them. The functions that create these, like year() and wday() are part of the {lubridate} library (which is automatically loaded when you run library(tidyverse).\nThe date/time variables are formatted like 04/03/2017 12:00:00 AM, which R is not able to automatically parse as a date when reading the CSV file. You can use the mdy_hms() function in the {lubridate} library to parse dates that are structured as “month-day-year-hour-minute”. There are also a bunch of other iterations of this function, like ymd(), dmy(), etc., for other date formats.\nThere’s one row with an unspecified borough, so I filter that out.\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nrats_raw &lt;- read_csv(\"data/Rat_Sightings.csv\", na = c(\"\", \"NA\", \"N/A\"))\n\n# If you get an error that says \"All formats failed to parse. No formats\n# found\", it's because the mdy_hms function couldn't parse the date. The date\n# variable *should* be in this format: \"04/03/2017 12:00:00 AM\", but in some\n# rare instances, it might load without the seconds as \"04/03/2017 12:00 AM\".\n# If there are no seconds, use mdy_hm() instead of mdy_hms().\nrats_clean &lt;- rats_raw %&gt;%\n  rename(created_date = `Created Date`,\n         location_type = `Location Type`,\n         borough = Borough) %&gt;%\n  mutate(created_date = mdy_hms(created_date)) %&gt;%\n  mutate(sighting_year = year(created_date),\n         sighting_month = month(created_date),\n         sighting_day = day(created_date),\n         sighting_weekday = wday(created_date, label = TRUE, abbr = FALSE)) %&gt;%\n  filter(borough != \"Unspecified\")\n\nYou’ll summarize the data with functions from {dplyr}, including stuff like count(), arrange(), filter(), group_by(), summarize(), and mutate(). Here are some examples of ways to summarize the data:\n\n# See the count of rat sightings by weekday\nrats_clean %&gt;%\n  count(sighting_weekday)\n\n# Assign a summarized data frame to an object to use it in a plot\nrats_by_weekday &lt;- rats_clean %&gt;%\n  count(sighting_weekday, sighting_year)\n\nggplot(rats_by_weekday, aes(y = fct_rev(sighting_weekday), x = n)) + \n  geom_col() +\n  facet_wrap(vars(sighting_year))\n\n# See the count of rat sightings by weekday and borough\nrats_clean %&gt;%\n  count(sighting_weekday, borough, sighting_year)\n\n# An alternative to count() is to specify the groups with group_by() and then\n# be explicit about how you're summarizing the groups, such as calculating the\n# mean, standard deviation, or number of observations (we do that here with `n()`).\nrats_clean %&gt;%\n  group_by(sighting_weekday, borough) %&gt;%\n  summarize(n = n())"
  },
  {
    "objectID": "assignment/01-exercise.html",
    "href": "assignment/01-exercise.html",
    "title": "Introduction to R and the tidyverse",
    "section": "",
    "text": "Important\n\n\n\nBefore starting this exercise, make sure you complete everything at the lesson for this week, especially the Posit Primers."
  },
  {
    "objectID": "assignment/01-exercise.html#task-1-make-an-rstudio-project",
    "href": "assignment/01-exercise.html#task-1-make-an-rstudio-project",
    "title": "Introduction to R and the tidyverse",
    "section": "Task 1: Make an RStudio Project",
    "text": "Task 1: Make an RStudio Project\n\nUse either Posit.cloud or RStudio on your computer (preferably RStudio on your computer! Follow these instructions to get started!) to create a new RStudio Project.\nCreate a folder named “data” in the project folder you just made.\nDownload this CSV file and place it in that folder:\n\n cars.csv\n\nIn RStudio, go to “File” &gt; “New File…” &gt; “R Markdown…” and click “OK” in the dialog without changing anything.\nDelete all the placeholder text in that new file and replace it with this:\n---\ntitle: \"Exercise 1\"\nauthor: \"Put your name here\"\n---\n\n# Reflection\n\nReplace this text with your reflection\n\n\n# My first plots\n\nInsert a chunk below and use it to create a scatterplot (hint: `geom_point()`) with diplacement (`displ`) on the x-axis, city MPG (`cty`) on the y-axis, and with the points colored by drive (`drv`).\n\nPUT CHUNK HERE\n\nInsert a chunk below and use it to create a histogram (hint: `geom_histogram()`) with highway MPG (`hwy`) on the x-axis. Do not include anything on the y-axis (`geom_histogram()` will do that automatically for you). Choose an appropriate bin width. If you're brave, facet by drive (`drv`).\n\nPUT CHUNK HERE\n\n\n# My first data manipulation\n\nInsert a chunk below and use it to calculate the average city MPG (`cty`) by class of car (`class`). This won't be a plot---it'll be a table. Hint: use a combination of `group_by()` and `summarize()`.\n\nPUT CHUNK HERE\nSave the R Markdown file with some sort of name (without any spaces!)\nYour project folder should look something like this:"
  },
  {
    "objectID": "assignment/01-exercise.html#task-2-work-with-r",
    "href": "assignment/01-exercise.html#task-2-work-with-r",
    "title": "Introduction to R and the tidyverse",
    "section": "Task 2: Work with R",
    "text": "Task 2: Work with R\n\nAdd your reading reflection to the appropriate place in the R Markdown file. You can type directly in RStudio if you want (though there’s no spell checker), or you can type it in Word or Google Docs and then paste it into RStudio.\nRemove the text that says “PUT CHUNK HERE” and insert a new R code chunk. Either type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS, or use the “Insert Chunk” menu:\n\n\n\nFollow the instructions for the three chunks of code.\nKnit your document as a Word file (or PDF if you’re brave and installed LaTeX). Use the “Knit” menu:\n\n\n\nUpload the knitted document to Canvas.\n🎉 Party! 🎉\n\n\n\n\n\n\n\nFile organization\n\n\n\nYou’ll be doing this same process for all your future exercises. Each exercise will involve an R Markdown file. You can either create a new RStudio Project directory for all your work:\n\nOr you can create individual projects for each assignment and mini-project:"
  },
  {
    "objectID": "assignment/02-exercise.html",
    "href": "assignment/02-exercise.html",
    "title": "Graphic design",
    "section": "",
    "text": "Write your reflection for the day’s readings."
  },
  {
    "objectID": "assignment/02-exercise.html#task-1-reflection",
    "href": "assignment/02-exercise.html#task-1-reflection",
    "title": "Graphic design",
    "section": "",
    "text": "Write your reflection for the day’s readings."
  },
  {
    "objectID": "assignment/02-exercise.html#task-2-crap-critique",
    "href": "assignment/02-exercise.html#task-2-crap-critique",
    "title": "Graphic design",
    "section": "Task 2: CRAP critique",
    "text": "Task 2: CRAP critique\nCritique the design of the poster for the BYU Student Wellness Center workshop below. Go through the CRAP checklist and analyze how well or poorly the poster follows each of the principles. Discuss how the poster’s adherence to (or non-adherence to) these principles influences its effectiveness.\n\n\n\n\n\n\nWhy BYU?\n\n\n\nThis would have been some random poster from GSU, but I hadn’t been on campus for months when I recorded this video in 2020 thanks to the pandemic r emoji::emoji(\"sob\"), so I dug this photo out from a previous version of this class"
  },
  {
    "objectID": "assignment/02-exercise.html#task-3-crap-redesign",
    "href": "assignment/02-exercise.html#task-3-crap-redesign",
    "title": "Graphic design",
    "section": "Task 3: CRAP redesign",
    "text": "Task 3: CRAP redesign\nRedesign the poster for the BYU Student Wellness Center workshop. Use whatever program you want—even PowerPoint if you’re most comfortable with that, though it’ll probably be easier to use something like Canva or Adobe Illustrator. If you use Canva, don’t use any of the built-in templates—start from scratch with a blank page.\nTo save you from retyping everything, I’ve included all the text and Student Wellness hex logo in the zip file below:\n\n 02-exercise.zip\n\nI didn’t include the Instagram logo. If you want to use that, go find one online. You don’t have to use it. You don’t have to use the big paragraph of text either—you can rewrite it to shrink it down if you want.\nCritique your new design using the CRAP checklist. How did you use contrast, repetition, alignment, and proximity in your improved design?"
  },
  {
    "objectID": "assignment/02-exercise.html#turning-everything-in",
    "href": "assignment/02-exercise.html#turning-everything-in",
    "title": "Graphic design",
    "section": "Turning everything in",
    "text": "Turning everything in\nYou don’t need to worry about using R Markdown for this assignment (unless you really want to). On Canvas, submit a PDF of your new poster, along with a PDF of your reflection and your critiques of the original poster and your new poster."
  },
  {
    "objectID": "assignment/03-exercise.html",
    "href": "assignment/03-exercise.html",
    "title": "Mapping data to graphics",
    "section": "",
    "text": "Do the lesson first!\n\n\n\nBefore starting this exercise, make sure you complete the Posit Primers at the lesson for this week.\nFor this exercise you’ll practice grouping, summarizing, and plotting data using the counts of words spoken in the Lord of the Rings trilogy across movie, sex, and fictional race. Hans Rosling’s gapminder data on health and wealth.\nYou’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\nAnd as always, if you’re struggling, please talk to me and use Slack and talk to your classmates! Don’t suffer in silence!"
  },
  {
    "objectID": "assignment/03-exercise.html#instructions",
    "href": "assignment/03-exercise.html#instructions",
    "title": "Mapping data to graphics",
    "section": "Instructions",
    "text": "Instructions\n\nIf you’re using R on your own computer, download this file, unzip it, and double click on the file named exercise-3.Rproj:  03-exercise.zip\nYou’ll need to make sure you have the tidyverse package installed on your computer. If you try to load it with library(tidyverse) and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Exercise 3” on Posit.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Exercise 3”.)\nRename the R Markdown file named your-name_exercise-3.Rmd to something that matches your name and open it in RStudio.\nComplete the tasks given in the R Markdown file.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\n\n\n\n\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n\nWhen you’re all done, click on the “Knit” button at the top of the editing window and create a Word or PDF version (if you’ve installed tinytex) of your document. Upload that file to Canvas. Do not upload a knitted HTML file (they don’t work on Canvas)."
  },
  {
    "objectID": "assignment/05-exercise.html",
    "href": "assignment/05-exercise.html",
    "title": "Themes",
    "section": "",
    "text": "For this assignment, you’re going to work with data compiled by data journalist Duncan Greere related to 48 Soviet dogs who flew as test subjects in USSR’s space program in the 1950s and 60s. The original data can be found here. This example page will be incredibly useful for you:\nYou’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\nFor this assignment, you’re going to forget all the wonderful CRAP design principles you just learned and try your hardest to make the ugliest plot in the world. Modify the color scale and change theme elements to make this plot truly hideous.\nAnd as always, if you’re struggling, please talk to me and use Slack and talk to your classmates! Don’t suffer in silence!"
  },
  {
    "objectID": "assignment/05-exercise.html#instructions",
    "href": "assignment/05-exercise.html#instructions",
    "title": "Themes",
    "section": "Instructions",
    "text": "Instructions\n\nIf you’re using R on your own computer, download this file, unzip it, and double click on the file named exercise-8.Rproj:  05-exercise.zip\nYou’ll need to make sure you have the tidyverse package installed on your computer. If you try to load it with library(tidyverse) and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Exercise 5” on Posit.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Exercise 5”.)\nRename the R Markdown file named your-name_exercise-5.Rmd to something that matches your name and open it in RStudio.\nComplete the tasks given in the R Markdown file.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\n\n\n\n\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n\nWhen you’re all done, click on the “Knit” button at the top of the editing window and create a Word or PDF version (if you’ve installed tinytex) of your document. Upload that file to Canvas. Do not upload a knitted HTML file (they don’t work on Canvas)."
  },
  {
    "objectID": "assignment/07-exercise.html",
    "href": "assignment/07-exercise.html",
    "title": "Relationships",
    "section": "",
    "text": "For this exercise you’ll use precinct-level data from the 2016 presidential election to visualize relationships between variables. This data comes from the MIT Election Data and Science Lab. This example page will be incredibly useful for you:\nYou’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\nYou don’t need to make your plots super fancy, but you should play around with the plots and try to go beyond the basics—experiment with adding a labs() layer or changing colors or modifying themes and theme elements.\nAnd as always, if you’re struggling, please talk to me and use Slack and talk to your classmates! Don’t suffer in silence!"
  },
  {
    "objectID": "assignment/07-exercise.html#instructions",
    "href": "assignment/07-exercise.html#instructions",
    "title": "Relationships",
    "section": "Instructions",
    "text": "Instructions\n\nIf you’re using R on your own computer, download this file, unzip it, and double click on the file named exercise-7.Rproj:  07-exercise.zip\nYou’ll need to make sure you have these packages installed on your computer: tidyverse, patchwork, broom. If you try to load one of those packages with library(tidyverse) or library(patchwork), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Exercise 7” on Posit.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Exercise 7”.)\nRename the R Markdown file named your-name_exercise-7.Rmd to something that matches your name and open it in RStudio.\nComplete the tasks given in the R Markdown file.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\n\n\n\n\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n\nWhen you’re all done, click on the “Knit” button at the top of the editing window and create a Word or PDF version (if you’ve installed tinytex) of your document. Upload that file to Canvas. Do not upload a knitted HTML file (they don’t work on Canvas)."
  },
  {
    "objectID": "assignment/09-exercise.html",
    "href": "assignment/09-exercise.html",
    "title": "Annotations",
    "section": "",
    "text": "For this exercise, you’ll use whatever data you want to make a plot and add annotations to it. Use a dataset from a past exercise, use one of the built-in datasets like mpg or gapminder from the {gapminder} package, download stuff from the World Bank using the {WDI} package, or use something from this list of datasets.\nThis example page will be incredibly useful for you:\nYou’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\nYou don’t need to make your plots super fancy (except for these annotations), but you should play around with the plots and try to go beyond the basics—experiment with changing colors or modifying themes and theme elements.\nAnd as always, if you’re struggling, please talk to me and use Slack and talk to your classmates! Don’t suffer in silence!"
  },
  {
    "objectID": "assignment/09-exercise.html#instructions",
    "href": "assignment/09-exercise.html#instructions",
    "title": "Annotations",
    "section": "Instructions",
    "text": "Instructions\n\nIf you’re using R on your own computer, download this file, unzip it, and double click on the file named exercise-9.Rproj:  09-exercise.zip\nYou’ll need to make sure you have the tidyverse package installed on your computer. If you try to load it with library(tidyverse) and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Exercise 9” on Posit.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Exercise 9”.)\nRename the R Markdown file named your-name_exercise-9.Rmd to something that matches your name and open it in RStudio.\nComplete the tasks given in the R Markdown file.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\n\n\n\n\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n\nWhen you’re all done, click on the “Knit” button at the top of the editing window and create a Word or PDF version (if you’ve installed tinytex) of your document. Upload that file to Canvas. Do not upload a knitted HTML file (they don’t work on Canvas)."
  },
  {
    "objectID": "assignment/11-exercise.html",
    "href": "assignment/11-exercise.html",
    "title": "Time",
    "section": "",
    "text": "For this exercise, you’ll visualize something over time. You can use whatever data you want. Use a dataset from a past exercise, use one of the built-in datasets like gapminder from the {gapminder} package, download stuff from the World Bank with the {WDI} package, or download stuff from FRED using the {tidyquant} package. Anything is fair game.\nThere’s no specific way you should visualize time. Show it as a line, or as bars, or with a heatmap, or with ridgeplots, or with whatever is most appropriate for the story you’re telling. You’re free to do whatever you want!\nThis can be as simple or as complex as you want. You don’t need to make your plot super fancy, but if you’re feeling brave, experiment with changing colors or modifying themes and theme elements.\nThis example page will be incredibly useful for you:\nYou’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\nAnd as always, if you’re struggling, please talk to me and use Slack and talk to your classmates! Don’t suffer in silence!"
  },
  {
    "objectID": "assignment/11-exercise.html#instructions",
    "href": "assignment/11-exercise.html#instructions",
    "title": "Time",
    "section": "Instructions",
    "text": "Instructions\n\nIf you’re using R on your own computer, download this file, unzip it, and double click on the file named exercise-11.Rproj:  11-exercise.zip\n(Alternatively, you can open the project named “Exercise 11” on Posit.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Exercise 11”.)\nRename the R Markdown file named your-name_exercise-11.Rmd to something that matches your name and open it in RStudio.\nComplete the tasks given in the R Markdown file.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\n\n\n\n\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n\nWhen you’re all done, click on the “Knit” button at the top of the editing window and create a Word or PDF version (if you’ve installed tinytex) of your document. Upload that file to Canvas. Do not upload a knitted HTML file (they don’t work on Canvas)."
  },
  {
    "objectID": "assignment/13-exercise.html",
    "href": "assignment/13-exercise.html",
    "title": "Text",
    "section": "",
    "text": "For this exercise, you’ll download some books from Project Gutenberg and visualize patterns in the words.\nThis example page will be incredibly useful for you:\nYou’ll be doing all your R work in R Markdown. You can download a zipped file of a pre-made project here:\nYou don’t need to make your plots super fancy (except for these annotations), but you should play around with the plots and try to go beyond the basics—experiment with changing colors or modifying themes and theme elements.\nAnd as always, if you’re struggling, please talk to me and use Slack and talk to your classmates! Don’t suffer in silence!"
  },
  {
    "objectID": "assignment/13-exercise.html#instructions",
    "href": "assignment/13-exercise.html#instructions",
    "title": "Text",
    "section": "Instructions",
    "text": "Instructions\n\nIf you’re using R on your own computer, download this file, unzip it, and double click on the file named exercise-13.Rproj:  13-exercise.zip\nYou’ll need to make sure you have these packages installed on your computer: tidyverse, tidytext, gutenbergr. If you try to load one of those packages with library(tidyverse) or library(tidytext), etc., and R gives an error that the package is missing, use the “Packages” panel in RStudio to install it.\n(Alternatively, you can open the project named “Exercise 13” on Posit.cloud and complete the assignment in your browser without needing to install anything. This link should take you to the project—if it doesn’t, log in and look for the project named “Exercise 13”.)\nRename the R Markdown file named your-name_exercise-13.Rmd to something that matches your name and open it in RStudio.\nComplete the tasks given in the R Markdown file.\nFill out code in the empty chunks provided (you can definitely copy, paste, and adapt from other code in the document or the example page—don’t try to write everything from scratch!).\nYou’ll need to insert your own code chunks. Rather than typing them by hand (that’s tedious!), use the “Insert” button at the top of the editing window, or press ⌥ + ⌘ + I on macOS, or ctrl + alt + I on Windows.\n\n\n\n\nRemember that you can run an entire chunk by clicking on the green play arrow in the top right corner of the chunk. You can also run lines of code line-by-line if you place your cursor on some R code and press ⌘ + enter (for macOS users) or ctrl + enter (for Windows users).\nMake sure you run each chunk sequentially. If you run a chunk in the middle of the document without running previous ones, it might not work, since previous chunks might do things that later chunks depend on.\n\nWhen you’re all done, click on the “Knit” button at the top of the editing window and create a Word or PDF version (if you’ve installed tinytex) of your document. Upload that file to Canvas. Do not upload a knitted HTML file (they don’t work on Canvas)."
  },
  {
    "objectID": "assignment/15-exercise.html",
    "href": "assignment/15-exercise.html",
    "title": "Truth, beauty, and data revisited",
    "section": "",
    "text": "For your final exercise, you won’t do anything with R. You’ll instead have two writing tasks. You can write these in R Markdown if you want, or you can do it in Word or Google Docs or wherever else."
  },
  {
    "objectID": "assignment/15-exercise.html#task-1-storytelling-reflection",
    "href": "assignment/15-exercise.html#task-1-storytelling-reflection",
    "title": "Truth, beauty, and data revisited",
    "section": "Task 1: Storytelling reflection",
    "text": "Task 1: Storytelling reflection\nWrite your standard reflection about the storytelling readings and videos."
  },
  {
    "objectID": "assignment/15-exercise.html#task-2-summary-reflection",
    "href": "assignment/15-exercise.html#task-2-summary-reflection",
    "title": "Truth, beauty, and data revisited",
    "section": "Task 2: Summary reflection",
    "text": "Task 2: Summary reflection\nWrite a longer (400ish words) reflection on what you learned in the course in general. What was new? What was exciting? What will you remember? How has this class changed the way you look at data and graphics?\nYou might explore a few of these summative questions (but definitely don’t just go through and answer each of these!):\n\nWhat is truth?\nHow do we find truth?\nAre facts truth?\nWhat’s the difference between content and form?\nDoes beauty matter when describing truth?\nHow does any of this philosophical humanities stuff relate to data visualization?"
  },
  {
    "objectID": "assignment/15-exercise.html#turning-everything-in",
    "href": "assignment/15-exercise.html#turning-everything-in",
    "title": "Truth, beauty, and data revisited",
    "section": "Turning everything in",
    "text": "Turning everything in\nWhen you’re all done, upload the document with both reflection to Canvas.\n🎉 Congratulations! You did it! 🎉"
  },
  {
    "objectID": "assignment/index.html",
    "href": "assignment/index.html",
    "title": "Assignment overview",
    "section": "",
    "text": "You will get the most of out this class if you:\nEach type of assignment in this class helps with one of these strategies."
  },
  {
    "objectID": "assignment/index.html#reflections",
    "href": "assignment/index.html#reflections",
    "title": "Assignment overview",
    "section": "Reflections",
    "text": "Reflections\nTo encourage engagement with the course content, you’ll need to write a ≈150 word reflection about the readings and lectures for the session That’s fairly short: there are ≈250 words on a typical double-spaced page in Microsoft Word (500 when single-spaced).\nYou can do a lot of different things with this memo: discuss something you learned from the course content, write about the best or worst data visualization you saw recently, connect the course content to your own work, etc. These reflections let you explore and answer some of the key questions of this course, including:\n\nWhat is truth? How is truth related to visualization?\nWhy do we visualize data?\nWhat makes a great visualization? What makes a bad visualization?\nHow do you choose which kind of visualization to use?\nWhat is the role of stories in presenting analysis?\n\nThe course content for each session will also include a set of questions specific to that topic. You do not have to answer all (or any) of these questions. That would be impossible. They exist to guide your thinking, that’s all.\nI will grade these memos using a check system:\n\n✔+: (11.5 points (115%) in gradebook) Reflection shows phenomenal thought and engagement with the course content. I will not assign these often.\n✔: (10 points (100%) in gradebook) Reflection is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance.\n✔−: (5 points (50%) in gradebook) Reflection is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.\n\nNotice that is essentially a pass/fail or completion-based system. I’m not grading your writing ability, I’m not counting the exact number of words you’re writing, and I’m not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. I’m looking for thoughtful engagement, that’s all. Do good work and you’ll get a ✓.\nYou will turn these reflections in via Canvas. You will write them using R Markdown and they will be the first section of your daily exercises (see below)."
  },
  {
    "objectID": "assignment/index.html#exercises",
    "href": "assignment/index.html#exercises",
    "title": "Assignment overview",
    "section": "Exercises",
    "text": "Exercises\nEach class session has interactive lessons and fully annotated examples of code that teach and demonstrate how to do specific tasks in R. However, without practicing these principles and making graphics on your own, you won’t remember what you learn!\nTo practice working with {ggplot2} and making data-based graphics, you will complete a brief set of exercises for each class session. These exercises will have 1–3 short tasks that are directly related to the topic for the session. You need to show that you made a good faith effort to work each question. The problem sets will also be graded using a check system:\n\n✔+: (11.5 points (115%) in gradebook) Exercises are 100% completed. Every task was attempted and answered, and most answers are correct. Knitted document is clean and easy to follow. Work is exceptional. I will not assign these often.\n✔: (10 points (100%) in gradebook) Exercises are 70–99% complete and most answers are correct. This is the expected level of performance.\n✔−: (5 points (50%) in gradebook) Exercises are less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often.\n\nNote that this is also essentially a pass/fail system. I’m not grading your coding ability, I’m not checking each line of code to make sure it produces some exact final figure, and I’m not looking for perfect. Also note that a ✓ does not require 100% completion—you will sometimes get stuck with weird errors that you can’t solve, or the demands of pandemic living might occasionally become overwhelming. I’m looking for good faith effort, that’s all. Try hard, do good work, and you’ll get a ✓.\nYou may (and should!) work together on the exercises, but you must turn in your own answers.\nYou will turn these exercises in using Canvas. You will include your reflection in the first part of the document—the rest will be your exercise tasks."
  },
  {
    "objectID": "assignment/index.html#mini-projects",
    "href": "assignment/index.html#mini-projects",
    "title": "Assignment overview",
    "section": "Mini projects",
    "text": "Mini projects\nTo give you practice with the data and design principles you’ll learn in this class, you will complete two mini projects. I will provide you with real-world data and pose one or more questions—you will make a pretty picture to answer those questions.\nThe mini projects will be graded using a check system:\n\n✔+: (85 points (≈115%) in gradebook) Project is phenomenally well-designed and uses advanced R techniques. The project uncovers an important story that is not readily apparent from just looking at the raw data. I will not assign these often.\n✔: (75 points (100%) in gradebook) Project is fine, follows most design principles, answers a question from the data, and uses R correctly. This is the expected level of performance.\n✔−: (37.5 points (50%) in gradebook) Project is missing large components, is poorly designed, does not answer a relevant question, and/or uses R incorrectly. This indicates that you need to improve next time. I will hopefully not assign these often.\n\nBecause these mini projects give you practice for the final project, I will provide you with substantial feedback on your design and code."
  },
  {
    "objectID": "assignment/index.html#final-project",
    "href": "assignment/index.html#final-project",
    "title": "Assignment overview",
    "section": "Final project",
    "text": "Final project\nAt the end of the course, you will demonstrate your data visualization skills by completing a final project.\nComplete details for the final project (including past examples of excellent projects) are here.\nThere is no final exam. This project is your final exam.\nThe project will not be graded using a check system. Instead I will use a rubric to grade four elements of your project:\n\nTechnical skills\nVisual design\nTruth and beauty\nStory\n\nIf you’ve engaged with the course content and completed the exercises and mini projects throughout the course, you should do just fine with the final project."
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "Graphic design",
    "section": "",
    "text": "Chapter 5 in The Truthful Art (Cairo 2016)\n Chapter 4 and Chapter 27 in Fundamentals of Data Visualization (Wilke 2018)\n Cédric Scherer, “Colors and emotions in data visualization”\n Lisa Charlotte Muth, “A detailed guide to colors in data vis style guides”. You can skim this—this is a good resource for future you.\n Summary of CRAP graphic design principles from Presentation Zen (Reynolds 2008). These principles are from Robin Williams’ The Non-Designer’s Design & Type Books (Williams 2008), which you should really get if you’re interested in doing anything design-related ever. Her stuff is life-changing.\n Typography in ten minutes. The rest of the Practical Typography book is phenomenal and you’d be remiss if you didn’t read the whole thing and bookmark it for life, but for now just read this quick summary.\n “What’s the Difference Between JPG, PNG, and GIF?”\n “File formats explained”\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nWhy does graphic design matter when conveying truth?\nWhat makes something well designed (vs. poorly designed)?"
  },
  {
    "objectID": "content/02-content.html#readings",
    "href": "content/02-content.html#readings",
    "title": "Graphic design",
    "section": "",
    "text": "Chapter 5 in The Truthful Art (Cairo 2016)\n Chapter 4 and Chapter 27 in Fundamentals of Data Visualization (Wilke 2018)\n Cédric Scherer, “Colors and emotions in data visualization”\n Lisa Charlotte Muth, “A detailed guide to colors in data vis style guides”. You can skim this—this is a good resource for future you.\n Summary of CRAP graphic design principles from Presentation Zen (Reynolds 2008). These principles are from Robin Williams’ The Non-Designer’s Design & Type Books (Williams 2008), which you should really get if you’re interested in doing anything design-related ever. Her stuff is life-changing.\n Typography in ten minutes. The rest of the Practical Typography book is phenomenal and you’d be remiss if you didn’t read the whole thing and bookmark it for life, but for now just read this quick summary.\n “What’s the Difference Between JPG, PNG, and GIF?”\n “File formats explained”\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nWhy does graphic design matter when conveying truth?\nWhat makes something well designed (vs. poorly designed)?"
  },
  {
    "objectID": "content/02-content.html#slides",
    "href": "content/02-content.html#slides",
    "title": "Graphic design",
    "section": "Slides",
    "text": "Slides\nThe slides for this week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands."
  },
  {
    "objectID": "content/02-content.html#videos",
    "href": "content/02-content.html#videos",
    "title": "Graphic design",
    "section": "Videos",
    "text": "Videos\nVideos for each section of the lecture are available at this YouTube playlist.\n\nIntroduction\nTruth, beauty, stories, design\nGraphic design and CRAP\nImage types\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "content/04-content.html",
    "href": "content/04-content.html",
    "title": "Amounts and proportions",
    "section": "",
    "text": "Chapter 6 in Fundamentals of Data Visualization (Wilke 2018)\n Chapter 6 in The Truthful Art (Cairo 2016)\n Chapter 10 in Fundamentals of Data Visualization (Wilke 2018)\n Engaging Readers with Square Pie/Waffle Charts\n Understanding Pie Charts\n Square pie chart beats out the rest in perception study\n Tweet and Twitter thread from John Burn-Murdoch on why the Financial Times doesn’t use population-adjusted numbers in their COVID-19 tracking charts\n Video from the Financial Times about the design decisions behind their COVID-19 tracking charts\n\n\n\n\n See how to create your own COVID-19 tracking chart with R\n\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nHow do these types of visualizations help or hinder our search for truth in data?\nWhat do you think of the Financial Times explanations of their use of absolute numbers (not per capita numbers) and log scales (not regular scales)? How have these decisions affected your perception of the pandemic? How have they affected others’ perceptions?"
  },
  {
    "objectID": "content/04-content.html#readings",
    "href": "content/04-content.html#readings",
    "title": "Amounts and proportions",
    "section": "",
    "text": "Chapter 6 in Fundamentals of Data Visualization (Wilke 2018)\n Chapter 6 in The Truthful Art (Cairo 2016)\n Chapter 10 in Fundamentals of Data Visualization (Wilke 2018)\n Engaging Readers with Square Pie/Waffle Charts\n Understanding Pie Charts\n Square pie chart beats out the rest in perception study\n Tweet and Twitter thread from John Burn-Murdoch on why the Financial Times doesn’t use population-adjusted numbers in their COVID-19 tracking charts\n Video from the Financial Times about the design decisions behind their COVID-19 tracking charts\n\n\n\n\n See how to create your own COVID-19 tracking chart with R\n\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nHow do these types of visualizations help or hinder our search for truth in data?\nWhat do you think of the Financial Times explanations of their use of absolute numbers (not per capita numbers) and log scales (not regular scales)? How have these decisions affected your perception of the pandemic? How have they affected others’ perceptions?"
  },
  {
    "objectID": "content/04-content.html#slides",
    "href": "content/04-content.html#slides",
    "title": "Amounts and proportions",
    "section": "Slides",
    "text": "Slides\nThe slides for this week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands."
  },
  {
    "objectID": "content/04-content.html#videos",
    "href": "content/04-content.html#videos",
    "title": "Amounts and proportions",
    "section": "Videos",
    "text": "Videos\nVideos for each section of the lecture are available at this YouTube playlist.\n\nIntroduction\nReproducibility\nAmounts\nProportions\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "content/06-content.html",
    "href": "content/06-content.html",
    "title": "Uncertainty",
    "section": "",
    "text": "Chapter 7 in Fundamentals of Data Visualization (Wilke 2018)\n Chapters 4, 7, and 11 in The Truthful Art (Cairo 2016)\n Why It’s So Hard for Us to Visualize Uncertainty\n Amanda Cox’s keynote address at the 2017 OpenVis Conf\n Communicating Uncertainty When Lives Are on the Line\n Showing uncertainty during the live election forecast & Trolling the uncertainty dial\n Cédric Scherer, “Visualizing distributions with raincloud plots with ggplot2”\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nWhy is it important to deal with uncertainty in data?\nWhat was good or bad about the New York Times’ 2016 live election guage?\nWhy is it so hard to visualize uncertainty?\nWhy is it so hard to communicate uncertainty to others?"
  },
  {
    "objectID": "content/06-content.html#readings",
    "href": "content/06-content.html#readings",
    "title": "Uncertainty",
    "section": "",
    "text": "Chapter 7 in Fundamentals of Data Visualization (Wilke 2018)\n Chapters 4, 7, and 11 in The Truthful Art (Cairo 2016)\n Why It’s So Hard for Us to Visualize Uncertainty\n Amanda Cox’s keynote address at the 2017 OpenVis Conf\n Communicating Uncertainty When Lives Are on the Line\n Showing uncertainty during the live election forecast & Trolling the uncertainty dial\n Cédric Scherer, “Visualizing distributions with raincloud plots with ggplot2”\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nWhy is it important to deal with uncertainty in data?\nWhat was good or bad about the New York Times’ 2016 live election guage?\nWhy is it so hard to visualize uncertainty?\nWhy is it so hard to communicate uncertainty to others?"
  },
  {
    "objectID": "content/06-content.html#slides",
    "href": "content/06-content.html#slides",
    "title": "Uncertainty",
    "section": "Slides",
    "text": "Slides\nThe slides for this week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands."
  },
  {
    "objectID": "content/06-content.html#videos",
    "href": "content/06-content.html#videos",
    "title": "Uncertainty",
    "section": "Videos",
    "text": "Videos\nVideos for each section of the lecture are available at this YouTube playlist.\n\nIntroduction\nCommunicating uncertainty\nVisualizing uncertainty\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "content/08-content.html",
    "href": "content/08-content.html",
    "title": "Comparisons",
    "section": "",
    "text": "Chapter 9 in Fundamentals of Data Visualization (Wilke 2018)\n Mike Bostock, “Methods of Comparison, Compared”. Explanation of the differences between showing relative differences, absolute differences, and log ratios.\n Sparkline theory and practice. This is a collection of posts by Edward Tufte about sparklines—scroll down a ways and check out his examples, analysis, and critiques.\n Time series sparklines\n Comparisons with lollipop charts. If you’re feeling adventurous (and you should!), do this tutorial as you read it.\n\n\n\nReprexes (or reproducible examples) are the best way to (1) get help online and (2) fix issues on your own.\nMaking a good reprex is tricky, but it’s a very valuable skill to know (regardless of programming language!). Here are some helpful resources for making them:\n\n What’s a reproducible example (reprex) and how do I do one?\n So you’ve been asked to make a reprex\n The reprex package\n\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nThese readings all show a ton of new ways to present comparisons. Which ones are your favorite? Which ones didn’t quite click with you? In what situations are some more appropriate than others?"
  },
  {
    "objectID": "content/08-content.html#readings",
    "href": "content/08-content.html#readings",
    "title": "Comparisons",
    "section": "",
    "text": "Chapter 9 in Fundamentals of Data Visualization (Wilke 2018)\n Mike Bostock, “Methods of Comparison, Compared”. Explanation of the differences between showing relative differences, absolute differences, and log ratios.\n Sparkline theory and practice. This is a collection of posts by Edward Tufte about sparklines—scroll down a ways and check out his examples, analysis, and critiques.\n Time series sparklines\n Comparisons with lollipop charts. If you’re feeling adventurous (and you should!), do this tutorial as you read it.\n\n\n\nReprexes (or reproducible examples) are the best way to (1) get help online and (2) fix issues on your own.\nMaking a good reprex is tricky, but it’s a very valuable skill to know (regardless of programming language!). Here are some helpful resources for making them:\n\n What’s a reproducible example (reprex) and how do I do one?\n So you’ve been asked to make a reprex\n The reprex package\n\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nThese readings all show a ton of new ways to present comparisons. Which ones are your favorite? Which ones didn’t quite click with you? In what situations are some more appropriate than others?"
  },
  {
    "objectID": "content/08-content.html#slides",
    "href": "content/08-content.html#slides",
    "title": "Comparisons",
    "section": "Slides",
    "text": "Slides\nThe slides for this week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands."
  },
  {
    "objectID": "content/08-content.html#videos",
    "href": "content/08-content.html#videos",
    "title": "Comparisons",
    "section": "Videos",
    "text": "Videos\nVideos for each section of the lecture are available at this YouTube playlist.\n\nIntroduction\nVisualizing comparisons\nReproducible examples\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "content/10-content.html",
    "href": "content/10-content.html",
    "title": "Interactivity",
    "section": "",
    "text": "Marcel Salathé and Nicky Case, “What Happens Next: COVID-19 Futures, Explained with Playable Situations”\n Brett Victor, “Explorable Explanations”\n Look at some of the explorable explorations here\n Dragicevic, Jansen, Sarma, Kay, and Chevalier, “Explorable Multiverse Analyses”. Use Chrome, open Example 1, scroll to page 2, and click on some of the blue text to change the results of the paper within the paper itself. This is magical. R Markdown can’t quite get this interactive in real-time, but you can knit different versions of a document with slightly different parameters and options.\n\n\n\n\n\n Look at some of these examples of Shiny apps\n Stephanie Evergreen, “How a Dashboard Changes the Conversation”\n Stephanie Evergreen, “The Problem with Dashboards (and a Solution)”\n Stephen Few, “2012 Perceptual Edge Dashboard Design Competition: A Solution of My Own”\n Skim through Stephen Few’s presentation on Information Dashboard Design\n Google “dashboard design” and skim through some of the thousands of articles about what makes a good (and bad) dashboard\n\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nHow helpful (or unhelpful) are explorable explanations?\nHave you seen examples of good dashboards before this class? Bad dashboards? What makes them good or bad?"
  },
  {
    "objectID": "content/10-content.html#readings",
    "href": "content/10-content.html#readings",
    "title": "Interactivity",
    "section": "",
    "text": "Marcel Salathé and Nicky Case, “What Happens Next: COVID-19 Futures, Explained with Playable Situations”\n Brett Victor, “Explorable Explanations”\n Look at some of the explorable explorations here\n Dragicevic, Jansen, Sarma, Kay, and Chevalier, “Explorable Multiverse Analyses”. Use Chrome, open Example 1, scroll to page 2, and click on some of the blue text to change the results of the paper within the paper itself. This is magical. R Markdown can’t quite get this interactive in real-time, but you can knit different versions of a document with slightly different parameters and options.\n\n\n\n\n\n Look at some of these examples of Shiny apps\n Stephanie Evergreen, “How a Dashboard Changes the Conversation”\n Stephanie Evergreen, “The Problem with Dashboards (and a Solution)”\n Stephen Few, “2012 Perceptual Edge Dashboard Design Competition: A Solution of My Own”\n Skim through Stephen Few’s presentation on Information Dashboard Design\n Google “dashboard design” and skim through some of the thousands of articles about what makes a good (and bad) dashboard\n\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nHow helpful (or unhelpful) are explorable explanations?\nHave you seen examples of good dashboards before this class? Bad dashboards? What makes them good or bad?"
  },
  {
    "objectID": "content/10-content.html#slides",
    "href": "content/10-content.html#slides",
    "title": "Interactivity",
    "section": "Slides",
    "text": "Slides\nThe slides for this week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands."
  },
  {
    "objectID": "content/10-content.html#videos",
    "href": "content/10-content.html#videos",
    "title": "Interactivity",
    "section": "Videos",
    "text": "Videos\nVideos for each section of the lecture are available at this YouTube playlist.\n\nIntroduction\nMaking interactive graphs\nSharing content\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "content/12-content.html",
    "href": "content/12-content.html",
    "title": "Space",
    "section": "",
    "text": "It looks like this is a lot of reading, but lots of these are short videos or tweets or interactive websites, so don’t worry!\n\n Chapter 10 in The Truthful Art (Cairo 2016)\n Chapter 7 in Data Visualization (Healy 2018)\nLook through these examples of using the {sf} package:\n\n Andrew Heiss, “How to make fancy road trip maps with R and OpenStreetMap”\n Andrew Heiss, “Road trip analysis! How to use and play with Google Location History in R”\n Andrew Heiss, “Making Middle Earth maps with R”\n\n Why all world maps are wrong\n The True Size Of…\n Projection comparison\n Map projections (try comparing Robinson with Mercator to see how badly Mercator exaggerates the northern hemisphere)\n Gall-Peters Projection\n “When Maps Lie”\n Animated Mercator distortion\n “These Twisted Maps Prove That America Isn’t a Red Country”\n “The next great fake news threat? Bot-designed maps”\n “New World Map That Accurately Shows Earth in 2D Created by Scientists”\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nHow can you know if a map projection is truthful or misleading?\nWhat’s wrong (or not wrong) with using points on maps? Choropleths? Lines?"
  },
  {
    "objectID": "content/12-content.html#readings",
    "href": "content/12-content.html#readings",
    "title": "Space",
    "section": "",
    "text": "It looks like this is a lot of reading, but lots of these are short videos or tweets or interactive websites, so don’t worry!\n\n Chapter 10 in The Truthful Art (Cairo 2016)\n Chapter 7 in Data Visualization (Healy 2018)\nLook through these examples of using the {sf} package:\n\n Andrew Heiss, “How to make fancy road trip maps with R and OpenStreetMap”\n Andrew Heiss, “Road trip analysis! How to use and play with Google Location History in R”\n Andrew Heiss, “Making Middle Earth maps with R”\n\n Why all world maps are wrong\n The True Size Of…\n Projection comparison\n Map projections (try comparing Robinson with Mercator to see how badly Mercator exaggerates the northern hemisphere)\n Gall-Peters Projection\n “When Maps Lie”\n Animated Mercator distortion\n “These Twisted Maps Prove That America Isn’t a Red Country”\n “The next great fake news threat? Bot-designed maps”\n “New World Map That Accurately Shows Earth in 2D Created by Scientists”\n\n\n\n\n\n\n\n\n\nThis is not a required list!\n\n\n\nRemember, you don’t need to answer all of these—or even any of them! These are just here to help guide your thinking. Write about whatever you want.\n\n\n\nHow can you know if a map projection is truthful or misleading?\nWhat’s wrong (or not wrong) with using points on maps? Choropleths? Lines?"
  },
  {
    "objectID": "content/12-content.html#other-resources",
    "href": "content/12-content.html#other-resources",
    "title": "Space",
    "section": "Other resources",
    "text": "Other resources\nIf you want to understand how the magic geometry column actually works, check out this fantastic post.\nCheck out this post where someone used {ggplot2} and {sf} to create fancy city map-based art that she printed for a friend. You can do similar things after this session!\nIn addition to the example for this session, you can check out this tutorial on using the {sf} package to create maps. It shows how to include fancy map stuff like a north arrow and scale bar."
  },
  {
    "objectID": "content/12-content.html#slides",
    "href": "content/12-content.html#slides",
    "title": "Space",
    "section": "Slides",
    "text": "Slides\nThe slides for this week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands."
  },
  {
    "objectID": "content/12-content.html#videos",
    "href": "content/12-content.html#videos",
    "title": "Space",
    "section": "Videos",
    "text": "Videos\nVideos for each section of the lecture are available at this YouTube playlist.\n\nIntroduction\nMaps and truth\nPutting data on maps\nGIS in R with sf\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "content/14-content.html",
    "href": "content/14-content.html",
    "title": "Enhancing graphics",
    "section": "",
    "text": "Chapter 14, Chapter 15, Chapter 16, Chapter 22, and Chapter 23 in Fundamentals of Data Visualization (Wilke 2018)\n Chapter 8 in Data Visualization (Healy 2018)\nBrowse through recent visualizations at The Pudding (like this one!), FiveThirtyEight, Vox, Christopher Ingraham’s articles at the Washington Post, WSJ Graphics, and the New York Times’s TheUpshot. Most (if not all) of these graphics were made in R (or something similar) and exported for enhancement in Illustrator or D3."
  },
  {
    "objectID": "content/14-content.html#readings",
    "href": "content/14-content.html#readings",
    "title": "Enhancing graphics",
    "section": "",
    "text": "Chapter 14, Chapter 15, Chapter 16, Chapter 22, and Chapter 23 in Fundamentals of Data Visualization (Wilke 2018)\n Chapter 8 in Data Visualization (Healy 2018)\nBrowse through recent visualizations at The Pudding (like this one!), FiveThirtyEight, Vox, Christopher Ingraham’s articles at the Washington Post, WSJ Graphics, and the New York Times’s TheUpshot. Most (if not all) of these graphics were made in R (or something similar) and exported for enhancement in Illustrator or D3."
  },
  {
    "objectID": "content/14-content.html#slides",
    "href": "content/14-content.html#slides",
    "title": "Enhancing graphics",
    "section": "Slides",
    "text": "Slides\nThe slides for this week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands."
  },
  {
    "objectID": "content/14-content.html#videos",
    "href": "content/14-content.html#videos",
    "title": "Enhancing graphics",
    "section": "Videos",
    "text": "Videos\nVideos for each section of the lecture are available at this YouTube playlist.\n\nEnhancing graphics\n\nYou can also watch the playlist (and skip around to different sections) here:"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Readings, lectures, and videos",
    "section": "",
    "text": "Each class session has a set of required readings that you should complete before watching the lecture or working through the lesson.\nI’ve included a set of questions that might guide your reflection response. You should not try to respond to all of these (or any of them if you don’t want to)—they’ll just help you know what to look for and think about as you read.\nEvery class session also has a YouTube playlist of short recorded videos for each of the lecture sections. The lecture slides are special HTML files made with the R package {xaringan} (R can do so much!). On each class session page you’ll buttons for opening the presentation in a new tab or for downloading a PDF of the slides in case you want to print them or store them on your computer:\n\n View all slides in new window  Download PDF of all slides\n\nThe slides are also embedded on each page. You can click in the slides and navigate through them with ← and →. If you type ? (or shift + /) while viewing the slides you can see a list of slide-specific commands (like f for fullscreen or p for presenter mode if you want to see my notes)."
  },
  {
    "objectID": "example/02-example.html",
    "href": "example/02-example.html",
    "title": "Graphic design",
    "section": "",
    "text": "For this example, I’m going to critique and improve this random flyer I found posted in the BYU library in September 2018:\nIt’s not the best designed poster, but it’s incredibly typical of what you see in the real world. By applying the principles of CRAP, we can improve the poster significantly.\nIf you download and unzip this file, you can follow along too (but you don’t have to—you can just sit back and enjoy the ride)."
  },
  {
    "objectID": "example/02-example.html#critique",
    "href": "example/02-example.html#critique",
    "title": "Graphic design",
    "section": "Critique",
    "text": "Critique"
  },
  {
    "objectID": "example/02-example.html#redesign-in-canva",
    "href": "example/02-example.html#redesign-in-canva",
    "title": "Graphic design",
    "section": "Redesign in Canva",
    "text": "Redesign in Canva"
  },
  {
    "objectID": "example/02-example.html#redesign-in-illustrator",
    "href": "example/02-example.html#redesign-in-illustrator",
    "title": "Graphic design",
    "section": "Redesign in Illustrator",
    "text": "Redesign in Illustrator"
  },
  {
    "objectID": "example/02-example.html#final-versions",
    "href": "example/02-example.html#final-versions",
    "title": "Graphic design",
    "section": "Final versions",
    "text": "Final versions"
  },
  {
    "objectID": "example/04-example.html",
    "href": "example/04-example.html",
    "title": "Amounts and proportions",
    "section": "",
    "text": "For this example, we’re going to use real world data to demonstrate some different ways to visualize amounts and proportions. We’ll use data from the CDC and the Social Security Administration about the number of daily births in the United States from 1994–2014. FiveThirtyEight reported a story using this data in 2016 and they posted relatively CSV files on GitHub, so we can download and use those.\nIf you want to follow along with this example, you can download the data directly from GitHub or by using these links (you’ll likely need to right click on these and choose “Save Link As…”):"
  },
  {
    "objectID": "example/04-example.html#live-coding-example",
    "href": "example/04-example.html#live-coding-example",
    "title": "Amounts and proportions",
    "section": "Live coding example",
    "text": "Live coding example\n\n\n\n\n\n\n\n\n\n\nSlight differences from the video\n\n\n\nThis is a slightly cleaned up version of the code from the video."
  },
  {
    "objectID": "example/04-example.html#load-data",
    "href": "example/04-example.html#load-data",
    "title": "Amounts and proportions",
    "section": "Load data",
    "text": "Load data\nThere are two CSV files:\n\nUS_births_1994-2003_CDC_NCHS.csv contains U.S. births data for the years 1994 to 2003, as provided by the Centers for Disease Control and Prevention’s National Center for Health Statistics.\nUS_births_2000-2014_SSA.csv contains U.S. births data for the years 2000 to 2014, as provided by the Social Security Administration.\n\nSince the two datasets overlap in 2000–2003, we use Social Security Administration data for those years.\nWe downloaded the data from GitHub and placed the CSV files in a folder named data. We’ll then load them with read_csv() and combine them into one data frame.\n\n\nCode\nlibrary(tidyverse)\nlibrary(scales)   # For nice labels in charts\n\nbirths_1994_1999 &lt;- read_csv(\"data/US_births_1994-2003_CDC_NCHS.csv\") %&gt;% \n  # Ignore anything after 2000\n  filter(year &lt; 2000)\n\nbirths_2000_2014 &lt;- read_csv(\"data/US_births_2000-2014_SSA.csv\")\n\nbirths_combined &lt;- bind_rows(births_1994_1999, births_2000_2014)"
  },
  {
    "objectID": "example/04-example.html#wrangle-data",
    "href": "example/04-example.html#wrangle-data",
    "title": "Amounts and proportions",
    "section": "Wrangle data",
    "text": "Wrangle data\nLet’s look at the first few rows of the data to see what we’re working with:\n\n\nCode\nhead(births_combined)\n## # A tibble: 6 × 5\n##    year month date_of_month day_of_week births\n##   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n## 1  1994     1             1           6   8096\n## 2  1994     1             2           7   7772\n## 3  1994     1             3           1  10142\n## 4  1994     1             4           2  11248\n## 5  1994     1             5           3  11053\n## 6  1994     1             6           4  11406\n\n\nThe columns for year and births seem straightforward and ready to use. The columns for month and day of the week could be improved if we changed them to text (i.e. January instead of 1; Tuesday instead of 3). To fix this, we can convert these columns to categorical variables, or factors in R. We can also specify that these categories (or factors) are ordered, meaning that Feburary comes after January, etc. Without ordering, R will plot them alphabetically, which isn’t very helpful.\nWe’ll make a new dataset named births that’s based on the combined births data, but with some new columns added:\n\n\nCode\n# The c() function lets us make a list of values\nmonth_names &lt;- c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\",\n                 \"August\", \"September\", \"October\", \"November\", \"December\")\n\nday_names &lt;- c(\"Monday\", \"Tuesday\", \"Wednesday\", \n               \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\")\n\nbirths &lt;- births_combined %&gt;% \n  # Make month an ordered factor, using the month_name list as labels\n  mutate(month = factor(month, labels = month_names, ordered = TRUE)) %&gt;% \n  mutate(day_of_week = factor(day_of_week, labels = day_names, ordered = TRUE),\n         date_of_month_categorical = factor(date_of_month)) %&gt;% \n  # Add a column indicating if the day is on a weekend\n  mutate(weekend = ifelse(day_of_week %in% c(\"Saturday\", \"Sunday\"), TRUE, FALSE))\n\nhead(births)\n## # A tibble: 6 × 7\n##    year month   date_of_month day_of_week births date_of_month_categor…¹ weekend\n##   &lt;dbl&gt; &lt;ord&gt;           &lt;dbl&gt; &lt;ord&gt;        &lt;dbl&gt; &lt;fct&gt;                   &lt;lgl&gt;  \n## 1  1994 January             1 Saturday      8096 1                       TRUE   \n## 2  1994 January             2 Sunday        7772 2                       TRUE   \n## 3  1994 January             3 Monday       10142 3                       FALSE  \n## 4  1994 January             4 Tuesday      11248 4                       FALSE  \n## 5  1994 January             5 Wednesday    11053 5                       FALSE  \n## 6  1994 January             6 Thursday     11406 6                       FALSE  \n## # ℹ abbreviated name: ¹​date_of_month_categorical\n\n\nIf you look at the data now, you can see the columns are changed and have different types. year and date_of_month are still numbers, but month, and day_of_week are ordered factors (ord) and date_of_month_categorical is a regular factor (fct). Technically it’s also ordered, but because it’s already alphabetical (i.e. 2 naturally comes after 1), we don’t need to force it to be in the right order.\nOur births data is now clean and ready to go!"
  },
  {
    "objectID": "example/04-example.html#bar-plot",
    "href": "example/04-example.html#bar-plot",
    "title": "Amounts and proportions",
    "section": "Bar plot",
    "text": "Bar plot\nFirst we can look at a bar chart showing the total number of births each day. We need to make a smaller summarized dataset and then we’ll plot it:\n\n\nCode\ntotal_births_weekday &lt;- births %&gt;% \n  group_by(day_of_week) %&gt;% \n  summarize(total = sum(births))\n\nggplot(data = total_births_weekday,\n       mapping = aes(x = day_of_week, y = total, fill = day_of_week)) +\n  geom_col() +\n  # Turn off the fill legend because it's redundant\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\nIf we fill by day of the week, we get 7 different colors, which is fine (I guess), but doesn’t really help tell a story. The main story here is that there are far fewer births during weekends. If we create a new column that flags if a row is Saturday or Sunday, we can fill by that column instead:\n\n\nCode\ntotal_births_weekday &lt;- births %&gt;% \n  group_by(day_of_week) %&gt;% \n  summarize(total = sum(births)) %&gt;% \n  mutate(weekend = ifelse(day_of_week %in% c(\"Saturday\", \"Sunday\"), TRUE, FALSE))\n\nggplot(data = total_births_weekday,\n       mapping = aes(x = day_of_week, y = total, fill = weekend)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\nNeat! Those default colors are kinda ugly, though, so let’s use the principles of preattentive processing and contrast to highlight the weekend bars:\n\n\nCode\nggplot(data = total_births_weekday,\n       mapping = aes(x = day_of_week, y = total, fill = weekend)) +\n  geom_col() +\n  # Use grey and orange\n  scale_fill_manual(values = c(\"grey70\", \"#f2ad22\")) +\n  # Use commas instead of scientific notation\n  scale_y_continuous(labels = label_comma()) +\n  # Turn off the legend since the title shows what the orange is\n  guides(fill = \"none\") +\n  labs(title = \"Weekends are unpopular times for giving birth\",\n       x = NULL, y = \"Total births\")"
  },
  {
    "objectID": "example/04-example.html#lollipop-chart",
    "href": "example/04-example.html#lollipop-chart",
    "title": "Amounts and proportions",
    "section": "Lollipop chart",
    "text": "Lollipop chart\nSince the ends of the bars are often the most important part of the graph, we can use a lollipop chart to emphasize them. We’ll keep all the same code from our bar chart and make a few changes:\n\nColor by weekend instead of fill by weekend, since points and lines are colored in ggplot, not filled\nSwitch scale_fill_manual() to scale_color_manual() and turn off the color legend in the guides() layer\nSwitch geom_col() to geom_pointrange(). The geom_pointrange() layer requires two additional aesthetics: ymin and ymax for the ends of the lines that come out of the point. Here we’ll set ymin to 0 so it starts at the x-axis, and we’ll set ymax to total so it ends at the point.\n\n\n\nCode\nggplot(data = total_births_weekday,\n       mapping = aes(x = day_of_week, y = total, color = weekend)) +\n  geom_pointrange(aes(ymin = 0, ymax = total),\n                  # Make the lines a little thicker and the dots a little bigger\n                  fatten = 5, size = 1.5) +\n  # Use grey and orange\n  scale_color_manual(values = c(\"grey70\", \"#f2ad22\")) +\n  # Use commas instead of scientific notation\n  scale_y_continuous(labels = label_comma()) +\n  # Turn off the legend since the title shows what the orange is\n  guides(color = \"none\") +\n  labs(title = \"Weekends are unpopular times for giving birth\",\n       x = NULL, y = \"Total births\")"
  },
  {
    "objectID": "example/04-example.html#strip-plot",
    "href": "example/04-example.html#strip-plot",
    "title": "Amounts and proportions",
    "section": "Strip plot",
    "text": "Strip plot\nHowever, we want to #barbarplots! (Though they’re arguably okay here, since they show totals and not averages). Let’s show all the data with points. We’ll use the full dataset now, map x to weekday, y to births, and change geom_col() to geom_point(). We’ll tell geom_point() to jitter the points randomly.\n\n\nCode\nggplot(data = births,\n       mapping = aes(x = day_of_week, y = births, color = weekend)) +\n  scale_color_manual(values = c(\"grey70\", \"#f2ad22\")) +\n  geom_point(size = 0.5, position = position_jitter(height = 0)) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\n\nThere are some interesting points in the low ends, likely because of holidays like Labor Day and Memorial Day (for the Mondays) and Thanksgiving (for the Thursday). If we had a column that indicated whether a day was a holiday, we could color by that and it would probably explain most of those low numbers. Unfortunately we don’t have that column, and it’d be hard to make. Some holidays are constant (Halloween is always October 31), but some aren’t (Thanksgiving is the fourth Thursday in November, so we’d need to find out which November 20-somethingth each year is the fourth Thursday, and good luck doing that at scale)."
  },
  {
    "objectID": "example/04-example.html#beeswarm-plot",
    "href": "example/04-example.html#beeswarm-plot",
    "title": "Amounts and proportions",
    "section": "Beeswarm plot",
    "text": "Beeswarm plot\nWe can add some structure to these points if we use the {ggbeeswarm} package, with either geom_beeswarm() or geom_quasirandom(). geom_quasirandom() actually works better here since there are so many points—geom_beeswarm() makes the clusters of points way too wide.\n\n\nCode\nlibrary(ggbeeswarm)\n## Warning: package 'ggbeeswarm' was built under R version 4.2.3\n\nggplot(data = births,\n       mapping = aes(x = day_of_week, y = births, color = weekend)) +\n  scale_color_manual(values = c(\"grey70\", \"#f2ad22\")) +\n  # Make these points suuuper tiny\n  geom_quasirandom(size = 0.0001) +\n  guides(color = \"none\")"
  },
  {
    "objectID": "example/04-example.html#heatmap",
    "href": "example/04-example.html#heatmap",
    "title": "Amounts and proportions",
    "section": "Heatmap",
    "text": "Heatmap\nFinally, let’s use something non-traditional to show the average births by day in a somewhat proportional way. We can calculate the average number of births every day and then make a heatmap that fills each square by that average, thus showing the relative differences in births per day.\nTo do this, we need to make a summarized data frame with group_by() %&gt;% summarize() to calculate the average number of births by month and day of the month (i.e. average for January 1, January 2, etc.).\nWe’ll then make a sort of calendar with date of the month on the x axis, month on the y axis, with heat map squares filled by the daily average. We’ll use geom_tile() to add squares for each day, and then add some extra scale, coordinates, and theme layers to clean up the plot:\n\n\nCode\navg_births_month_day &lt;- births %&gt;% \n  group_by(month, date_of_month_categorical) %&gt;% \n  summarize(avg_births = mean(births))\n\nggplot(data = avg_births_month_day,\n       # By default, the y-axis will have December at the top, so use fct_rev() to reverse it\n       mapping = aes(x = date_of_month_categorical, y = fct_rev(month), fill = avg_births)) +\n  geom_tile() +\n  # Add viridis colors\n  scale_fill_viridis_c(option = \"inferno\", labels = label_comma()) + \n  # Add nice labels\n  labs(x = \"Day of the month\", y = NULL,\n       title = \"Average births per day\",\n       subtitle = \"1994-2014\",\n       fill = \"Average births\") +\n  # Force all the tiles to have equal widths and heights\n  coord_equal() +\n  # Use a cleaner theme\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNeat! There are some really interesting trends here. Most obvious, probably, is that very few people are born on New Year’s Day, July 4th, Halloween, Thanksgiving, and Christmas.\n\n\nCode\navg_births_month_day %&gt;% \n  arrange(avg_births)\n## # A tibble: 366 × 3\n## # Groups:   month [12]\n##    month    date_of_month_categorical avg_births\n##    &lt;ord&gt;    &lt;fct&gt;                          &lt;dbl&gt;\n##  1 December 25                             6601.\n##  2 January  1                              7827.\n##  3 December 24                             8103.\n##  4 July     4                              8825.\n##  5 January  2                              9356.\n##  6 December 26                             9599.\n##  7 November 27                             9770.\n##  8 November 23                             9919.\n##  9 November 25                            10001 \n## 10 October  31                            10030.\n## # ℹ 356 more rows\n\n\nThe days with the highest average are in mid-September (lol my birthday is #2), likely because that’s about 9 months after the first week of January. July 7th at #7 is odd and I have no idea why it might be so popular 🤷.\n\n\nCode\navg_births_month_day %&gt;% \n  arrange(desc(avg_births))\n## # A tibble: 366 × 3\n## # Groups:   month [12]\n##    month     date_of_month_categorical avg_births\n##    &lt;ord&gt;     &lt;fct&gt;                          &lt;dbl&gt;\n##  1 September 9                             12344.\n##  2 September 19                            12285.\n##  3 September 12                            12282.\n##  4 September 17                            12201.\n##  5 September 10                            12190.\n##  6 September 20                            12162.\n##  7 July      7                             12147.\n##  8 September 15                            12126.\n##  9 September 16                            12114.\n## 10 September 18                            12112.\n## # ℹ 356 more rows\n\n\nThe funniest trend is the very visible dark column for the 13th of every month. People really don’t want to give birth on the 13th."
  },
  {
    "objectID": "example/06-example.html",
    "href": "example/06-example.html",
    "title": "Uncertainty",
    "section": "",
    "text": "For this example, we’re going to use historical weather data from the now-defunct Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. I downloaded this data using Dark Sky’s (now-retired-because-they-were-bought-by-Apple) API using the {darksky} package.\nIf you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):"
  },
  {
    "objectID": "example/06-example.html#live-coding-example",
    "href": "example/06-example.html#live-coding-example",
    "title": "Uncertainty",
    "section": "Live coding example",
    "text": "Live coding example\n\n\n\n\n\n\n\n\n\n\nSlight differences from the video\n\n\n\nThis is a slightly cleaned up version of the code from the video."
  },
  {
    "objectID": "example/06-example.html#load-and-clean-data",
    "href": "example/06-example.html#load-and-clean-data",
    "title": "Uncertainty",
    "section": "Load and clean data",
    "text": "Load and clean data\nFirst, we load the libraries we’ll be using:\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggridges)\nlibrary(gghalves)\n\n\nThen we load the data with read_csv(). Here I assume that the CSV file lives in a subfolder in my project named data:\n\n\nCode\nweather_atl_raw &lt;- read_csv(\"data/atl-weather-2019.csv\")\n\n\nWe’ll add a couple columns that we can use for faceting and filling using the month() and wday() functions from {lubridate} (which was loaded as part of {tidyverse}) for extracting parts of the date:\n\n\nCode\nweather_atl &lt;- weather_atl_raw %&gt;% \n  mutate(Month = month(time, label = TRUE, abbr = FALSE),\n         Day = wday(time, label = TRUE, abbr = FALSE))\n\n\nNow we’re ready to go!"
  },
  {
    "objectID": "example/06-example.html#histograms",
    "href": "example/06-example.html#histograms",
    "title": "Uncertainty",
    "section": "Histograms",
    "text": "Histograms\nWe can first make a histogram of wind speed. We’ll use a bin width of 1 and color the edges of the bars white:\n\n\nCode\nggplot(weather_atl, aes(x = windSpeed)) +\n  geom_histogram(binwidth = 1, color = \"white\")\n\n\n\n\n\n\n\n\n\nThis is fine enough, but we can improve it by forcing the buckets/bins to start at whole numbers instead of containing ranges like 2.5–3.5. We’ll use the boundary argument for that. We also add scale_x_continuous() to add our own x-axis breaks instead of having things like 2.5, 5, and 7.5:\n\n\nCode\nggplot(weather_atl, aes(x = windSpeed)) +\n  geom_histogram(binwidth = 1, color = \"white\", boundary = 1) +\n  scale_x_continuous(breaks = seq(0, 12, by = 1))\n\n\n\n\n\n\n\n\n\nWe can show the distribution of wind speed by month if we map the Month column we made onto the fill aesthetic:\n\n\nCode\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\n  geom_histogram(binwidth = 1, color = \"white\", boundary = 1) +\n  scale_x_continuous(breaks = seq(0, 12, by = 1))\n\n\n\n\n\n\n\n\n\nThis is colorful, but it’s impossible to actually interpret. Instead of only filling, we’ll also facet by month to see separate graphs for each month. We can turn off the fill legend because it’s now redundant.\n\n\nCode\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\n  geom_histogram(binwidth = 1, color = \"white\", boundary = 1) +\n  scale_x_continuous(breaks = seq(0, 12, by = 1)) + \n  guides(fill = \"none\") +\n  facet_wrap(vars(Month))\n\n\n\n\n\n\n\n\n\nNeat! January, March, and April appear to have the most variation in windy days, with a few wind-less days and a few very-windy days, while August was very wind-less."
  },
  {
    "objectID": "example/06-example.html#density-plots",
    "href": "example/06-example.html#density-plots",
    "title": "Uncertainty",
    "section": "Density plots",
    "text": "Density plots\nThe code to create a density plot is nearly identical to what we used for the histogram—the only thing we change is the geom layer:\n\n\nCode\nggplot(weather_atl, aes(x = windSpeed)) +\n  geom_density(color = \"grey20\", fill = \"grey50\")\n\n\n\n\n\n\n\n\n\nIf we want, we can mess with some of the calculus options like the kernel and bandwidth:\n\n\nCode\nggplot(weather_atl, aes(x = windSpeed)) +\n  geom_density(color = \"grey20\", fill = \"grey50\",\n               bw = 0.1, kernel = \"epanechnikov\")\n\n\n\n\n\n\n\n\n\nWe can also fill by month. We’ll make the different layers 50% transparent so we can kind of see through the whole stack:\n\n\nCode\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nEven with the transparency, this is really hard to interpret. We can fix this by faceting, like we did with the histograms:\n\n\nCode\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\n  geom_density(alpha = 0.5) +\n  guides(fill = \"none\") +\n  facet_wrap(vars(Month))\n\n\n\n\n\n\n\n\n\nOr we can stack the density plots behind each other with {ggridges}. For that to work, we also need to map Month to the y-axis. We can reverse the y-axis so that January is at the top if we use the fct_rev() function:\n\n\nCode\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +\n  geom_density_ridges() +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\nWe can add some extra information to geom_density_ridges() with some other arguments like quantile_lines. We can use the quantiles argument to tell the plow how many parts to be cut into. Since we just want to show the median, we’ll set that to 2 so each density plot is divided in half:\n\n\nCode\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +\n  geom_density_ridges(quantile_lines = TRUE, quantiles = 2) +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\nNow that we have good working code, we can easily substitute in other variables by changing the x mapping:\n\n\nCode\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = Month)) +\n  geom_density_ridges(quantile_lines = TRUE, quantiles = 2) +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\nWe can get extra fancy if we fill by temperature instead of filling by month. To get this to work, we need to use geom_density_ridges_gradient(), and we need to change the fill mapping to the strange looking after_stat(x), which is a neat ggplot trick that tells it to use the variable we mapped to the x-axis. For whatever reason, fill = temperatureHigh doesn’t work 🤷:\n\n\nCode\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = after_stat(x))) +\n  geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(x = \"High temperature\", y = NULL, color = \"Temp\")\n\n\n\n\n\n\n\n\n\nAnd finally, we can get extra fancy and show the distributions for both the high and low temperatures each month. To make this work, we need to manipulate the data a little. Right now there are two columns for high and low temperature: temperatureLow and temperatureHigh. To be able to map temperature to the x-axis and high vs. low to another aesthetic (like linetype), we need a column with the temperature and a column with an indicator variable for whether it is high or low. This data needs to be tidied (since right now we have a variable (high/low) encoded in the column name). We can tidy this data using pivot_longer() from {tidyr}, which was already loaded with library(tidyverse). In the Posit primers, you did this same thing with gather()—pivot_longer() is the newer version of gather():\n\n\nCode\nweather_atl_long &lt;- weather_atl %&gt;% \n  pivot_longer(cols = c(temperatureLow, temperatureHigh),\n               names_to = \"temp_type\",\n               values_to = \"temp\") %&gt;% \n  # Clean up the new temp_type column so that \"temperatureHigh\" becomes \"High\", etc.\n  mutate(temp_type = recode(temp_type, \n                            temperatureHigh = \"High\",\n                            temperatureLow = \"Low\")) %&gt;% \n  # This is optional—just select a handful of columns\n  select(time, temp_type, temp, Month) \n\n# Show the first few rows\nhead(weather_atl_long)\n## # A tibble: 6 × 4\n##   time                temp_type  temp Month  \n##   &lt;dttm&gt;              &lt;chr&gt;     &lt;dbl&gt; &lt;ord&gt;  \n## 1 2019-01-01 05:00:00 Low        50.6 January\n## 2 2019-01-01 05:00:00 High       63.9 January\n## 3 2019-01-02 05:00:00 Low        49.0 January\n## 4 2019-01-02 05:00:00 High       57.4 January\n## 5 2019-01-03 05:00:00 Low        53.1 January\n## 6 2019-01-03 05:00:00 High       55.3 January\n\n\nNow we have a column for the temperature (temp) and a column indicating if it is high or low (temp_type). The dataset is also twice as long (730 rows) because each day has two rows (high and low). Let’s plot it and map high/low to the linetype aesthetic to show high/low in the border of the plots:\n\n\nCode\nggplot(weather_atl_long, aes(x = temp, y = fct_rev(Month), \n                             fill = after_stat(x), linetype = temp_type)) +\n  geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(x = \"High temperature\", y = NULL, color = \"Temp\")\n\n\n\n\n\n\n\n\n\nSuper neat! We can see much wider temperature disparities during the summer, with large gaps between high and low, and relatively equal high/low temperatures during the winter."
  },
  {
    "objectID": "example/06-example.html#box-violin-and-rain-cloud-plots",
    "href": "example/06-example.html#box-violin-and-rain-cloud-plots",
    "title": "Uncertainty",
    "section": "Box, violin, and rain cloud plots",
    "text": "Box, violin, and rain cloud plots\nFinally, we can look at the distribution of variables with box plots, violin plots, and other similar graphs. First, we’ll make a box plot of windspeed, filled by the Day variable we made indicating weekday:\n\n\nCode\nggplot(weather_atl,\n       aes(y = windSpeed, fill = Day)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nWe can switch this to a violin plot by just changing the geom layer and mapping Day to the x-axis:\n\n\nCode\nggplot(weather_atl,\n       aes(y = windSpeed, x = Day, fill = Day)) +\n  geom_violin()\n\n\n\n\n\n\n\n\n\nWith violin plots it’s typically good to overlay other geoms. We can add some jittered points for a strip plot:\n\n\nCode\nggplot(weather_atl,\n       aes(y = windSpeed, x = Day, fill = Day)) +\n  geom_violin() +\n  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\nWe can also add larger points for the daily averages. We’ll use a special layer for this: stat_summary(). It has a slightly different syntax, since we’re not actually mapping a column from the dataset. Instead, we’re feeding a column from a dataset into a function (here \"mean\") and then plotting that result:\n\n\nCode\nggplot(weather_atl,\n       aes(y = windSpeed, x = Day, fill = Day)) +\n  geom_violin() +\n  stat_summary(geom = \"point\", fun = \"mean\", size = 5, color = \"white\") +\n  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\nWe can also show the mean and confidence interval at the same time by changing the summary function:\n\n\nCode\nggplot(weather_atl,\n       aes(y = windSpeed, x = Day, fill = Day)) +\n  geom_violin() +\n  stat_summary(geom = \"pointrange\", fun.data = \"mean_se\", size = 1, color = \"white\") +\n  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\nOverlaying the points directly on top of the violins shows extra information, but it’s also really crowded and hard to read. If we use the {gghalves} package, we can use special halved versions of some of these geoms like so:\n\n\nCode\nggplot(weather_atl,\n       aes(x = fct_rev(Day), y = temperatureHigh)) +\n  geom_half_point(aes(color = Day), side = \"l\", size = 0.5) +\n  geom_half_boxplot(aes(fill = Day), side = \"r\") +\n  guides(color = \"none\", fill = \"none\")\n\n\n\n\n\n\n\n\n\nNote the side argument for specifying which half of the column the geom goes. We can also use geom_half_violin():\n\n\nCode\nggplot(weather_atl,\n       aes(x = fct_rev(Day), y = temperatureHigh)) +\n  geom_half_point(aes(color = Day), side = \"l\", size = 0.5) +\n  geom_half_violin(aes(fill = Day), side = \"r\") +\n  guides(color = \"none\", fill = \"none\")\n\n\n\n\n\n\n\n\n\nIf we flip the plot, we can make a rain cloud plot:\n\n\nCode\nggplot(weather_atl,\n       aes(x = fct_rev(Day), y = temperatureHigh)) +\n  geom_half_boxplot(aes(fill = Day), side = \"l\", width = 0.5, nudge = 0.1) +\n  geom_half_point(aes(color = Day), side = \"l\", size = 0.5) +\n  geom_half_violin(aes(fill = Day), side = \"r\") +\n  guides(color = \"none\", fill = \"none\") + \n  coord_flip()\n\n\n\n\n\n\n\n\n\nNeat!"
  },
  {
    "objectID": "example/08-example.html",
    "href": "example/08-example.html",
    "title": "Comparisons",
    "section": "",
    "text": "For this example, we’re going to use cross-national data, but instead of using the typical gapminder dataset, we’re going to collect data directly from the World Bank’s Open Data portal\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):"
  },
  {
    "objectID": "example/08-example.html#live-coding-example",
    "href": "example/08-example.html#live-coding-example",
    "title": "Comparisons",
    "section": "Live coding example",
    "text": "Live coding example\n\n\n\n\n\n\n\n\n\n\nSlight differences from the video\n\n\n\nThis is a slightly cleaned up version of the code from the video."
  },
  {
    "objectID": "example/08-example.html#load-and-clean-data",
    "href": "example/08-example.html#load-and-clean-data",
    "title": "Comparisons",
    "section": "Load and clean data",
    "text": "Load and clean data\nFirst, we load the libraries we’ll be using:\n\n\nCode\nlibrary(tidyverse)  # For ggplot, dplyr, and friends\nlibrary(WDI)        # For getting data from the World Bank\nlibrary(geofacet)   # For map-shaped facets\nlibrary(scales)     # For helpful scale functions like label_dollar()\nlibrary(ggrepel)    # For non-overlapping labels\n\n\nThe World Bank has a ton of country-level data at data.worldbank.org. We can use a package named {WDI} (world development indicators) to access their servers and download the data directly into R.\nTo do this, we need to find the special World Bank codes for specific variables we want to get. These codes come from the URLs of the World Bank’s website. For instance, if you search for “access to electricity” at the World Bank’s website, you’ll find this page. If you look at the end of the URL, you’ll see a cryptic code: EG.ELC.ACCS.ZS. That’s the World Bank’s ID code for the “Access to electricity (% of population)” indicator.\nWe can feed a list of ID codes to the WDI() function to download data for those specific indicators. We can feed it a list of just the codes (e.g., c(\"SP.DYN.LE00.IN\", \"EG.ELC.ACCS.ZS\", ...)), but then the columns will be named with those ugly inscrutable names. Alternatively, we can feed the WDI() function a named list (e.g. c(life_expectancy = \"SP.DYN.LE00.IN\", ...)), which will create columns using the names we specify there. This is much nicer.\nWe want data from 1995-2015, so we set the start and end years accordingly. The extra=TRUE argument means that it’ll also include other helpful details like region, aid status, etc. Without it, it would only download the indicators we listed.\n\n\nCode\nindicators &lt;- c(life_expectancy = \"SP.DYN.LE00.IN\",  # Life expectancy\n                access_to_electricity = \"EG.ELC.ACCS.ZS\",  # Access to electricity\n                co2_emissions = \"EN.ATM.CO2E.PC\",  # CO2 emissions\n                gdp_per_cap = \"NY.GDP.PCAP.KD\")  # GDP per capita\n\nwdi_raw &lt;- WDI(country = \"all\", indicators, extra = TRUE, \n               start = 1995, end = 2015)\n\nhead(wdi_raw)\n\n\nDownloading data from the World Bank every time you knit will get tedious and take a long time (plus if their servers are temporarily down, you won’t be able to get the data). It’s good practice to save this raw data as a CSV file and then work with that.\n\n\nCode\nwrite_csv(wdi_raw, \"data/wdi_comparisons.csv\")\n\n\nSince we care about reproducibility, we still want to include the code we used to get data from the World Bank, we just don’t want it to actually run. You can include chunks but not run them by setting eval=FALSE in the chunk options. In this little example, we show the code for downloading the data, but we don’t evaluate the chunk. We then include a chunk that loads the data from a CSV file with read_csv(), but we don’t include it (include=FALSE). That way, in the knitted file we see the WDI() code, but in reality it’s loading the data from CSV. Super tricky.\nI first download data from the World Bank:\n\n```{r get-wdi-data, eval=FALSE}\nwdi_raw &lt;- WDI(...)\n\nwrite_csv(wdi_raw, \"data/wdi_comparisons.csv\")\n```\n\n```{r load-wdi-data-real, include=FALSE}\nwdi_raw &lt;- read_csv(\"data/wdi_comparisons.csv\")\n```\nThen we clean up the data a little by filtering out rows that aren’t actually countries:\n\n\nCode\nwdi_clean &lt;- wdi_raw %&gt;% \n  filter(region != \"Aggregates\")\n\nhead(wdi_clean)\n## # A tibble: 6 × 16\n##   country     iso2c iso3c  year status lastupdated life_expectancy access_to_electricity co2_emissions gdp_per_cap region   capital longitude latitude\n##   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt;  &lt;date&gt;                &lt;dbl&gt;                 &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n## 1 Afghanistan AF    AFG    1995 NA     2024-06-28             52.5                  NA          0.0888         NA  South A… Kabul        69.2     34.5\n## 2 Afghanistan AF    AFG    2015 NA     2024-06-28             62.7                  71.5        0.298         567. South A… Kabul        69.2     34.5\n## 3 Afghanistan AF    AFG    2008 NA     2024-06-28             59.9                  42.4        0.166         419. South A… Kabul        69.2     34.5\n## 4 Afghanistan AF    AFG    1996 NA     2024-06-28             53.2                  NA          0.0823         NA  South A… Kabul        69.2     34.5\n## 5 Afghanistan AF    AFG    2009 NA     2024-06-28             60.4                  48.3        0.240         490. South A… Kabul        69.2     34.5\n## 6 Afghanistan AF    AFG    2007 NA     2024-06-28             59.1                  38.4        0.110         411. South A… Kabul        69.2     34.5\n## # ℹ 2 more variables: income &lt;chr&gt;, lending &lt;chr&gt;"
  },
  {
    "objectID": "example/08-example.html#small-multiples",
    "href": "example/08-example.html#small-multiples",
    "title": "Comparisons",
    "section": "Small multiples",
    "text": "Small multiples\nFirst we can make some small multiples plots and show life expectancy over time for a handful of countries. We’ll make a list of some countries chosen at random while I scrolled through the data, and then filter our data to include only those rows. We then plot life expectancy, faceting by country.\n\n\nCode\nlife_expectancy_small &lt;- wdi_clean %&gt;%\n  filter(country %in% c(\"Argentina\", \"Bolivia\", \"Brazil\", \n                        \"Belize\", \"Canada\", \"Chile\"))\n\nggplot(data = life_expectancy_small, \n       mapping = aes(x = year, y = life_expectancy)) +\n  geom_line(linewidth = 1) +\n  facet_wrap(vars(country))\n\n\n\n\n\n\n\n\n\nSmall multiples! That’s all we need to do.\nWe can do some fancier things, though. We can make this plot hyper minimalist:\n\n\nCode\nggplot(data = life_expectancy_small, \n       mapping = aes(x = year, y = life_expectancy)) +\n  geom_line(linewidth = 1) +\n  facet_wrap(vars(country), scales = \"free_y\") +\n  theme_void() +\n  theme(strip.text = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\nWe can do a whole part of a continent (poor Iraq and Syria 😞)\n\n\nCode\nlife_expectancy_mena &lt;- wdi_clean %&gt;% \n  filter(region == \"Middle East & North Africa\")\n\nggplot(data = life_expectancy_mena, \n       mapping = aes(x = year, y = life_expectancy)) +\n  geom_line(linewidth = 1) +\n  facet_wrap(vars(country), scales = \"free_y\", nrow = 3) +\n  theme_void() +\n  theme(strip.text = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\nWe can use the {geofacet} package to arrange these facets by geography:\n\n\nCode\nlife_expectancy_eu &lt;- wdi_clean %&gt;% \n  filter(region == \"Europe & Central Asia\")\n\nggplot(life_expectancy_eu, aes(x = year, y = life_expectancy)) +\n  geom_line(linewidth = 1) +\n  facet_geo(vars(country), grid = \"eu_grid1\", scales = \"free_y\") +\n  labs(x = NULL, y = NULL, title = \"Life expectancy from 1995–2015\",\n       caption = \"Source: The World Bank (SP.DYN.LE00.IN)\") +\n  theme_minimal() +\n  theme(strip.text = element_text(face = \"bold\"),\n        plot.title = element_text(face = \"bold\"),\n        axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nNeat!"
  },
  {
    "objectID": "example/08-example.html#sparklines",
    "href": "example/08-example.html#sparklines",
    "title": "Comparisons",
    "section": "Sparklines",
    "text": "Sparklines\nSparklines are just line charts (or bar charts) that are really really small.\n\n\nCode\nindia_co2 &lt;- wdi_clean %&gt;% \n  filter(country == \"India\")\n\nplot_india &lt;- ggplot(india_co2, aes(x = year, y = co2_emissions)) +\n  geom_line() +\n  theme_void()\nplot_india\n\n\n\n\n\n\n\n\n\n\n\nCode\nggsave(\"india_co2.pdf\", plot_india, width = 1, height = 0.15, units = \"in\")\nggsave(\"india_co2.png\", plot_india, width = 1, height = 0.15, units = \"in\")\n\n\n\n\nCode\nchina_co2 &lt;- wdi_clean %&gt;% \n  filter(country == \"China\")\n\nplot_china &lt;- ggplot(china_co2, aes(x = year, y = co2_emissions)) +\n  geom_line() +\n  theme_void()\nplot_china\n\n\n\n\n\n\n\n\n\n\n\nCode\nggsave(\"china_co2.pdf\", plot_china, width = 1, heighlt = 0.15, units = \"in\")\nggsave(\"china_co2.png\", plot_china, width = 1, height = 0.15, units = \"in\")\n\n\nYou can then use those saved tiny plots in your text.\n\nBoth India  and China  have seen increased CO2 emissions over the past 20 years."
  },
  {
    "objectID": "example/08-example.html#slopegraphs",
    "href": "example/08-example.html#slopegraphs",
    "title": "Comparisons",
    "section": "Slopegraphs",
    "text": "Slopegraphs\nWe can make a slopegraph to show changes in GDP per capita between two time periods. We need to first filter our WDI to include only the start and end years (here 1995 and 2015). Then, to make sure that we’re using complete data, we’ll get rid of any country that has missing data for either 1995 or 2015. The group_by(...) %&gt;% filter(...) %&gt;% ungroup() pipeline does this, with the !any(is.na(gdp_per_cap)) test keeping any rows where any of the gdp_per_cap values are not missing for the whole country.\nWe then add a couple special columns for labels. The paste0() function concatenates strings and variables together, so that paste0(\"2 + 2 = \", 2 + 2) would show “2 + 2 = 4”. Here we make labels that say either “Country name: $GDP” or “$GDP” depending on the year.\n\n\nCode\ngdp_south_asia &lt;- wdi_clean %&gt;% \n  filter(region == \"South Asia\") %&gt;% \n  filter(year %in% c(1995, 2015)) %&gt;% \n  # Look at each country individually\n  group_by(country) %&gt;%\n  # Remove the country if any of its gdp_per_cap values are missing\n  filter(!any(is.na(gdp_per_cap))) %&gt;%\n  ungroup() %&gt;%\n  # Make year a factor\n  mutate(year = factor(year)) %&gt;% \n  # Make some nice label columns\n  # If the year is 1995, format it like \"Country name: $GDP\". If the year is\n  # 2015, format it like \"$GDP\"\n  mutate(label_first = ifelse(year == 1995, paste0(country, \": \", label_dollar()(round(gdp_per_cap))), NA),\n         label_last = ifelse(year == 2015, label_dollar()(round(gdp_per_cap, 0)), NA))\n\n\nWith the data filtered like this, we can plot it by mapping year to the x-axis, GDP per capita to the y-axis, and coloring by country. To make the lines go across the two categorical labels in the x-axis (since we made year a factor/category), we need to also specify the group aesthetic.\n\n\nCode\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +\n  geom_line(linewidth = 1.5)\n\n\n\n\n\n\n\n\n\nCool! We’re getting closer. We can definitely see different slopes, but with 7 different colors, it’s hard to see exactly which country is which. Instead, we can directly label each of these lines with geom_text():\n\n\nCode\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +\n  geom_line(linewidth = 1.5) +\n  geom_text(aes(label = country)) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\n\nThat gets us a little closer, but the country labels are hard to see, and we could include more information, like the actual values. Remember those label_first and label_last columns we made? Let’s use those instead:\n\n\nCode\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +\n  geom_line(linewidth = 1.5) +\n  geom_text(aes(label = label_first)) +\n  geom_text(aes(label = label_last)) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\n\nNow we have dollar amounts and country names, but the labels are still overlapping and really hard to read. To fix this, we can make the labels repel away from each other and randomly position in a way that makes them not overlap. The {ggrepel} package lets us do this with geom_text_repel()\n\n\nCode\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +\n  geom_line(linewidth = 1.5) +\n  geom_text_repel(aes(label = label_first)) +\n  geom_text_repel(aes(label = label_last)) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\n\nNow none of the labels are on top of each other, but the labels are still on top of the lines. Also, some of the labels moved inward and outward along the x-axis, but they don’t need to do that—they just need to shift up and down. We can force the labels to only move up and down by setting the direction = \"y\" argument, and we can move all the labels to the left or right with the nudge_x argument. The seed argument makes sure that the random label placement is the same every time we run this. It can be whatever number you want—it just has to be a number.\n\n\nCode\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +\n  geom_line(linewidth = 1.5) +\n  geom_text_repel(aes(label = label_first), direction = \"y\", nudge_x = -1, seed = 1234) +\n  geom_text_repel(aes(label = label_last), direction = \"y\", nudge_x = 1, seed = 1234) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\n\nThat’s it! Let’s take the theme off completely, change the colors a little, and it should be perfect.\n\n\nCode\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +\n  geom_line(linewidth = 1.5) +\n  geom_text_repel(aes(label = label_first), direction = \"y\", nudge_x = -1, seed = 1234) +\n  geom_text_repel(aes(label = label_last), direction = \"y\", nudge_x = 1, seed = 1234) +\n  guides(color = \"none\") +\n  scale_color_viridis_d(option = \"magma\", end = 0.9) +\n  theme_void()"
  },
  {
    "objectID": "example/08-example.html#bump-charts",
    "href": "example/08-example.html#bump-charts",
    "title": "Comparisons",
    "section": "Bump charts",
    "text": "Bump charts\nFinally, we can make a bump chart that shows changes in rankings over time. We’ll look at CO2 emissions in South Asia. First we need to calculate a new variable that shows the rank of each country within each year. We can do this if we group by year and then use the rank() function to rank countries by the co2_emissions column.\n\n\nCode\nsa_co2 &lt;- wdi_clean %&gt;% \n  filter(region == \"South Asia\") %&gt;% \n  filter(year &gt;= 2004, year &lt; 2015) %&gt;% \n  group_by(year) %&gt;% \n  mutate(rank = rank(co2_emissions))\n\n\nWe then plot this with points and lines, reversing the y-axis so 1 is at the top:\n\n\nCode\nggplot(sa_co2, aes(x = year, y = rank, color = country)) +\n  geom_line() +\n  geom_point() +\n  scale_y_reverse(breaks = 1:8)\n\n\n\n\n\n\n\n\n\nAfghanistan and Nepal switched around for the number 1 spot, while India dropped from 4 to 6, switching places with Pakistan.\nAs with the slopegraph, there are 8 different colors in the legend and it’s hard to line them all up with the different lines, so we can plot the text directly instead. We’ll use geom_text() again. We don’t need to repel anything, since the text should fit in each row just fine. We need to change the data argument in geom_text() though and filter the data to only include one year, otherwise we’ll get labels on every point, which is excessive. We can also adjust the theme and colors to make it cleaner.\n\n\nCode\nggplot(sa_co2, aes(x = year, y = rank, color = country)) +\n  geom_line(linewidth = 2) +\n  geom_point(size = 4) +\n  geom_text(data = filter(sa_co2, year == 2004),\n            aes(label = iso2c, x = 2003.25),\n            fontface = \"bold\") +\n  geom_text(data = filter(sa_co2, year == 2014),\n            aes(label = iso2c, x = 2014.75),\n            fontface = \"bold\") +\n  guides(color = \"none\") +\n  scale_y_reverse(breaks = 1:8) +\n  scale_x_continuous(breaks = 2004:2014) +\n  scale_color_viridis_d(option = \"magma\", begin = 0.2, end = 0.9) +\n  labs(x = NULL, y = \"Rank\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        panel.grid.minor.x = element_blank())\n\n\n\n\n\n\n\n\n\nIf you want to be super fancy, you can use flags instead of country codes, but that’s a little more complicated (you need to install the {ggflags} package. See here for an example."
  },
  {
    "objectID": "example/10-example.html",
    "href": "example/10-example.html",
    "title": "Interactivity",
    "section": "",
    "text": "For this example we’ll use data from the World Bank once again, which we download using the {WDI} package.\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):"
  },
  {
    "objectID": "example/10-example.html#live-coding-example",
    "href": "example/10-example.html#live-coding-example",
    "title": "Interactivity",
    "section": "Live coding example",
    "text": "Live coding example\nThere is no video for this one, since it really only involves feeding a few ggplot plots fed into ggplotly()."
  },
  {
    "objectID": "example/10-example.html#get-and-clean-data",
    "href": "example/10-example.html#get-and-clean-data",
    "title": "Interactivity",
    "section": "Get and clean data",
    "text": "Get and clean data\nFirst, we load the libraries we’ll be using:\n\n\nCode\nlibrary(tidyverse)  # For ggplot, dplyr, and friends\nlibrary(WDI)        # Get data from the World Bank\nlibrary(scales)     # For nicer label formatting\nlibrary(plotly)     # For easy interactive plots\n\n\n\n\nCode\nindicators &lt;- c(population = \"SP.POP.TOTL\",  # Population\n                prop_women_parl = \"SG.GEN.PARL.ZS\",  # Proportion of seats held by women in national parliaments (%)\n                gdp_per_cap = \"NY.GDP.PCAP.KD\")  # GDP per capita\n\nwdi_parl_raw &lt;- WDI(country = \"all\", indicators, extra = TRUE, \n                    start = 2000, end = 2019)\n\n\nThen we clean the data by removing non-country countries:\n\n\nCode\nwdi_clean &lt;- wdi_parl_raw %&gt;% \n  filter(region != \"Aggregates\")"
  },
  {
    "objectID": "example/10-example.html#creating-a-basic-interactive-chart",
    "href": "example/10-example.html#creating-a-basic-interactive-chart",
    "title": "Interactivity",
    "section": "Creating a basic interactive chart",
    "text": "Creating a basic interactive chart\nLet’s make a chart that shows the distribution of the proportion of women in national parliaments in 2019, by continent. We’ll use a strip plot with jittered points.\nFirst we need to make a regular static plot with ggplot:\n\n\nCode\nwdi_2019 &lt;- wdi_clean %&gt;% \n  filter(year == 2019) %&gt;% \n  drop_na(prop_women_parl) %&gt;% \n  # Scale this down from 0-100 to 0-1 so that scales::label_percent() can format\n  # it as an actual percent\n  mutate(prop_women_parl = prop_women_parl / 100)\n\nstatic_plot &lt;- ggplot(wdi_2019, \n                      aes(y = fct_rev(region), x = prop_women_parl, color = region)) +\n  geom_point(position = position_jitter(width = 0, height = 0.15, seed = 1234)) +\n  guides(color = \"none\") +\n  scale_x_continuous(labels = label_percent()) +\n  # I used https://medialab.github.io/iwanthue/ to generate these colors\n  scale_color_manual(values = c(\"#425300\", \"#e680ff\", \"#01bd71\", \"#ff3aad\",\n                                \"#9f3e00\", \"#0146bf\", \"#671d56\")) +\n  labs(x = \"% women in parliament\", y = NULL, caption = \"Source: The World Bank\") +\n  theme_bw()\nstatic_plot\n\n\n\n\n\n\n\n\n\nGreat! That looks pretty good.\nTo make it interactive, all we have to do is feed the static_plot object into ggplotly(). That’s it.\n\n\nCode\nggplotly(static_plot)\n\n\n\n\n\n\nNot everything translates over to JavaScript—the caption is gone now, and the legend is back (which is fine, I guess, since the legend is interactive). But still, this is magic."
  },
  {
    "objectID": "example/10-example.html#modifying-the-tooltip",
    "href": "example/10-example.html#modifying-the-tooltip",
    "title": "Interactivity",
    "section": "Modifying the tooltip",
    "text": "Modifying the tooltip\nRight now, the default tooltip you see when you hover over the points includes the actual proportion of women in parliament for each point, along with the continent, which is neat, but it’d be great if we could see the country name too. The tooltip picks up the information to include from the variables we use in aes(), and we never map the country column to any aesthetic, so it doesn’t show up.\nTo get around this, we can add a new aesthetic for country to the points. Instead of using one of the real ggplot aesthetics like color or fill, we’ll use a fake one called text (we can call it whatever we want! asdf would also work). ggplot has no idea how to do anything with the text aesthetic, and it’ll give you a warning, but that’s okay. The static plot looks the same:\n\n\nCode\nstatic_plot_toolip &lt;- ggplot(wdi_2019, \n                             aes(y = fct_rev(region), x = prop_women_parl, color = region)) +\n  geom_point(aes(text = country),\n             position = position_jitter(width = 0, height = 0.15, seed = 1234)) +\n  guides(color = \"none\") +\n  scale_x_continuous(labels = label_percent()) +\n  # I used https://medialab.github.io/iwanthue/ to generate these colors\n  scale_color_manual(values = c(\"#425300\", \"#e680ff\", \"#01bd71\", \"#ff3aad\",\n                                \"#9f3e00\", \"#0146bf\", \"#671d56\")) +\n  labs(x = \"% women in parliament\", y = NULL, caption = \"Source: The World Bank\") +\n  theme_bw()\n## Warning in geom_point(aes(text = country), position = position_jitter(width = 0, : Ignoring unknown aesthetics: text\nstatic_plot_toolip\n\n\n\n\n\n\n\n\n\nNow we can tell plotly to use this fake text aesthetic for the tooltip:\n\n\nCode\nggplotly(static_plot_toolip, tooltip = \"text\")\n\n\n\n\n\n\nNow we should just see the country names in the tooltips!"
  },
  {
    "objectID": "example/10-example.html#including-more-information-in-the-tooltip",
    "href": "example/10-example.html#including-more-information-in-the-tooltip",
    "title": "Interactivity",
    "section": "Including more information in the tooltip",
    "text": "Including more information in the tooltip\nWe have country names, but we lost the values in the x-axis. Rwanda has the highest proportion of women in parliament, but what’s the exact number? It’s somewhere above 60%, but that’s all we can see now.\nTo fix this, we can make a new column in the data with all the text we want to include in the tooltip. We’ll use paste0() to combine text and variable values to make the tooltip follow this format:\nName of country\nX% women in parliament\nLet’s add a new column with mutate(). A couple things to note here:\n\nThe &lt;br&gt; is HTML code for a line break\nWe use the label_percent() function to format numbers as percents. The accuracy argument tells R how many decimal points to use. If we used 1, it would say 12%; if we used 0.01, it would say 12.08%; etc.\n\n\n\nCode\nwdi_2019 &lt;- wdi_clean %&gt;% \n  filter(year == 2019) %&gt;% \n  drop_na(prop_women_parl) %&gt;% \n  # Scale this down from 0-100 to 0-1 so that scales::label_percent() can format\n  # it as an actual percent\n  mutate(prop_women_parl = prop_women_parl / 100) %&gt;% \n  mutate(fancy_label = paste0(country, \"&lt;br&gt;\", \n                              label_percent(accuracy = 0.1)(prop_women_parl), \n                              \" women in parliament\"))\n\n\nLet’s check to see if it worked:\n\n\nCode\nwdi_2019 %&gt;% select(country, prop_women_parl, fancy_label) %&gt;% head()\n## # A tibble: 6 × 3\n##   country             prop_women_parl fancy_label                                     \n##   &lt;chr&gt;                         &lt;dbl&gt; &lt;chr&gt;                                           \n## 1 Afghanistan                   0.279 Afghanistan&lt;br&gt;27.9% women in parliament        \n## 2 Albania                       0.295 Albania&lt;br&gt;29.5% women in parliament            \n## 3 Algeria                       0.258 Algeria&lt;br&gt;25.8% women in parliament            \n## 4 Andorra                       0.464 Andorra&lt;br&gt;46.4% women in parliament            \n## 5 Angola                        0.3   Angola&lt;br&gt;30.0% women in parliament             \n## 6 Antigua and Barbuda           0.111 Antigua and Barbuda&lt;br&gt;11.1% women in parliament\n\n\nNow instead of using text = country we’ll use text = fancy_label to map that new column onto the plot. Again, this won’t be visible in the static plot (and you’ll get a warning), but it will show up in the interactive plot.\n\n\nCode\nstatic_plot_toolip_fancy &lt;- ggplot(wdi_2019, \n                                   aes(y = fct_rev(region), \n                                       x = prop_women_parl, \n                                       color = region)) +\n  geom_point(aes(text = fancy_label),\n             position = position_jitter(width = 0, height = 0.15, seed = 1234)) +\n  guides(color = \"none\") +\n  scale_x_continuous(labels = label_percent()) +\n  # I used https://medialab.github.io/iwanthue/ to generate these colors\n  scale_color_manual(values = c(\"#425300\", \"#e680ff\", \"#01bd71\", \"#ff3aad\",\n                                \"#9f3e00\", \"#0146bf\", \"#671d56\")) +\n  labs(x = \"% women in parliament\", y = NULL, caption = \"Source: The World Bank\") +\n  theme_bw()\n## Warning in geom_point(aes(text = fancy_label), position = position_jitter(width = 0, : Ignoring unknown aesthetics: text\n\n\n\n\nCode\nggplotly(static_plot_toolip_fancy, tooltip = \"text\")\n\n\n\n\n\n\nPerfect!\nFinally, if we want to save this plot as a standalone self-contained HTML file, we can use the saveWidget() function from the {htmlwidgets} package.\n\n\nCode\n# This is like ggsave, but for interactive HTML plots\ninteractive_plot &lt;- ggplotly(static_plot_toolip_fancy, tooltip = \"text\")\n\nhtmlwidgets::saveWidget(interactive_plot, \"fancy_plot.html\")"
  },
  {
    "objectID": "example/10-example.html#making-a-dashboard-with-flexdashboard",
    "href": "example/10-example.html#making-a-dashboard-with-flexdashboard",
    "title": "Interactivity",
    "section": "Making a dashboard with {flexdashboard}",
    "text": "Making a dashboard with {flexdashboard}\nThe documentation for {flexdashboard} is so great and complete that I’m not going to include a full example here. There is also a brief overview in chapter 5 of the official R Markdown book. You can also watch this really quick video here. She uses a package called {dimple} instead of {plotly}, which doesn’t work with ggplot like ggplotly(), so ignore her code about dimple() and use your ggplotly() skills instead. You can search YouTube for a bunch of other short tutorial videos, too.\nThe quickest and easiest way to get started is to install the {flexdashboard} package and then in RStudio go to File &gt; New File… &gt; R Markdown… &gt; From Template &gt; Flexdashboard:\n\n\n\nThat will give you an empty dashboard with three chart areas spread across two columns. Put static or dynamic graphs in the different chart areas, knit, and you’ll be good to go!\nIf you’re interested in making the dashboard reactive with Shiny-like elements, check out this tutorial."
  },
  {
    "objectID": "example/12-example.html",
    "href": "example/12-example.html",
    "title": "Space",
    "section": "",
    "text": "Shapefiles are special types of data that include information about geography, such as points (latitude, longitude), paths (a bunch of connected latitudes and longitudes) and areas (a bunch of connected latitudes and longitudes that form a complete shape). Nowadays, most government agencies provide shapefiles for their jurisdictions. For global mapping data, you can use the Natural Earth project:\n\nNatural Earth\nUS Census Bureau\nGeorgia GIS Clearinghouse (requires a free account; the interface is incredibly clunky)\nAtlanta Regional Council\nFulton County GIS Portal\nCity of Atlanta, Department of City Planning"
  },
  {
    "objectID": "example/12-example.html#shapefiles",
    "href": "example/12-example.html#shapefiles",
    "title": "Space",
    "section": "",
    "text": "Shapefiles are special types of data that include information about geography, such as points (latitude, longitude), paths (a bunch of connected latitudes and longitudes) and areas (a bunch of connected latitudes and longitudes that form a complete shape). Nowadays, most government agencies provide shapefiles for their jurisdictions. For global mapping data, you can use the Natural Earth project:\n\nNatural Earth\nUS Census Bureau\nGeorgia GIS Clearinghouse (requires a free account; the interface is incredibly clunky)\nAtlanta Regional Council\nFulton County GIS Portal\nCity of Atlanta, Department of City Planning"
  },
  {
    "objectID": "example/12-example.html#projections-and-coordinate-reference-systems",
    "href": "example/12-example.html#projections-and-coordinate-reference-systems",
    "title": "Space",
    "section": "Projections and coordinate reference systems",
    "text": "Projections and coordinate reference systems\nAs you read in this week’s readings, projections matter a lot for maps. You can convert your geographic data between different coordinate systems (or projections) fairly easily with {sf}. You can use coord_sf(crs = st_crs(\"XXXX\")) to convert coordinate reference systems (CRS) as you plot, or use st_transform() to convert data frames to a different CRS.\n\n\n\n\n\n\nNot 100% correct\n\n\n\nTECHNICALLY coordinate systems and projection systems are different things, but I’m not a geographer and I don’t care that much about the nuance.\n\n\nThere are standard indexes of more than 4,000 of these projections (!!!) at epsg.io.\nSuper important: When using these projections, you need to specify both the projection catalog (ESRI or EPSG; see here for the difference) and the projection number, separated by a colon (e.g. “ESRI:54030”). Fortunately epsg.io makes this super easy: go to the epsg.io page for the projection you want to use and the page title will have the correct name.\nHere are some common ones:\n\nESRI:54002: Equidistant cylindrical projection for the world1\nEPSG:3395: Mercator projection for the world\nESRI:54008: Sinusoidal projection for the world\nESRI:54009: Mollweide projection for the world\nESRI:54030: Robinson projection for the world (This is my favorite world projection.)\nEPSG:4326: WGS 84: DOD GPS coordinates (standard −180 to 180 system)\nEPSG:4269: NAD 83: Relatively common projection for North America\nESRI:102003: Albers projection specifically for the contiguous United States\n\nAlternatively, instead of using these index numbers, you can use any of the names listed here, such as:\n\n\"+proj=merc\": Mercator\n\"+proj=robin\": Robinson\n\"+proj=moll\": Mollweide\n\"+proj=aeqd\": Azimuthal Equidistant\n\"+proj=cass\": Cassini-Soldner"
  },
  {
    "objectID": "example/12-example.html#shapefiles-to-download",
    "href": "example/12-example.html#shapefiles-to-download",
    "title": "Space",
    "section": "Shapefiles to download",
    "text": "Shapefiles to download\nI use a lot of different shapefiles in this example. To save you from having to go find and download each individual one, you can download this zip file:\n\n shapefiles.zip\n\nUnzip this and put all the contained folders in a folder named data if you want to follow along. You don’t need to follow along!\nYour project should be structured like this:\nyour-project-name\\\n  some-name.Rmd\n  your-project-name.Rproj\n  data\\\n    cb_2022_us_county_5m\\\n      ...\n      cb_2022_us_county_5m.shp\n      ...\n    cb_2022_us_state_20m\\\n    ne_10m_admin_1_states_provinces\\\n    ne_10m_lakes\\\n    ne_10m_rivers_lake_centerlines\\\n    ne_10m_rivers_north_america\\\n    ne_110m_admin_0_countries\\\n    schools_2009\\\nThese shapefiles all came from these sources:\n\n World map: 110m “Admin 0 - Countries” from Natural Earth\n US states: 20m 2022 state boundaries from the US Census Bureau\n US counties: 5m 2022 county boundaries from the US Census Bureau\n US states high resolution: 10m “Admin 1 – States, Provinces” from Natural Earth\n Global rivers: 10m “Rivers + lake centerlines” from Natural Earth\n North American rivers: 10m “Rivers + lake centerlines, North America supplement” from Natural Earth\n Global lakes: 10m “Lakes + Reservoirs” from Natural Earth\n Georgia K–12 schools, 2009: “Georgia K-12 Schools” from the Georgia Department of Education (you must be logged in to access this)"
  },
  {
    "objectID": "example/12-example.html#live-coding-example",
    "href": "example/12-example.html#live-coding-example",
    "title": "Space",
    "section": "Live coding example",
    "text": "Live coding example\n\n\n\n\n\n\n\n\n\n\nSlight differences from the video\n\n\n\nThis is a slightly cleaned up version of the code from the video."
  },
  {
    "objectID": "example/12-example.html#load-and-look-at-data",
    "href": "example/12-example.html#load-and-look-at-data",
    "title": "Space",
    "section": "Load and look at data",
    "text": "Load and look at data\nFirst we’ll load the libraries we’re going to use:\n\n\nCode\nlibrary(tidyverse)  # For ggplot, dplyr, and friends\nlibrary(sf)         # For GIS magic\n\n\nNext we’ll load all the different shapefiles we downloaded using read_sf():\n\n\nCode\n# Download \"Admin 0 – Countries\" from\n# https://www.naturalearthdata.com/downloads/110m-cultural-vectors/\nworld_map &lt;- read_sf(\"data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp\")\n\n# Download cb_2022_us_state_20m.zip under \"States\" from\n# https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html\nus_states &lt;- read_sf(\"data/cb_2022_us_state_20m/cb_2022_us_state_20m.shp\")\n\n# Download cb_2022_us_county_5m.zip under \"County\" from\n# https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html\nus_counties &lt;- read_sf(\"data/cb_2022_us_county_5m/cb_2022_us_county_5m.shp\")\n\n# Download \"Admin 1 – States, Provinces\" from\n# https://www.naturalearthdata.com/downloads/10m-cultural-vectors/\nus_states_hires &lt;- read_sf(\"data/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp\")\n\n# Download \"Rivers + lake centerlines\" from\n# https://www.naturalearthdata.com/downloads/10m-physical-vectors/\nrivers_global &lt;- read_sf(\"data/ne_10m_rivers_lake_centerlines/ne_10m_rivers_lake_centerlines.shp\")\n\n# Download \"Rivers + lake centerlines, North America supplement\" from\n# https://www.naturalearthdata.com/downloads/10m-physical-vectors/\nrivers_na &lt;- read_sf(\"data/ne_10m_rivers_north_america/ne_10m_rivers_north_america.shp\")\n\n# Download \"Lakes + Reservoirs\" from\n# https://www.naturalearthdata.com/downloads/10m-physical-vectors/\nlakes &lt;- read_sf(\"data/ne_10m_lakes/ne_10m_lakes.shp\")\n\n# Download from https://data.georgiaspatial.org/index.asp?body=preview&dataId=41516\n# after creating an account and logging in\nga_schools &lt;- read_sf(\"data/schools_2009/DOE Schools 2009.shp\")"
  },
  {
    "objectID": "example/12-example.html#basic-plotting",
    "href": "example/12-example.html#basic-plotting",
    "title": "Space",
    "section": "Basic plotting",
    "text": "Basic plotting\nIf you look at the world_map dataset in RStudio, you’ll see it’s just a standard data frame with 177 rows and 169 columns. The last column is the magical geometry column with the latitude/longitude details for the borders for every country. RStudio only shows you 50 columns at a time in the RStudio viewer, so you’ll need to move to the next page of columns with the » button in the top left corner.\nBecause this is just a data frame, we can do all our normal dplyr things to it. Let’s get rid of Antarctica, since it takes up a big proportion of the southern hemisphere:\n\n\nCode\nworld_sans_antarctica &lt;- world_map %&gt;% \n  filter(ISO_A3 != \"ATA\")\n\n\nReady to plot a map? Here’s all you need to do:\n\n\nCode\nggplot() + \n  geom_sf(data = world_sans_antarctica)\n\n\n\n\n\n\n\n\n\nIf you couldn’t tell from the lecture, I’m completely blown away by how amazingly easy this every time I plot a map :)\nBecause this a regular ggplot geom, all our regular aesthetics and themes and everything work:\n\n\nCode\nggplot() + \n  geom_sf(data = world_sans_antarctica, \n          fill = \"#669438\", color = \"#32481B\", linewidth = 0.25) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nThe Natural Earth dataset happens to come with some columns with a coloring scheme with 7–13 colors (MAPCOLOR7, MAPCOLOR9, etc.) so that no countries with a shared border share a color. We can fill by that column:\n\n\nCode\nggplot() + \n  geom_sf(data = world_sans_antarctica, \n          aes(fill = as.factor(MAPCOLOR7)),\n          color = \"#401D16\", linewidth = 0.25) +\n  scale_fill_viridis_d(option = \"plasma\") +\n  guides(fill = \"none\") +\n  theme_void()"
  },
  {
    "objectID": "example/12-example.html#world-map-with-different-projections",
    "href": "example/12-example.html#world-map-with-different-projections",
    "title": "Space",
    "section": "World map with different projections",
    "text": "World map with different projections\nChanging projections is trivial: add a coord_sf() layer where you specify the CRS you want to use.\nHere’s Robinson (yay):\n\n\nCode\nggplot() + \n  geom_sf(data = world_sans_antarctica, \n          fill = \"#669438\", color = \"#32481B\", linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"ESRI:54030\")) +  # Robinson\n  # Or use the name instead of the number\n  # coord_sf(crs = \"+proj=robin\")\n  theme_void()\n\n\n\n\n\n\n\n\n\nHere’s sinusoidal:\n\n\nCode\nggplot() + \n  geom_sf(data = world_sans_antarctica, \n          fill = \"#669438\", color = \"#32481B\", linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"ESRI:54008\")) +  # Sinusoidal\n  theme_void()\n\n\n\n\n\n\n\n\n\nAnd here’s Mercator (ewww):\n\n\nCode\nggplot() + \n  geom_sf(data = world_sans_antarctica, \n          fill = \"#669438\", color = \"#32481B\", linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"EPSG:3395\")) +  # Mercator\n  # Or use the name instead of the number\n  # coord_sf(crs = \"+proj=merc\")\n  theme_void()"
  },
  {
    "objectID": "example/12-example.html#us-map-with-different-projections",
    "href": "example/12-example.html#us-map-with-different-projections",
    "title": "Space",
    "section": "US map with different projections",
    "text": "US map with different projections\nThis same process works for any shapefile. The map of the US can also be projected differently—two common projections are NAD83 and Albers. We’ll take the us_states dataset, remove Alaska, Hawaii, and Puerto Rico (they’re so far from the rest of the lower 48 states that they make an unusable map—see the next section for a way to include them), and plot it.\n\n\nCode\nlower_48 &lt;- us_states %&gt;% \n  filter(!(NAME %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")))\n\nggplot() + \n  geom_sf(data = lower_48, fill = \"#192DA1\", color = \"white\", linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"EPSG:4269\")) +  # NAD83\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot() + \n  geom_sf(data = lower_48, fill = \"#192DA1\", color = \"white\", linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_void()"
  },
  {
    "objectID": "example/12-example.html#us-map-with-non-continguous-parts",
    "href": "example/12-example.html#us-map-with-non-continguous-parts",
    "title": "Space",
    "section": "US map with non-continguous parts",
    "text": "US map with non-continguous parts\nPlotting places like Alaska, Hawaii, and Puerto Rico gets a little tricky since they’re far away from the contiguous 48 states. There’s an easy way to handle it though!\nFirst, there’s a package named {tigris} that provides a neat interface for working with spatial data from the US Census’s TIGER shapefiles. In addition to providing a ton of functions for getting shapefiles for states, counties, voting districts, Tribal areas, military bases, and dozens of other things, {tigris} has a shift_geometry() function that will change the coordinates for Alaska, Hawaii, and Puerto Rico so that they end up in Mexico and the Gulf of Mexico.\n\n\nCode\nlibrary(tigris)\n\n# This is the Census shapefile we loaded earlier. Note how we're not filtering\n# out AK, HI, and PR now\nus_states_shifted &lt;- us_states %&gt;% \n  shift_geometry()  # Move AK, HI, and PR around\n\nggplot() +\n  geom_sf(data = us_states_shifted) +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_void()\n\n\n\n\n\n\n\n\n\nThe shift_geometry() function should work on any shapefile. What if you have a shapefile with the coordinates of all public libraries in Alaska? Those will use the actual coordinates, not the shifted-to-Mexico coordinates. Feed that data to shift_geometry() and it should translate any coordinates you have in Alaska, Hawaii, and Puerto Rico to the Mexico area so they’ll plot correctly.\nshift_geometry() has an optional position argument that lets you control where the non-contiguous states go. By default they’ll go below the continental US (position = \"below\"), but you can also use position = \"outside\" to place them more in relation to where they are in real life:\n\n\nCode\nus_states_shifted &lt;- us_states %&gt;% \n  shift_geometry(position = \"outside\")\n\nggplot() +\n  geom_sf(data = us_states_shifted) +\n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_void()"
  },
  {
    "objectID": "example/12-example.html#individual-states",
    "href": "example/12-example.html#individual-states",
    "title": "Space",
    "section": "Individual states",
    "text": "Individual states\nAgain, because these shapefiles are really just fancy data frames, we can filter them with normal dplyr functions. Let’s plot just Georgia:\n\n\nCode\nonly_georgia &lt;- lower_48 %&gt;% \n  filter(NAME == \"Georgia\")\n\nggplot() +\n  geom_sf(data = only_georgia, fill = \"#EC8E55\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nWe can also use a different projection. If we look at epsg.io, there’s a version of NAD83 that’s focused specifically on Georgia.\n\n\nCode\nggplot() +\n  geom_sf(data = only_georgia, fill = \"#EC8E55\") +\n  theme_void() +\n  coord_sf(crs = st_crs(\"EPSG:2239\"))  # NAD83 focused on Georgia\n\n\n\n\n\n\n\n\n\nThere’s one small final issue though: we’re missing all the Atlantic islands in the southeast like Cumberland Island and Amelia Island. That’s because we’re using the Census’s low resolution (20m) data. That’s fine for the map of the whole country, but if we’re looking at a single state, we probably want better detail in the borders. We can use the Census’s high resolution (500k) data, but even then it doesn’t include the islands for whatever reason, but Natural Earth has high resolution US state data that does have the islands, so we can use that:\n\n\nCode\nonly_georgia_high &lt;- us_states_hires %&gt;% \n  filter(iso_3166_2 == \"US-GA\")\n\nggplot() +\n  geom_sf(data = only_georgia_high, fill = \"#EC8E55\") +\n  theme_void() +\n  coord_sf(crs = st_crs(\"EPSG:2239\"))  # NAD83 focused on Georgia\n\n\n\n\n\n\n\n\n\nPerfect."
  },
  {
    "objectID": "example/12-example.html#plotting-multiple-shapefile-layers",
    "href": "example/12-example.html#plotting-multiple-shapefile-layers",
    "title": "Space",
    "section": "Plotting multiple shapefile layers",
    "text": "Plotting multiple shapefile layers\nThe state shapefiles from the Census Bureau only include state boundaries. If we want to see counties in Georgia, we need to download and load the Census’s county shapefiles (which we did above). We can then add a second geom_sf() layer for the counties.\nFirst we need to filter the county data to only include Georgia counties. The counties data doesn’t include a column with the state name or state abbreviation, but it does include a column named STATEFP, which is the state FIPS code. Looking at lower_48 we can see that the state FIPS code for Georgia is 13, so we use that to filter.\n\n\nCode\nga_counties &lt;- us_counties %&gt;% \n  filter(STATEFP == 13)\n\n\nNow we can plot just the counties:\n\n\nCode\nggplot() +\n  geom_sf(data = ga_counties) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nTechnically we can just draw the county boundaries instead of layer the state boundary + the counties, since the borders of the counties make up the border of the state. But there’s an advantage to including both: we can use different aesthetics on each, like adding a thicker border on the state:\n\n\nCode\nggplot() +\n  geom_sf(data = only_georgia_high, color = \"#EC8E55\", linewidth = 3) +\n  geom_sf(data = ga_counties, fill = \"#A5D46A\", color = \"white\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nIt’s also useful if we want to only show some counties, like metropolitan Atlanta:\n\n\nCode\natl_counties &lt;- ga_counties %&gt;% \n  filter(NAME %in% c(\"Cherokee\", \"Clayton\", \"Cobb\", \"DeKalb\", \"Douglas\",\n                     \"Fayette\", \"Fulton\", \"Gwinnett\", \"Henry\", \"Rockdale\"))\nggplot() +\n  geom_sf(data = only_georgia_high, fill = \"#EC8E55\") +\n  geom_sf(data = atl_counties, fill = \"#A5D46A\", color = \"white\") +\n  theme_void()"
  },
  {
    "objectID": "example/12-example.html#plotting-multiple-shapefile-layers-when-some-are-bigger-than-the-parent-shape",
    "href": "example/12-example.html#plotting-multiple-shapefile-layers-when-some-are-bigger-than-the-parent-shape",
    "title": "Space",
    "section": "Plotting multiple shapefile layers when some are bigger than the parent shape",
    "text": "Plotting multiple shapefile layers when some are bigger than the parent shape\nSo far we’ve been able to filter out states and counties that we don’t want to plot using filter(), which works because the shapefiles have geometry data for each state or county. But what if you’re plotting stuff that doesn’t follow state or county boundaries, like freeways, roads, rivers, or lakes?\nAt the beginning we loaded a shapefile for all large and small rivers in the US. Look at the first few rows of rivers_na:\n\n\nCode\nhead(rivers_na)\n## Simple feature collection with 6 features and 40 fields\n## Geometry type: MULTILINESTRING\n## Dimension:     XY\n## Bounding box:  xmin: -95 ymin: 36 xmax: -91 ymax: 40\n## Geodetic CRS:  WGS 84\n## # A tibble: 6 × 41\n##   strokeweig name   featurecla scalerank min_zoom min_label  ne_id rivernum dissolve name_alt note  name_full label wikidataid name_ar name_bn name_de\n##        &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n## 1       0.15 North… River             12        7         8 1.75e9       NA &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;      &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n## 2       0.2  North… River             12        7         8 1.75e9       NA &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;      &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n## 3       0.2  Salt   River             12        7         8 1.16e9    22682 22682Ri… &lt;NA&gt;     ID i… Salt Riv… Salt  Q745070    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n## 4       0.15 Spring River             12        7         8 1.75e9       NA &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;      &lt;NA&gt;  &lt;NA&gt;       &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n## 5       0.2  Spring River             12        7         8 1.75e9       NA &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;      &lt;NA&gt;  Q7580668   &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n## 6       0.15 Illin… River             12        7         8 1.16e9    22643 22643Ri… &lt;NA&gt;     ID i… Illinois… Illi… Q5999623   &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n## # ℹ 24 more variables: name_en &lt;chr&gt;, name_es &lt;chr&gt;, name_fr &lt;chr&gt;, name_el &lt;chr&gt;, name_hi &lt;chr&gt;, name_hu &lt;chr&gt;, name_id &lt;chr&gt;, name_it &lt;chr&gt;,\n## #   name_ja &lt;chr&gt;, name_ko &lt;chr&gt;, name_nl &lt;chr&gt;, name_pl &lt;chr&gt;, name_pt &lt;chr&gt;, name_ru &lt;chr&gt;, name_sv &lt;chr&gt;, name_tr &lt;chr&gt;, name_vi &lt;chr&gt;,\n## #   name_zh &lt;chr&gt;, name_fa &lt;chr&gt;, name_he &lt;chr&gt;, name_uk &lt;chr&gt;, name_ur &lt;chr&gt;, name_zht &lt;chr&gt;, geometry &lt;MULTILINESTRING [°]&gt;\n\n\nThe first row is the whole Colorado river, which flows through seven states. We can’t just use filter() to only select some parts of it based on states.\nHere’s what happens if we combine our Georgia map with rivers and lakes:\n\n\nCode\nggplot() +\n  geom_sf(data = only_georgia, fill = \"#EC8E55\") +\n  geom_sf(data = rivers_na) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nIt plots Georgia, and it’s filled with orange, but it also plots every single river in North America. Oops.\nWe need to do a little GIS work to basically use only_georgia as a cookie cutter and keep only the rivers that are contained in the only_georgia boundaries. Fortunately, there’s a function in the {sf} package that does this: st_intersection(). Feed it two shapefile datasets and it will select the parts of the second that fall within the boundaries of the first:\n\n\nCode\nga_rivers_na &lt;- st_intersection(only_georgia, rivers_na)\n## Error in geos_op2_geom(\"intersection\", x, y, ...): st_crs(x) == st_crs(y) is not TRUE\n\n\nOh no! An error! It’s complaining that the reference systems used in these two datasets don’t match. We can check the CRS with st_crs():\n\n\nCode\nst_crs(only_georgia)\n## Coordinate Reference System:\n##   User input: NAD83 \n##   wkt:\n## GEOGCRS[\"NAD83\",\n##     DATUM[\"North American Datum 1983\",\n##         ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n##             LENGTHUNIT[\"metre\",1]]],\n##     PRIMEM[\"Greenwich\",0,\n##         ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     CS[ellipsoidal,2],\n##         AXIS[\"latitude\",north,\n##             ORDER[1],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         AXIS[\"longitude\",east,\n##             ORDER[2],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     ID[\"EPSG\",4269]]\nst_crs(rivers_na)\n## Coordinate Reference System:\n##   User input: WGS 84 \n##   wkt:\n## GEOGCRS[\"WGS 84\",\n##     DATUM[\"World Geodetic System 1984\",\n##         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n##             LENGTHUNIT[\"metre\",1]]],\n##     PRIMEM[\"Greenwich\",0,\n##         ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     CS[ellipsoidal,2],\n##         AXIS[\"latitude\",north,\n##             ORDER[1],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##         AXIS[\"longitude\",east,\n##             ORDER[2],\n##             ANGLEUNIT[\"degree\",0.0174532925199433]],\n##     ID[\"EPSG\",4326]]\n\n\nThe Georgia map uses EPSG:4269 (or NAD83), while the rivers map uses EPSG:4326 (or the GPS system of latitude and longitude). We need to convert one of them to make them match. It doesn’t matter which one.\n\n\nCode\nonly_georgia_4326 &lt;- only_georgia %&gt;% \n  st_transform(crs = st_crs(\"EPSG:4326\"))\n\nga_rivers_na &lt;- st_intersection(only_georgia_4326, rivers_na)\n## Warning: attribute variables are assumed to be spatially constant throughout all geometries\n\n\nYou’ll get an ominous warning, but you should be okay—it’s just because flattening globes into flat planes is hard, and the cutting might not be 100% accurate, but it’ll be close enough for our mapping purposes.\nNow we can plot our state shape and the truncated rivers:\n\n\nCode\nggplot() +\n  geom_sf(data = only_georgia, fill = \"#EC8E55\") +\n  geom_sf(data = ga_rivers_na) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nHey! It worked! Let’s put all the rivers and lakes on at once and make it a little more artsy. We’ll use the high resolution Georgia map too, which conveniently already matches the CRS of the rivers and lakes:\n\n\nCode\nga_rivers_na &lt;- st_intersection(only_georgia_high, rivers_na)\nga_rivers_global &lt;- st_intersection(only_georgia_high, rivers_global)\n\n# sf v1.0 changed how it handles shapefiles with spherical elements, which\n# apparently the lakes data uses. Currently when using st_intersection() and\n# other GIS-related functions, it breaks. This can be fixed by feeding the lakes\n# data to st_make_valid(), which does something fancy behind the scenes to make\n# it work. See this: https://github.com/r-spatial/sf/issues/1649#issuecomment-853279986\nga_lakes &lt;- st_intersection(only_georgia_high, st_make_valid(lakes))\n\nggplot() +\n  geom_sf(data = only_georgia_high, \n          color = \"black\", linewidth = 0.1, fill = \"black\") +\n  geom_sf(data = ga_rivers_global, linewidth = 0.3, color = \"grey80\") +\n  geom_sf(data = ga_rivers_na, linewidth = 0.15, color = \"grey80\") +\n  geom_sf(data = ga_lakes, linewidth = 0.3, fill = \"grey80\", color = NA) +\n  coord_sf(crs = st_crs(\"EPSG:4326\")) +  # NAD83\n  theme_void()\n\n\n\n\n\n\n\n\n\nHeck yeah. That’s a great map. This is basically what Kieran Healy did here, but he used even more detailed shapefiles from the US Geological Survey."
  },
  {
    "objectID": "example/12-example.html#plotting-schools-in-georgia",
    "href": "example/12-example.html#plotting-schools-in-georgia",
    "title": "Space",
    "section": "Plotting schools in Georgia",
    "text": "Plotting schools in Georgia\nShapefiles are not limited to just lines and areas—they can also contain points. I made a free account at the Georgia GIS Clearinghouse, searched for “schools” and found a shapefile of all the K–12 schools in 2009. This is the direct link to the page, but it only works if you’re logged in to their system. This is the official metadata for the shapefile, which you can see if you’re not logged in, but you can’t download anything. It’s a dumb system and other states are a lot better at offering their GIS data (like, here’s a shapefile of all of Utah’s schools and libraries as of 2017, publicly accessible without an account).\nWe loaded the shapefile up at the top, but now let’s look at it:\n\n\nCode\nhead(ga_schools)\n## Simple feature collection with 6 features and 16 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 2100000 ymin: 320000 xmax: 2200000 ymax: 5e+05\n## Projected CRS: NAD83 / Georgia West (ftUS)\n## # A tibble: 6 × 17\n##      ID  DATA COUNTY  DISTRICT   SCHOOLNAME GRADES ADDRESS CITY  STATE ZIP   TOTAL SCHOOLID DOE_CONGRE CONGRESS SENATE HOUSE                 geometry\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;POINT [US_survey_foot]&gt;\n## 1  4313   224 Early   Early Cou… Early Cou… PK,KK… 283 Ma… Blak… GA    3982…  1175    43549 2          002      011    149           (2052182 494322)\n## 2  4321   227 Early   Early Cou… ETN Ecker… 06,07… 313 E … Blak… GA    3982…    30    47559 2          002      011    149            (2053200 5e+05)\n## 3  4329   226 Early   Early Cou… Early Cou… 06,07… 12053 … Blak… GA    3982…   539    43550 2          002      011    149            (2055712 5e+05)\n## 4  4337   225 Early   Early Cou… Early Cou… 09,10… 12020 … Blak… GA    3982…   716    43552 2          002      011    149            (2055712 5e+05)\n## 5  4345   189 Decatur Decatur C… John John… PK,KK… 1947 S… Bain… GA    3981…   555    43279 2          002      011    172           (2168090 321781)\n## 6  4353   192 Decatur Decatur C… Potter St… PK,KK… 725 Po… Bain… GA    3981…   432    43273 2          002      011    172           (2168751 327375)\n\n\nWe have a bunch of columns like GRADES that has a list of what grades are included in the school, and TOTAL, which I’m guessing is the number of students. Let’s map it!\nIf we add a geom_sf() layer just for ga_schools, it’ll plot a bunch of points:\n\n\nCode\nggplot() +\n  geom_sf(data = ga_schools)\n\n\n\n\n\n\n\n\n\nOne of these rows is wildly miscoded and ended up Indonesia! If you sort by the geometry column in RStudio, you’ll see that it’s most likely Allatoona High School in Cobb County (id = 22097). The coordinates are different from all the others, and it has no congressional district information. Let’s remove it.\n\n\nCode\nga_schools_fixed &lt;- ga_schools %&gt;% \n  filter(ID != 22097)\n\nggplot() +\n  geom_sf(data = ga_schools_fixed)\n\n\n\n\n\n\n\n\n\nThat’s better. However, all we’re plotting now are the points—we’ve lost the state and/or county boundaries. Let’s include those:\n\n\nCode\nggplot() +\n  geom_sf(data = only_georgia_high) +\n  geom_sf(data = ga_schools_fixed) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nWe’re getting closer. We have some issues with overplotting, so let’s shrink the points down and make them a little transparent:\n\n\nCode\nggplot() +\n  geom_sf(data = only_georgia_high) +\n  geom_sf(data = ga_schools_fixed, size = 0.5, alpha = 0.5) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nNeat. One last thing we can do is map the TOTAL column to the color aesthetic and color the points by how many students attend each school:\n\n\nCode\nggplot() +\n  geom_sf(data = only_georgia_high) +\n  geom_sf(data = ga_schools_fixed, aes(color = TOTAL), size = 0.75, alpha = 0.5) +\n  scale_color_viridis_c() +\n  theme_void()\n\n\n\n\n\n\n\n\n\nMost schools appear to be under 1,000 students, except for a cluster in Gwinnett County north of Atlanta. Its high schools have nearly 4,000 students each!\n\n\nCode\nga_schools_fixed %&gt;% \n  select(COUNTY, SCHOOLNAME, TOTAL) %&gt;% \n  arrange(desc(TOTAL)) %&gt;% \n  head()\n## Simple feature collection with 6 features and 3 fields\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: 2300000 ymin: 1400000 xmax: 2400000 ymax: 1500000\n## Projected CRS: NAD83 / Georgia West (ftUS)\n## # A tibble: 6 × 4\n##   COUNTY   SCHOOLNAME                  TOTAL                 geometry\n##   &lt;chr&gt;    &lt;chr&gt;                       &lt;dbl&gt; &lt;POINT [US_survey_foot]&gt;\n## 1 Gwinnett Mill Creek High School       3997        (2384674 1482772)\n## 2 Gwinnett Collins Hill High School     3720        (2341010 1461730)\n## 3 Gwinnett Brookwood High School        3455        (2334543 1413396)\n## 4 Gwinnett Grayson High School          3230        (2370186 1408579)\n## 5 Gwinnett Peachtree Ridge High School  3118        (2319344 1459458)\n## 6 Gwinnett Berkmar High School          3095        (2312983 1421933)"
  },
  {
    "objectID": "example/12-example.html#making-your-own-geocoded-data",
    "href": "example/12-example.html#making-your-own-geocoded-data",
    "title": "Space",
    "section": "Making your own geocoded data",
    "text": "Making your own geocoded data\nSo, plotting shapefiles with geom_sf() is magical because {sf} deals with all of the projection issues for us automatically and it figures out how to plot all the latitude and longitude data for us automatically. But lots of data doesn’t some as shapefiles. The rats data from mini project 1, for instance, has two columns indicating the latitude and longitude of each rat sighting, but those are stored as just numbers. If we try to use geom_sf() with the rat data, it won’t work. We need that magical geometry column.\nFortunately, if we have latitude and longitude information, we can make our own geometry column.\nLet’s say we want to mark some cities on our map of Georgia. We can make a mini dataset using tribble(). I found these points from Google Maps: right click anywhere in Google Maps, select “What’s here?”, and you’ll see the exact coordinates for that spot.\n\n\nCode\nga_cities &lt;- tribble(\n  ~city, ~lat, ~long,\n  \"Atlanta\", 33.748955, -84.388099,\n  \"Athens\", 33.950794, -83.358884,\n  \"Savannah\", 32.113192, -81.089350\n)\nga_cities\n## # A tibble: 3 × 3\n##   city       lat  long\n##   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n## 1 Atlanta   33.7 -84.4\n## 2 Athens    34.0 -83.4\n## 3 Savannah  32.1 -81.1\n\n\nThis is just a normal dataset, and the lat and long columns are just numbers. R doesn’t know that those are actually geographic coordinates. This is similar to the rats data, or any other data that has columns for latitude and longitude.\nWe can convert those two columns to the magic geometry column with the st_as_sf() function. We have to define two things in the function: which coordinates are the longitude and latitude, and what CRS the coordinates are using. Google Maps uses EPSG:4326, or the GPS system, so we specify that:\n\n\nCode\nga_cities_geometry &lt;- ga_cities %&gt;% \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(\"EPSG:4326\"))\nga_cities_geometry\n## Simple feature collection with 3 features and 1 field\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -84 ymin: 32 xmax: -81 ymax: 34\n## Geodetic CRS:  WGS 84\n## # A tibble: 3 × 2\n##   city        geometry\n## * &lt;chr&gt;    &lt;POINT [°]&gt;\n## 1 Atlanta     (-84 34)\n## 2 Athens      (-83 34)\n## 3 Savannah    (-81 32)\n\n\nThe longitude and latitude columns are gone now, and we have a single magical geometry column. That means we can plot it with geom_sf():\n\n\nCode\nggplot() +\n  geom_sf(data = only_georgia_high, fill = \"#EC8E55\") +\n  geom_sf(data = ga_cities_geometry, size = 3) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nWe can use geom_sf_label() (or geom_sf_text()) to add labels in the correct locations too. It will give you a warning, but you can ignore it—again, it’s complaining that the positioning might not be 100% accurate because of issues related to taking a globe and flattening it. It’s fine.\n\n\nCode\nggplot() +\n  geom_sf(data = only_georgia_high, fill = \"#EC8E55\") +\n  geom_sf(data = ga_cities_geometry, size = 3) +\n  geom_sf_label(data = ga_cities_geometry, aes(label = city),\n                nudge_y = 0.2) +\n  theme_void()"
  },
  {
    "objectID": "example/12-example.html#automatic-geocoding-by-address",
    "href": "example/12-example.html#automatic-geocoding-by-address",
    "title": "Space",
    "section": "Automatic geocoding by address",
    "text": "Automatic geocoding by address\nUsing st_as_sf() is neat when you have latitude and longitude data already, but what if you have a list of addresses or cities instead, with no fancy geographic information? It’s easy enough to right click on Google Maps, but you don’t really want to do that hundreds of times for large-scale data.\nFortunately there are a bunch of different online geocoding services that return GIS data for addresses and locations that you feed them, like magic.\nThe easiest way to use any of these services is to use the {tidygeocoder} package, which connects with all these different free and paid services (run ?geo in R for complete details):\n\n\"osm\": OpenStreetMap through Nominatim. FREE.\n\"census\": US Census. Geographic coverage is limited to the United States. FREE.\n\"arcgis\": ArcGIS. FREE and paid.\n\"geocodio\": Geocodio. Geographic coverage is limited to the United States and Canada. An API key must be stored in \"GEOCODIO_API_KEY\".\n\"iq\": Location IQ. An API key must be stored in \"LOCATIONIQ_API_KEY\".\n\"google\": Google. An API key must be stored in \"GOOGLEGEOCODE_API_KEY\".\n\"opencage\": OpenCage. An API key must be stored in \"OPENCAGE_KEY\".\n\"mapbox\": Mapbox. An API key must be stored in \"MAPBOX_API_KEY\".\n\"here\": HERE. An API key must be stored in \"HERE_API_KEY\".\n\"tomtom\": TomTom. An API key must be stored in \"TOMTOM_API_KEY\".\n\"mapquest\": MapQuest. An API key must be stored in \"MAPQUEST_API_KEY\".\n\"bing\": Bing. An API key must be stored in \"BINGMAPS_API_KEY\".\n\nNot all services work equally well, and the free ones have rate limits (like, don’t try to geocode a million rows of data with the US Census), so you’ll have to play around with different services. You can also provide a list of services and {tidygeocoder} will cascade through them—if it can’t geocode an address with OpenStreetMap, it can move on to the Census, then ArcGIS, and so on. You need to set the cascade_order argument in geocode() for this to work.\nLet’s make a little dataset with some addresses to geocode:\n\n\nCode\nsome_addresses &lt;- tribble(\n  ~name,             ~address,\n  \"The White House\", \"1600 Pennsylvania Ave NW, Washington, DC\",\n  \"The Andrew Young School of Policy Studies\", \"55 Park Place SE, Atlanta, GA 30303\"\n)\n\nsome_addresses\n## # A tibble: 2 × 2\n##   name                                      address                                 \n##   &lt;chr&gt;                                     &lt;chr&gt;                                   \n## 1 The White House                           1600 Pennsylvania Ave NW, Washington, DC\n## 2 The Andrew Young School of Policy Studies 55 Park Place SE, Atlanta, GA 30303\n\n\nTo geocode these addresses, we can feed this data into geocode() and tell it to use the address column. We’ll use the Census geocoding system for fun (surely they know where the White House is):\n\n\nCode\nlibrary(tidygeocoder)\n\ngeocoded_addresses &lt;- some_addresses %&gt;% \n  geocode(address, method = \"census\")\n\ngeocoded_addresses\n\n\n\n## # A tibble: 2 × 3\n##   name                                        lat  long\n##   &lt;chr&gt;                                     &lt;dbl&gt; &lt;dbl&gt;\n## 1 The White House                            38.9 -77.0\n## 2 The Andrew Young School of Policy Studies  33.8 -84.4\n\nIt worked!\nThose are just numbers, though, and not the magical geometry column, so we need to use st_as_sf() to convert them to actual GIS data.\n\n\nCode\naddresses_geometry &lt;- geocoded_addresses %&gt;% \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(\"EPSG:4326\"))\n\naddresses_geometry %&gt;% select(-address)\n## Simple feature collection with 2 features and 1 field\n## Geometry type: POINT\n## Dimension:     XY\n## Bounding box:  xmin: -84 ymin: 34 xmax: -77 ymax: 39\n## Geodetic CRS:  WGS 84\n## # A tibble: 2 × 2\n##   name                                         geometry\n##   &lt;chr&gt;                                     &lt;POINT [°]&gt;\n## 1 The White House                              (-77 39)\n## 2 The Andrew Young School of Policy Studies    (-84 34)\n\n\nLet’s plot these on a US map:\n\n\nCode\nggplot() + \n  geom_sf(data = lower_48, fill = \"#192DA1\", color = \"white\", linewidth = 0.25) +\n  geom_sf(data = addresses_geometry, size = 5, color = \"#FF851B\") +\n  # Albers uses meters as its unit of measurement, so we need to nudge these\n  # labels up by a lot. I only settled on 175,000 here after a bunch of trial\n  # and error, adding single zeroes and rerunning the plot until the labels\n  # moved. 175,000 meters = 108.74 miles\n  geom_sf_label(data = addresses_geometry, aes(label = name),\n                size = 4, fill = \"#FF851B\", nudge_y = 175000) + \n  coord_sf(crs = st_crs(\"ESRI:102003\")) +  # Albers\n  theme_void()"
  },
  {
    "objectID": "example/12-example.html#plotting-other-data-on-maps",
    "href": "example/12-example.html#plotting-other-data-on-maps",
    "title": "Space",
    "section": "Plotting other data on maps",
    "text": "Plotting other data on maps\nSo far we’ve just plotted whatever data the shapefile creators decided to include and publish in their data. But what if you want to visualize some other variable on a map? We can do this by combining our shapefile data with any other kind of data, as long as the two have a shared column. For instance, we can make a choropleth map of life expectancy with data from the World Bank.\nFirst, let’s grab some data from the World Bank for just 2015:\n\n\nCode\nlibrary(WDI)  # For getting data from the World Bank\n\nindicators &lt;- c(life_expectancy = \"SP.DYN.LE00.IN\")  # Life expectancy\n\nwdi_raw &lt;- WDI(country = \"all\", indicators, extra = TRUE, \n               start = 2015, end = 2015)\n\n\nLet’s see what we got:\n\n\nCode\nhead(wdi_raw)\n## # A tibble: 6 × 13\n##   country                     iso2c iso3c  year life_expectancy status lastupdated region                    capital longitude latitude income lending\n##   &lt;chr&gt;                       &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;           &lt;dbl&gt; &lt;lgl&gt;  &lt;date&gt;      &lt;chr&gt;                     &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  \n## 1 Afghanistan                 AF    AFG    2015            62.7 NA     2024-06-28  South Asia                Kabul       69.2      34.5 Low i… IDA    \n## 2 Africa Eastern and Southern ZH    AFE    2015            61.9 NA     2024-06-28  Aggregates                &lt;NA&gt;        NA        NA   Aggre… Aggreg…\n## 3 Africa Western and Central  ZI    AFW    2015            56.2 NA     2024-06-28  Aggregates                &lt;NA&gt;        NA        NA   Aggre… Aggreg…\n## 4 Albania                     AL    ALB    2015            78.6 NA     2024-06-28  Europe & Central Asia     Tirane      19.8      41.3 Upper… IBRD   \n## 5 Algeria                     DZ    DZA    2015            75.6 NA     2024-06-28  Middle East & North Afri… Algiers      3.05     36.7 Lower… IBRD   \n## 6 American Samoa              AS    ASM    2015            NA   NA     2024-06-28  East Asia & Pacific       Pago P…   -171.      -14.3 Upper… Not cl…\n\n\nWe have a bunch of columns here, but we care about two in particular: life expectancy, and the ISO3 code. This three-letter code is a standard system for identifying countries (see the full list here), and that column will let us combine this World Bank data with the global shapefile, which also has a column for the ISO3 code.\n\n\n\n\n\n\nWorld capitals\n\n\n\nWe also have columns for the latitude and longitude for each capital, so we could theoretically convert those to a geometry column with st_as_sf() and plot world capitals, which would be neat, but we won’t do that now.\n\n\nLet’s clean up the WDI data by shrinking it down substantially so we only join the one life_expectancy column that we care about:\n\n\nCode\nwdi_clean_small &lt;- wdi_raw %&gt;% \n  select(life_expectancy, iso3c)\nwdi_clean_small\n## # A tibble: 266 × 2\n##    life_expectancy iso3c\n##              &lt;dbl&gt; &lt;chr&gt;\n##  1            62.7 AFG  \n##  2            61.9 AFE  \n##  3            56.2 AFW  \n##  4            78.6 ALB  \n##  5            75.6 DZA  \n##  6            NA   ASM  \n##  7            NA   AND  \n##  8            60.7 AGO  \n##  9            77.9 ATG  \n## 10            70.8 ARB  \n## # ℹ 256 more rows\n\n\nNext we need to merge this tiny dataset into the world_map_sans_antarctica shapefile data we were using earlier. To do this we’ll use a function named left_join(). We feed two data frames into left_join(), and R will keep all the rows from the first and include all the columns from both the first and the second wherever the two datasets match with one specific column. That’s wordy and weird—stare at this animation here for a few seconds to see what’s really going to happen. We’re essentially going to append the World Bank data to the end of the world shapefiles and line up rows that have matching ISO3 codes. The ISO3 column is named ISO_A3 in the shapefile data, and it’s named iso3c in the WDI data, so we tell left_join() that those are the same column:\n\n\nCode\nworld_map_with_life_expectancy &lt;- world_sans_antarctica %&gt;% \n  left_join(wdi_clean_small, by = c(\"ISO_A3\" = \"iso3c\"))\n\n\nIf you look at this dataset in RStudio now and look at the last column, you’ll see the WDI life expectancy right next to the magic geometry column.\nWe technically didn’t need to shrink the WDI data down to just two columns—had we left everything else, all the WDI columns would have come over to the world_sans_antarctica, including columns for region and income level, etc. But I generally find it easier and cleaner to only merge in the columns I care about instead of making massive datasets with a billion extra columns.\nNow that we have a column for life expectancy, we can map it to the fill aesthetic and fill each country by 2015 life expectancy:\n\n\nCode\nggplot() + \n  geom_sf(data = world_map_with_life_expectancy, \n          aes(fill = life_expectancy),\n          linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"ESRI:54030\")) +  # Robinson\n  scale_fill_viridis_c(option = \"viridis\") +\n  labs(fill = \"Life expectancy\") +\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nVoila! Global life expectancy in 2015!\n\n\n\n\n\n\n\nFrance and Norway\n\n\n\nSharp-eyed readers will notice that France and Norway are grayed out because they’re missing data. That’s because the ISO_A3 code in the Natural Earth data is missing for both France and Norway for whatever reason, so the WDI data didn’t merge with those rows. To fix that, we can do some manual recoding before joining in the WDI data.\n\n\n\n\nCode\nworld_sans_antarctica_fixed &lt;- world_sans_antarctica %&gt;% \n  mutate(ISO_A3 = case_when(\n    # If the country name is Norway or France, redo the ISO3 code\n    ADMIN == \"Norway\" ~ \"NOR\",\n    ADMIN == \"France\" ~ \"FRA\",\n    # Otherwise use the existing ISO3 code\n    TRUE ~ ISO_A3\n  )) %&gt;% \n  left_join(wdi_clean_small, by = c(\"ISO_A3\" = \"iso3c\"))\n\nggplot() + \n  geom_sf(data = world_sans_antarctica_fixed, \n          aes(fill = life_expectancy),\n          linewidth = 0.25) +\n  coord_sf(crs = st_crs(\"ESRI:54030\")) +  # Robinson\n  scale_fill_viridis_c(option = \"viridis\") +\n  labs(fill = \"Life expectancy\") +\n  theme_void() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "example/12-example.html#footnotes",
    "href": "example/12-example.html#footnotes",
    "title": "Space",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is essentially the Gall-Peters projection from the West Wing clip.↩︎"
  },
  {
    "objectID": "example/14-example.html",
    "href": "example/14-example.html",
    "title": "Enhancing graphics",
    "section": "",
    "text": "The content from this week isn’t really code-based at all. Instead, you’re learning about how to take a plot from R and make it fancy in a vector editing program like Adobe Illustrator, Affinity Designer, or Inkscape.\nThis concept comes from a common workflow in the real world, where organizations like news outlets, think tanks, research centers, or nonprofits will publish highly polished plots in annual reports, magazines, and other types of publications. These graphics often have to follow specific in-house style guidelines and use specific colors and fonts and other design elements. Even if you don’t work for a place with in-house style guides, you’ll often want to make some edits to your plots by hand after you create them.\nThe general workflow goes like this:\n\nCreate a plot in R and ggplot\nExport that plot as a vector image (either a PDF or an SVG)\nEdit and enhance the vector image in a vector editor, like Adobe Illustrator\nExport the polished version from Illustrator as either a PDF or PNG (or both)\n\nBig data-focused organizations have been using a process like this for years. Nathan Yau describes this whole process in his 2011 book Visualize This and the book contains a bunch of tutorials to help you learn how create something in R, export it, and edit it in Illustrator.\nFor instance, in his first chapter, he guides you through the process of creating the skeleton of this chart in R, exporting it as a PDF, and adding all the titles and annotations and arrows and extra lines in Illustrator (original post from 2008):"
  },
  {
    "objectID": "example/14-example.html#why-enhance-graphics",
    "href": "example/14-example.html#why-enhance-graphics",
    "title": "Enhancing graphics",
    "section": "",
    "text": "The content from this week isn’t really code-based at all. Instead, you’re learning about how to take a plot from R and make it fancy in a vector editing program like Adobe Illustrator, Affinity Designer, or Inkscape.\nThis concept comes from a common workflow in the real world, where organizations like news outlets, think tanks, research centers, or nonprofits will publish highly polished plots in annual reports, magazines, and other types of publications. These graphics often have to follow specific in-house style guidelines and use specific colors and fonts and other design elements. Even if you don’t work for a place with in-house style guides, you’ll often want to make some edits to your plots by hand after you create them.\nThe general workflow goes like this:\n\nCreate a plot in R and ggplot\nExport that plot as a vector image (either a PDF or an SVG)\nEdit and enhance the vector image in a vector editor, like Adobe Illustrator\nExport the polished version from Illustrator as either a PDF or PNG (or both)\n\nBig data-focused organizations have been using a process like this for years. Nathan Yau describes this whole process in his 2011 book Visualize This and the book contains a bunch of tutorials to help you learn how create something in R, export it, and edit it in Illustrator.\nFor instance, in his first chapter, he guides you through the process of creating the skeleton of this chart in R, exporting it as a PDF, and adding all the titles and annotations and arrows and extra lines in Illustrator (original post from 2008):"
  },
  {
    "objectID": "example/14-example.html#enhancing-graphics-in-2023",
    "href": "example/14-example.html#enhancing-graphics-in-2023",
    "title": "Enhancing graphics",
    "section": "Enhancing graphics in 2023",
    "text": "Enhancing graphics in 2023\nIn 2011, that was the best possible workflow because ggplot couldn’t deal with subtitles, captions, repelled labels, embedded fonts, and differently-styled text (like bold in the middle of a title). Illustrator was the only way to do this stuff.\nNowadays in 2023, though, you can do nearly all of this annotating and enhancing with packages like {ggtext} and {patchwork} and {ggrepel}. You can almost perfectly replicate in-house style guides with the theme() function and put text and arrows and labels and text boxes wherever you want with annotate(). It’s a brave exciting new world.\nYou still can’t do everything with R. ggplot can’t create fancy font ligatures like “ﬁ” in words that have an “f” followed by an “i”, and it can’t handle automatic hyphenation and full text justification, among other limitations. But these are the minorest of graphic design issues (and the ggplot team is working on them!).\nThat all said, it’s still often faster and easier to make edits to your graphs in Illustrator rather than fight with a reluctant annotate() layer that just won’t put an arrow exactly where you want. So it’s still a good idea to understand how to follow the standard workflow of exporting from R and enhancing in Illustrator."
  },
  {
    "objectID": "example/14-example.html#abbreviated-example",
    "href": "example/14-example.html#abbreviated-example",
    "title": "Enhancing graphics",
    "section": "Abbreviated example",
    "text": "Abbreviated example\nIn this video I use the code for the hot dog plot that I provide in this week’s assignment to create a plot, export it, and make edits to it both in Illustrator and Gravit Designer (which actually no longer exists; see here for alternatives to Illustrator). It’s not a complete example at all, but I show you the general process for adding text and lines and editing plot elements."
  },
  {
    "objectID": "example/index.html",
    "href": "example/index.html",
    "title": "Code examples",
    "section": "",
    "text": "This section contains fully annotated R code that you can use as a reference for creating your own visualizations. In the lessons section, you sequentially build up your understanding of R and {ggplot2}; here you can see how all the pieces work together.\nVisit this section after you have finished the readings, lecture videos, and lesson. The examples here will be indispensable for you as you work on your assignments and mini projects.\nEach section also contains videos of me live coding the examples so you can see what it looks like to work with R in real time. You’ll notice me make all sorts of little errors, which is totally normal—everyone does!"
  },
  {
    "objectID": "lesson/01-lesson.html",
    "href": "lesson/01-lesson.html",
    "title": "Introduction to R and the tidyverse",
    "section": "",
    "text": "For the first part of this week’s lesson, you need to work through a few of Posit’s introductory primers. You’ll do these in your browser and type code and see results there.\nYou’ll learn some of the basics of R, as well as some powerful methods for manipulating data with the {dplyr} package.\nComplete these primers. It seems like there are a lot, but they’re short and go fairly quickly (especially as you get the hang of the syntax). Also, I have no way of seeing what you do or what you get wrong or right, and that’s totally fine! If you get stuck and want to skip some (or if it gets too easy), go right ahead and skip them!\n\nThe Basics\n\nVisualization Basics\nProgramming Basics\n\nWork with Data\n\nWorking with Tibbles\nIsolating Data with dplyr\nDeriving Information with dplyr\n\n\nThe content from these primers comes from the (free and online!) book R for Data Science by Garrett Grolemund and Hadley Wickham. I highly recommend the book as a reference and for continuing to learn and use R in the future (like running regression models and other types of statistical analysis)"
  },
  {
    "objectID": "lesson/01-lesson.html#part-1-the-basics-of-r-and-dplyr",
    "href": "lesson/01-lesson.html#part-1-the-basics-of-r-and-dplyr",
    "title": "Introduction to R and the tidyverse",
    "section": "",
    "text": "For the first part of this week’s lesson, you need to work through a few of Posit’s introductory primers. You’ll do these in your browser and type code and see results there.\nYou’ll learn some of the basics of R, as well as some powerful methods for manipulating data with the {dplyr} package.\nComplete these primers. It seems like there are a lot, but they’re short and go fairly quickly (especially as you get the hang of the syntax). Also, I have no way of seeing what you do or what you get wrong or right, and that’s totally fine! If you get stuck and want to skip some (or if it gets too easy), go right ahead and skip them!\n\nThe Basics\n\nVisualization Basics\nProgramming Basics\n\nWork with Data\n\nWorking with Tibbles\nIsolating Data with dplyr\nDeriving Information with dplyr\n\n\nThe content from these primers comes from the (free and online!) book R for Data Science by Garrett Grolemund and Hadley Wickham. I highly recommend the book as a reference and for continuing to learn and use R in the future (like running regression models and other types of statistical analysis)"
  },
  {
    "objectID": "lesson/01-lesson.html#part-2-getting-familiar-with-rstudio",
    "href": "lesson/01-lesson.html#part-2-getting-familiar-with-rstudio",
    "title": "Introduction to R and the tidyverse",
    "section": "Part 2: Getting familiar with RStudio",
    "text": "Part 2: Getting familiar with RStudio\nThe RStudio primers you just worked through are a great introduction to writing and running R code, but you typically won’t type code in a browser when you work with R. Instead, you’ll use a nicer programming environment like RStudio, which lets you type and save code in scripts, run code from those scripts, and see the output of that code, all in the same program.\nTo get familiar with RStudio, watch this video:"
  },
  {
    "objectID": "lesson/01-lesson.html#part-3-rstudio-projects",
    "href": "lesson/01-lesson.html#part-3-rstudio-projects",
    "title": "Introduction to R and the tidyverse",
    "section": "Part 3: RStudio Projects",
    "text": "Part 3: RStudio Projects\nOne of the most powerful and useful aspects of RStudio is its ability to manage projects.\nWhen you first open R, it is “pointed” at some folder on your computer, and anything you do will be relative to that folder. The technical term for this is a “working directory.”\nWhen you first open RStudio, look in the area right at the top of the Console pane to see your current working directory. Most likely you’ll see something cryptic: ~/\n\n\n\nThat tilde sign (~) is a shortcut that stands for your user directory. On Windows this is C:\\Users\\your_user_name\\; on macOS this is /Users/your_user_name/. With the working directory set to ~/, R is “pointed” at that folder, and anything you save will end up in that folder, and R will expect any data that you load to be there too.\nIt’s always best to point R at some other directory. If you don’t use RStudio, you need to manually set the working directory to where you want it with setwd(), and many R scripts in the wild include something like setwd(\"C:\\\\Users\\\\bill\\\\Desktop\\\\Important research project\") at the beginning to change the directory. THIS IS BAD THOUGH (see here for an explanation). If you ever move that directory somewhere else, or run the script on a different computer, or share the project with someone, the path will be wrong and nothing will run and you will be sad.\nThe best way to deal with working directories with RStudio is to use RStudio Projects. These are special files that RStudio creates for you that end in a .Rproj extension. When you open one of these special files, a new RStudio instance will open up and be pointed at the correct directory automatically. If you move the folder later or open it on a different computer, it will work just fine and you will not be sad.\nRead this super short chapter on RStudio projects.\nIn general, you can create a new project by going to File &gt; New Project &gt; New Directory &gt; Empty Project, which will create a new folder on your computer that is empty except for a single .Rproj file. Double click on that file to open an RStudio instance that is pointed at the correct folder."
  },
  {
    "objectID": "lesson/01-lesson.html#part-4-getting-familiar-with-r-markdown",
    "href": "lesson/01-lesson.html#part-4-getting-familiar-with-r-markdown",
    "title": "Introduction to R and the tidyverse",
    "section": "Part 4: Getting familiar with R Markdown",
    "text": "Part 4: Getting familiar with R Markdown\nTo ensure that the analysis and graphics you make are reproducible, you’ll do the majority of your work in this class using R Markdown files.\nDo the following things:\n\nWatch this video:\n\n\n\n\n\n \n\nSkim through the content at these pages:\n\nUsing Markdown\nUsing R Markdown\nHow it Works\nCode Chunks\nInline Code\nMarkdown Basics (The R Markdown Reference Guide is super useful here.)\nOutput Formats\n\nWatch this video:"
  },
  {
    "objectID": "lesson/03-lesson.html",
    "href": "lesson/03-lesson.html",
    "title": "Mapping data to graphics",
    "section": "",
    "text": "For the first part of this week’s lesson, you need to work through RStudio’s introductory primers for {ggplot2}. You’ll do these in your browser and type code and see results there.\nIt seems like there are a lot, but they’re short and go fairly quickly (especially as you get the hang of the ggplot() syntax). Complete these:\n\nVisualize Data\n\nExploratory Data Analysis\nBar Charts\nHistograms\nBoxplots and Counts\nScatterplots\nLine plots\nOverplotting and Big Data\nCustomize Your Plots"
  },
  {
    "objectID": "lesson/03-lesson.html#part-1-data-visualization-with-ggplot2",
    "href": "lesson/03-lesson.html#part-1-data-visualization-with-ggplot2",
    "title": "Mapping data to graphics",
    "section": "",
    "text": "For the first part of this week’s lesson, you need to work through RStudio’s introductory primers for {ggplot2}. You’ll do these in your browser and type code and see results there.\nIt seems like there are a lot, but they’re short and go fairly quickly (especially as you get the hang of the ggplot() syntax). Complete these:\n\nVisualize Data\n\nExploratory Data Analysis\nBar Charts\nHistograms\nBoxplots and Counts\nScatterplots\nLine plots\nOverplotting and Big Data\nCustomize Your Plots"
  },
  {
    "objectID": "lesson/03-lesson.html#part-2-reshaping-data-with-tidyr",
    "href": "lesson/03-lesson.html#part-2-reshaping-data-with-tidyr",
    "title": "Mapping data to graphics",
    "section": "Part 2: Reshaping data with {tidyr}",
    "text": "Part 2: Reshaping data with {tidyr}\nFor the last part of this week’s lesson, you’ll work through just one RStudio primer to learn how to use the {tidyr} package to reshape data from wide to long and back to wide.\nComplete this:\n\nTidy Your Data\n\nReshape Data\n\n\n\n\n\n\n\n\nPivoting\n\n\n\nNewer versions of {tidyr} have renamed these core functions: gather() is now pivot_longer() and spread() is now pivot_wider(). The syntax for these pivot_*() functions is slightly different from what it was in gather() and spread(), so you can’t just replace the names. Fortunately, both gather() and spread() still work and won’t go away for a while, so you can still use them as you learn about reshaping and tidying data. It would be worth learning how the newer pivot_*() functions work, eventually, though (see here for examples)."
  },
  {
    "objectID": "lesson/05-lesson.html#complete-ggplot-themes",
    "href": "lesson/05-lesson.html#complete-ggplot-themes",
    "title": "Themes",
    "section": "Complete ggplot themes",
    "text": "Complete ggplot themes\nThere are many built-in complete themes that have a good combination of all the different theme() options already set for you. By default, ggplot uses theme_gray() (also spelled theme_grey() for UK English; because the first developer of ggplot (Hadley Wickham) is from New Zealand, British spelling works throughout (e.g. you can use colour instead of color))\n\n\n\n\n\n\nYour turn\n\n\n\nAdd theme_minimal() to this plot:\n\n\n\n\n\n\nHopefully that was easy!\nIf you look at the documentation for the different theme functions, you’ll notice that there are a few optional arguments, like base_size and base_family. The base_size argument changes the base font size for the text in the plot, and it is 11 by default. Changing it to something like 20 will not make all the text in the plot be sized at 20—functions like theme_minimal() set the size of plot elements based on the base_size. For instance, in theme_minimal(), the plot title is set to be 120% of base_size, while the caption is 80%. Changing base_size will resize all the different elements accordingly.\n\n\n\n\n\n\nYour turn\n\n\n\nModify this plot to use theme_minimal() with a base size of 16:\n\n\n\n\n\n\nHopefully that was also fairly straightforward!"
  },
  {
    "objectID": "lesson/05-lesson.html#modifying-plot-elements-with-theme",
    "href": "lesson/05-lesson.html#modifying-plot-elements-with-theme",
    "title": "Themes",
    "section": "Modifying plot elements with theme()",
    "text": "Modifying plot elements with theme()\nUsing a complete theme like theme_minimal() or theme_bw() is a great starting point for getting a nice, clean, well designed plot. You’ll often need to make adjustments to smaller, more specific parts of the plot though. To do this, you can use the theme() function.\ntheme() is a massive function and has perhaps the most possible arguments of any function in R. It is impossible to remember everything it can possibly do. Fortunately its documentation is incredible. Run ?theme in your R console to see the help page, or go to this page online.\n\nDeal with general plot elements\nA few arguments to theme() don’t use any special function—you can just specify settings with text like \"bottom\" or \"right\"\n\n\n\n\n\n\nYour turn\n\n\n\nLook at the documentation for theme() online. Make this plot’s legend appear on the bottom instead of the left.\n\n\n\n\n\n\n\n\nDisable elements completely with element_blank()\nAny plot element can be disabled by using element_blank(). For instance, if you want to remove the axis ticks, you can use theme(axis.ticks = element_blank()).\n\n\n\n\n\n\nYour turn\n\n\n\nLook at the documentation for theme() online. Disable the panel grid in this plot.\n\n\n\n\n\n\nYou can also target more specific plot elements. You can specify something like axis.text, which applies to all axis text, or you can use axis.text.y to only target the text on the y-axis.\n\n\n\n\n\n\nYour turn\n\n\n\nLook at the documentation for theme() online. Make the following changes to this plot:\n\nDisable the major panel grid for the x-axis\nDisable the minor panel grid for the x-axis\nDisable the minor panel grid for the y-axis.\n\nYou should only have three horizontal lines for the grid.\n\n\n\n\n\n\n\nAlmost every other plot element fits into one of three categories: a rectangle, a line, or text. Changing the settings on these elements requires specific functions that correspond to these categories.\n\n\nDeal with borders and backgrounds with element_rect()\nThings like the plot background or the panel background or the legend background are rectangles and can be manipulated with element_rect(). If you want the legend box to be yellow with a thin black border, you would use theme(legend.box.background = element_rect(fill = \"yellow\", color = \"black\", linewidth = 1).\n\n\n\n\n\n\nYour turn\n\n\n\nLook at the documentation for theme() and the documentation for element() online. Make the following changes to this plot:\n\nFill the plot background with #F2D8CE\nFill the panel background with #608BA6, and make the border #184759 with linewidth = 5\n\nThis will be a fairly ugly plot.\n\n\n\n\n\n\n\n\nDeal with lines with element_line()\nThings like the panel grid, tick marks, and axis lines are all lines and can be manipulated with element_line(). If you want the x-axis line to be a dotted orange like, you would use theme(axis.line.x = element_line(color = \"orange\", linetype = \"dotted\").\n\n\n\n\n\n\nYour turn\n\n\n\nLook at the documentation for theme() and the documentation for element() online. Make the following changes to this plot:\n\nMake the major panel gridlines blue and dashed with linewidth = 1\n\nThis will also be a fairly ugly plot.\n\n\n\n\n\n\n\n\nDeal with text with element_text()\nFinally, anything with text can be manipulated with element_text(), and you can specify all sorts of things, including font family (family), font weight (face), color (color), horizontal justification (hjust), angle (angle), and a bunch of other options. If you want the x-axis text to be italicized and rotated at a 45º angle, you would use theme(axis.text.x = element_text(face = \"italic\", angle = 45)).\n\n\n\n\n\n\nYour turn\n\n\n\nLook at the documentation for theme() and the documentation for element() online. Make the following changes to this plot:\n\nMake the y-axis text italic\nMake the plot title right aligned, bold, and colored with #8C7811\nMake the plot subtitle right aligned"
  },
  {
    "objectID": "lesson/05-lesson.html#important-note-about-ordering",
    "href": "lesson/05-lesson.html#important-note-about-ordering",
    "title": "Themes",
    "section": "Important note about ordering",
    "text": "Important note about ordering\nThings like theme_grey() or theme_minimal() are really just collections of changes to theme(), so the order is important when using a complete theme. If you do something like this to turn off the gridlines in the plot panel:\n\nggplot(...) +\n  geom_point(...) +\n  theme(panel.grid = element_blank()) +\n  theme_bw()\n\n…you’ll still have panel gridlines! That’s because theme_bw() turns them on, and you typed it after you turned it off. If you want to use both theme_bw() and remove the gridlines, you need to make sure any theme adjustments come after theme_bw():\n\nggplot(...) +\n  geom_point(...) +\n  theme_bw() +\n  theme(panel.grid = element_blank())"
  },
  {
    "objectID": "lesson/05-lesson.html#fonts",
    "href": "lesson/05-lesson.html#fonts",
    "title": "Themes",
    "section": "Fonts",
    "text": "Fonts\nYou can use theme() to change the fonts as well, though sometimes it’s a little tricky to get R to see the fonts on your computer—especially if you use Windows. This detailed blog post explains how to work with custom fonts in ggplot and shows how to get it set up on Windows. It should Just Work™ on macOS.\nIn short, as long as you load the fonts correctly, you can specify different fonts either in a complete theme like theme_minimal(base_family = \"Comic Sans MS\") or in theme() like theme(plot.title = element_text(family = \"Papyrus\"))."
  },
  {
    "objectID": "lesson/05-lesson.html#reusing-themes",
    "href": "lesson/05-lesson.html#reusing-themes",
    "title": "Themes",
    "section": "Reusing themes",
    "text": "Reusing themes\nIf you want to repeat specific theme settings throughout a document, you can save yourself a ton of typing by storing the results of theme() to an object and reusing it. For instance, suppose you want your plots to be based on theme_minimal, have right aligned title and subtitle text, have the legend at the bottom, and have no minor gridlines. You can save all of that into an object named my_neato_theme or something, and then reuse it:\n\nmy_neato_theme &lt;- theme_minimal() +\n  theme(plot.title = element_text(hjust = 1),\n        plot.subtitle = element_text(hjust = 1),\n        legend.position = \"bottom\",\n        panel.grid.minor = element_blank())\n\n# Make one plot\nggplot(data = mpg,\n       mapping = aes(x = displ, y = hwy, color = drv)) +\n  geom_point(size = 3) +\n  labs(title = \"Engine displacement and highway MPG\",\n       subtitle = \"Heavier cars get worse mileage\") +\n  my_neato_theme\n\n\n\n\n\n\n\n\n# Make another plot\nggplot(data = mpg,\n       mapping = aes(x = displ, y = hwy, color = cty)) +\n  geom_point(size = 3) +\n  labs(title = \"Engine displacement and highway MPG\",\n       subtitle = \"Points colored by city MPG\") +\n  my_neato_theme"
  },
  {
    "objectID": "lesson/05-lesson.html#saving-plots",
    "href": "lesson/05-lesson.html#saving-plots",
    "title": "Themes",
    "section": "Saving plots",
    "text": "Saving plots\nSo far, all your plots have ended up either in RStudio or in a knitted HTML, Word, or PDF document. But what if you want to save just the plot to your computer so you can send it out to the world?! You could take a screenshot, but that won’t provide the highest resolution, and that will only save the plot as a bitmap-based PNG, not an infinitely resizable vector-based PDF!\nFortunately it’s pretty easy to save a plot using the special ggsave() function. You can specify whatever dimensions you want and whatever file type you want and save the standalone plot to your computer. You should look at the documentation for ggsave() for complete details of all the different options and arguments it can take. Typically, you do something like this.\nFirst create a plot and store it as an object. We haven’t done that yet in this lesson—so far we’ve just run ggplot() and seen the output immediately. If you save the output of ggplot() to an object, you actually won’t see anything until you run the name of the object.\n\na_cool_plot &lt;- ggplot(data = mpg,\n                      mapping = aes(x = displ, y = hwy, color = drv)) +\n  geom_point(size = 3) +\n  labs(title = \"Engine displacement and highway MPG\",\n       subtitle = \"Heavier cars get worse mileage\")\n\n# Make sure you run this so you can see the plot\na_cool_plot\n\n\n\n\n\n\n\n\nNext you can feed your saved plot to ggsave() to save it. It will automatically determine how to save it based on the filename you provide. If you tell it to be something.png, R will make a PNG; if you tell it to be something.pdf, R will make a PDF, and so on. Common types are PDF, PNG, JPEG (ew though), SVG, TIFF, and others.\nYou can also save the plot as multiple files. I typically make PNG and PDF versions of any plots I export like so:\n\nggsave(filename = \"a_cool_plot.pdf\", plot = a_cool_plot,\n       width = 6, height = 4.5, units = \"in\")\n\nggsave(filename = \"a_cool_plot.png\", plot = a_cool_plot,\n       width = 6, height = 4.5, units = \"in\")\n\nFrom a file management perspective, it often makes sense to store all your output in a separate folder in your project, like output or figures or something. If you want to put saved images in a subfolder, include the name in the file name:\n\nggsave(filename = \"figures/a_cool_plot.png\", plot = a_cool_plot,\n       width = 6, height = 4.5, units = \"in\")\n\nAnd finally, if you’re using custom fonts, you need to add one bit of wizardry to get the fonts to embed correctly in PDFs. This is something you just have to memorize or copy and paste a lot—if you want to know the full details, see this blog post. In short, R’s default PDF writer doesn’t know how to embed fonts and will panic if you make it try. R can use a different PDF-writing engine named Cairo that embeds fonts just fine, though, so you need to tell ggsave() to use it:\n\nggsave(filename = \"figures/a_cool_plot.pdf\", plot = a_cool_plot,\n       width = 6, height = 4.5, units = \"in\", device = cairo_pdf)"
  },
  {
    "objectID": "lesson/07-lesson.html",
    "href": "lesson/07-lesson.html",
    "title": "Relationships",
    "section": "",
    "text": "There isn’t really a lesson for this week, and as we get further into the semester, the need for lessons will continue to decrease. Now that each section is focused on a few specific geoms and how to apply them, you don’t need to go through interactive tutorials so much, since you should (hopefully!) be getting the hang of how ggplot works. (IF NOT, please reach out for help on Slack or via e-mail! I’m more than happy and ready to help!)\nFor the lesson, read through the code examples in the example to see how to make dual y-axes, scatterplot matrices, coefficient plots, and marginal effects plots."
  },
  {
    "objectID": "lesson/09-lesson.html",
    "href": "lesson/09-lesson.html",
    "title": "Annotations",
    "section": "",
    "text": "Ha, so in the video I said there would be interactive lessons, but I changed my mind! You’re only working with a few new functions this session (annotate(), geom_text(), geom_label(), geom_text_repel(), and geom_label_repel()), and the best way to figure out how to use them is to use them!\nThere are some helpful blog posts and other resources online with examples and explanations. Read through these in addition to the documentation for annotate(), geom_text()/geom_label() and {ggrepel}:\n\n “Add shapes with annotate()”\n “Annotations”"
  },
  {
    "objectID": "lesson/11-lesson.html",
    "href": "lesson/11-lesson.html",
    "title": "Time",
    "section": "",
    "text": "Once again, there’s no lesson this time. You’re all understanding the basics of R and {ggplot2} and {dplyr} really well (I’m seriously so impressed and proud of you all!).\nIn your exercise this week you’ll visualize trends in time using one of three different real-world datasets. In the example I demonstrate how to remove seasonality from time series data, which is a useful skill, but not always applicable to every time series dataset. If there’s no seasonality in your data, you don’t need to remove it.\nSo head over to the example or the exercise to get started!"
  },
  {
    "objectID": "lesson/13-lesson.html",
    "href": "lesson/13-lesson.html",
    "title": "Text",
    "section": "",
    "text": "There’s no lesson for this session. In your exercise this week you’ll visualize text data using {tidytext}, and the best way to figure that out is to just play with data.\nSo head over to the example to see how it’s done, or the exercise to get started!"
  },
  {
    "objectID": "lesson/15-lesson.html",
    "href": "lesson/15-lesson.html",
    "title": "Truth, beauty, and data revisited",
    "section": "",
    "text": "There’s no lesson for this session. You made it to the end of the course! Congratulations!"
  },
  {
    "objectID": "news/2023-08-18_announcements-updates.html",
    "href": "news/2023-08-18_announcements-updates.html",
    "title": "Announcements and updates",
    "section": "",
    "text": "← News\n\n\n\nHaha one last thing for real!\nOver the course of the semester, I’ll be sending you lots of updates and announcements and FAQs.\nMy philosophy for this class is to let you maintain access to as much content as possible, which is why so little of the the class material is on iCollege.\nI will post all these announcements and e-mails at the “News” section of the course website. This means that even after you finish the class and lose access to iCollege, and even after you graduate from GSU and lose access to your gsu.edu e-mail address, you’ll still have a searchable archive of messages and FAQs from this class.\nI’ll let you know when there’s a new item in the news section in a few different ways:\n\nI’ll e-mail you a link to the newest message. I’ll use the e-mail address you have listed in PAWS—if you want me to use a different address, let me know and I’ll update my list\nNew messages should post automatically to the #general channel in Slack\nIf you use an RSS feed reader (RIP Google Reader), you can subscribe to an RSS feed of the news section. There’s a link at the top of the news page.\n\nPhew. That should be it from me for a while. Remember to read the syllabus and the rest of the course website, answer the welcome survey, sign up for Slack, and join the Posit.cloud workspace. Also, remember to take care of yourselves! I mean every word in the “Learning during a pandemic” section of the syllabus. This all sucks so much, and I will try my hardest to accommodate your needs throughout the summer\nThanks!"
  },
  {
    "objectID": "news/2023-08-18_welcome.html",
    "href": "news/2023-08-18_welcome.html",
    "title": "Welcome to class!",
    "section": "",
    "text": "← News\n\n\n\nHello everyone!\nI’m Andrew Heiss, your professor for PMAP 8551/4551 (Data visualization with R) this fall, and I’m so excited for the class! Out of all the different classes I teach, this is probably my favorite. It’s so satisfying to run a few lines of code and see beautiful data-based graphics.\nI have a few important announcements before class:\n\nBefore class, I’d love to get to know each you a little first, so I’ve created a quick survey to fill out. I’ve sent you a link to it via e-mail. Please take it at your earliest convenience.\nThe entire course is available at a special class website at https://datavizf23.classes.andrewheiss.com/ (surprise—you’re at it right now). Bookmark this site—it’ll be your best friend for the next few months. I only use iCollege for collecting your assignments and posting answer keys (since it’s password protected). This website is the official source of dates and all other class information. Because it’s not part of iCollege, you’ll be able to reference it even after you graduate and lose access to GSU resources. You can even share it with others—it’s just a website!\nPlease read the main explanatory pages at your earliest convenience. The instructions and expectations for the class are divided across different pages, all accessible from the menu bar at the top of the site. Please read the main pages for the syllabus, content, lessons, examples, and assignments.\nAs you’ll see on the syllabus and schedule pages, the class is divided into 15 sessions (corresponding to a typical 15-week semester). Since this is a summer class, we’re covering two sessions a week. You’ll follow this general process for each week:\n\nGo to the content page, read the readings, and watch the slides/lectures\nGo to the lesson page and do the lessons\nDo the assignment, referring to the example page as you go\n\nIf you look at the syllabus page, you’ll notice that there are no physical textbooks for the class. Everything is free. Neat.\n\nQuick background about me: I’m an assistant professor here at the Andrew Young School, where I’ve been teaching MPA/MPP microeconomics, program evaluation, and data visualization. I moved here from Utah in Fall 2019, where I was a visiting professor of public management at the Marriott School of Business at Brigham Young University (BYU). While at BYU, I taught microeconomics, statistics, and data visualization (basically the same stuff I’m teaching now). I finished my PhD in public policy and political science from Duke in 2017, and before then I finished my MPA in nonprofit management from BYU in 2012. When I’m not teaching, I research international nonprofits and human rights.\nI’m also a graphic designer (mostly print; sometimes web). I minored in design as an undergrad and worked remotely for an academic press at BYU throughout my PhD (yay side hustles), where I typeset a couple dozen academic books and dozens of articles. I still do design work now (though not as much) and am the main typesetter for a small nonprofit press named BCC Press. The fun thing about my design background is that it spills over into my research, statistical work, and teaching—it’s in part why I created this class!\nWhen I’m not teaching or researching, I’m normally chilling at home with my 6 kids (see https://www.heissatopia.com for photos and hilarious stories).\nAgain, I’m really excited to get started next week. This summer should be a blast!"
  },
  {
    "objectID": "news/2023-08-21_copy-paste-tweak.html",
    "href": "news/2023-08-21_copy-paste-tweak.html",
    "title": "Copy, paste, and tweak",
    "section": "",
    "text": "← News\n\n\n\nLearning programming is a little different from learning other skills or materials. In other classes you’ve taken, you read articles or textbooks, take notes on them, talk about the readings in class, maybe complete problem sets or exercises about the materials, and then you take a test.\nWith programming, though, the approach is typically different. You read things, you copy code, you paste that code into a file on your computer, and you tinker with the code to see how it works and what changes as you change things.\nThe R community in particular has embraced this approach to learning how to code. Posit, the company that develops RStudio, has a whole team of professional educators dedicated to improving R pedagogy, and they use this approach in all their teaching materials. The Software Carpentry project (a group dedicated to teaching data and programming skills) does too. I’m a fan of it as well. As one of the textbooks I teach from in another class states:\n\nTake the “copy, paste, and tweak” approach: Especially when you learn your first programming language or you need to understand particularly complicated code, it is often much easier to take existing code that you know works and modify it to suit your ends. This is as opposed to trying to type out the code from scratch. We call this the “copy, paste, and tweak” approach. So early on, we suggest not trying to write code from memory, but rather take existing examples we have provided you, then copy, paste, and tweak them to suit your goals. After you start feeling more confident, you can slowly move away from this approach and write code from scratch. Think of the “copy, paste, and tweak” approach as training wheels for a child learning to ride a bike. After getting comfortable, they won’t need them anymore.\n\nTo make your copying/pasting life easier, each session has an extensive example page and an accompanying video showing how to create the different visualizations. Copy/paste/tweak liberally from the example pages!\nEvery chunk of code on the course website has a neat little  clipboard icon that you can use to copy the whole chunk:\n\n# Click on the clipboard icon in the top right corner → to copy this code\nx &lt;- 1:5\nx + 5\n## [1]  6  7  8  9 10\n\nUse that button!\nInevitably in the classes where I teach R, I have students say things like “I’m trying to do this without looking at any documentation!” or “I can’t do this without googling—I’m a failure!”. While the effort to be fully independent and perfect with code is noble, it’s totally unnecessary. Everyone looks stuff up all the time—being able to do something without looking stuff up shouldn’t be your end goal.\nEventually you’ll be able to whip out basic ggplot(..., aes(...)) + geom_point() + geom_smooth() + labs() kinds of things without copying and pasting—that comes naturally over time, and you see me do that in the videos. But as soon as I have to start changing axis breaks or do anything beyond the standard stuff, I immediately start googling and looking at the documentation. Everyone does it. The authors of these packages do it. That’s why documentation exists. So don’t feel bad if you do it too. It’s the dirty little secret of all programming everywhere—it’s impossible to write code without referring to documentation or other people’s code (or your past code).\nThis is why I have you do stuff in R Markdown. Everything in your problem sets is completely reproducible. The document tells you exactly what you ran to get the data and results and figures that you made. Copy and paste from your past assignments liberally. Copy and paste from the examples liberally.\nDo not be super noble and force yourself to write everything from scratch!\nFinally, if you ever get frustrated when running code or making a plot because it goes wild and vomits geoms all over the page, don’t worry! It happens to everyone! There’s actually a whole hashtag on Twitter for #accidentalaRt (and a corresponding @accidental__aRt Twitter account that collects these plots) where people post their plotting disasters. Scroll through the feed and see the marvelous accidental art people make. It’s fantastic."
  },
  {
    "objectID": "news/2023-08-21_first-day-of-class.html",
    "href": "news/2023-08-21_first-day-of-class.html",
    "title": "First day of class!",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nToday’s the first day of class, and I’m excited to get started!\nA few quick reminders:\n\nIf you haven’t yet, please visit the class website at https://datavizf23.classes.andrewheiss.com/. This is where all the class materials live and it’s the official source of dates and all other class information. I only use iCollege for collecting your assignments and posting answer keys (since it’s password protected). Because it’s not part of iCollege, you’ll be able to reference it even after you graduate and lose access to GSU resources. You can even share it with others—it’s just a website! (In the past I’ve had students spend like an hour downloading all the content from the class website so they can keep it; don’t worry about that though—it’ll all stay there indefinitely!)\nAs you’ll see on the syllabus and schedule pages, the class is divided into 15 sessions (corresponding to a typical 15-week semester). Since this is a summer class, we’re covering two sessions a week. You’ll follow this general process for each session:\n\nGo to the content page, read the readings, and watch the slides/lectures\nGo to the lesson page and do the lessons\nDo the assignment, referring to the example page as you go\n\nAs you’ll read here, we’re using R and RStudio in this class. You can use a neat service called Posit.cloud to get started (join the class workspace here using the link I sent via e-mail), but eventually you’ll want to have R and RStudio on your computer. Follow the instructions here to do that\nIf you haven’t already, join the class Slack workspace at using the link I sent via e-mail. This is the best and fastest place to get help and talk to me and your classmates during the semester. I’ll also post general tips and tricks and advice there, so check it regularly.\nIn addition to the lessons, examples, and assignments, I’ve included a bunch of extra resources on the class website - check out those different pages for extra links to things about graphic design, data visualization, R, R best practices, data, and lots of other things.\n\nCool cool cool, let’s go!"
  },
  {
    "objectID": "news/2023-08-30_faqs_week-01.html",
    "href": "news/2023-08-30_faqs_week-01.html",
    "title": "Week 1 FAQs",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nI just finished grading everything you’ve turned in so far and you’re doing great work! I’ve also been happy to see activity on Slack. Please keep commenting and discussing stuff there!\nI have just a few quick FAQs and tips I’ve been giving as feedback to many of you:\n\nSlack in public\nOne reminder regarding Slack: lots of you have been sending me private messages there for assistance, and that’s great and I’m happy to respond like that. However, one way to help build a stronger community for this class is to ask questions in public in the various #help channels instead. There are a couple reasons for this:\n\nIt reduces duplication: Many of you have had almost identical questions and I’ve been able to copy/paste screenshots between different private conversations asking for help. Having those questions and answers in #help instead will let you get answers to common questions faster\nIt allows you to help: Some of you have R experience already, and even if you don’t, as the summer goes on, you’ll get more comfortable with it and will start being able to answer your classmates’ questions. You might have just fixed a similar issue in a past exercise, or you might be able to spot a typo in their code, or you might otherwise know how to help. Step in and help! Slack is for building a community, not just for getting assistance from me.\n\n\n\nLoad libraries and data just once\nWhen working with the Posit Primers, you ran code in little text boxes in your browser and often reloaded libraries and data in each of the little text boxes, like this to make a scatterplot:\n\nlibrary(tidyverse)\nlibrary(gampinder)\n\ngapminder_2007 &lt;- gapminder %&gt;% \n  filter(year == 2007)\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point()\n\nOr this to make a histogram:\n\nlibrary(tidyverse)\nlibrary(gampinder)\n\ngapminder_2007 &lt;- gapminder %&gt;% \n  filter(year == 2007)\n\nggplot(gapminder_2007, aes(y = lifeExp)) +\n  geom_histogram()\n\nWhen you’re working in a document in RStudio, though, you do not and should not need to keep reloading the different libraries and datasets. Instead, do it once at the beginning of your document—all the chunks in the rest of the document will have access to the libraries and datasets that you’ve loaded.\n\nDo thisDon’t do this\n\n\nHere I'll load the data first\n\n```{r}\nlibrary(tidyverse)\nlibrary(gampinder)\n\ngapminder_2007 &lt;- gapminder %&gt;% \n  filter(year == 2007)\n```\n\nHere's a scatterplot: \n\n```{r}\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point()\n```\n\nAnd here's a histogram:\n\n```{r}\nggplot(gapminder_2007, aes(y = lifeExp)) +\n  geom_histogram()\n```\n\n\nHere's a scatterplot: \n\n```{r}\nlibrary(tidyverse)\nlibrary(gampinder)\n\ngapminder_2007 &lt;- gapminder %&gt;% \n  filter(year == 2007)\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point()\n```\n\nAnd here's a histogram:\n\n```{r}\nlibrary(tidyverse)\nlibrary(gampinder)\n\ngapminder_2007 &lt;- gapminder %&gt;% \n  filter(year == 2007)\n\nggplot(gapminder_2007, aes(y = lifeExp)) +\n  geom_histogram()\n```\n\n\n\n\n\nKnitted document format\nR Markdown is great and wonderful in part because you can write a document in one .Rmd file and have it magically turn into an HTML file, a Word file, or a PDF (or if you want to get extra fancy later, a slideshow, a dashboard, or even a full website).\niCollege doesn’t like HTML files, though. It won’t show images that get uploaded because of weird server restrictions or something. So when you submit your exercises, make sure you knit to PDF or Word:\n\n\n\nKnit to either Word or PDF when submitting to iCollege\n\n\nI’d recommend knitting to HTML often as you work on the exercise. Knitting to PDF takes a few extra seconds, and knitting to Word is a hassle because Word gets mad if you have a previous version of the document open when knitting. HTML is pretty instantaneous. When I work on R Markdown files, I put a browser window on one of my monitors and RStudio on the other and knit and reknit often to HTML while working. Once I’m done and the document all works and the images, tables, text, etc. are all working, I’ll knit to PDF or Word or whatever final format I want.\n\n\nColumn names and the hyperliterality of computers\nComputers are incredibly literal and they cannot read your mind! As humans we’re good at figuring out information when data is missing or distorted—if someone sends you a text saying that they’re “running 5 minutes latte”, you know that they’re not running with some weird time-based coffee but are instead running late. Computers can’t figure that out.\nFor example, in Exercise 1 if you want to make a plot showing the relationship between highway MPG and cylinders, if you type anything like this, it won’t work.\nHere there’s an error because there are no columns named cylinders or highway:\n\nggplot(data = cars, mapping = aes(x = cylinders, y = highway)) +\n  geom_point()\n## Error in `geom_point()`:\n## ! Problem while computing aesthetics.\n## ℹ Error occurred in the 1st layer.\n## Caused by error in `FUN()`:\n## ! object 'cylinders' not found\n\nHere there’s an error because there is no column named Cyl. It’s cyl with a lowercase c. R is case sensitive.\n\nggplot(data = cars, mapping = aes(x = Cyl, y = hwy)) +\n  geom_point()\n## Error in `geom_point()`:\n## ! Problem while computing aesthetics.\n## ℹ Error occurred in the 1st layer.\n## Caused by error in `FUN()`:\n## ! object 'Cyl' not found\n\nThe easiest way to make sure you’re using the right column names is to look at them. RStudio gives you a couple easy ways to do this. After you’ve loaded your data, you should see it in your environment panel:\n\n\n\nThe cars dataset in the environment panel\n\n\nIf you click on the little blue arrow next to the name of the dataset, RStudio will show you the column names and the first few values in each:\n\n\n\nColumn names in the cars dataset\n\n\nIf you click on the name of the dataset (cars here), RStudio will open a new tab with a read-only view of the data. You can click on the column names to sort and filter things too, which is convenient.\n\n\n\nThe cars dataset in a viewer tab\n\n\nI constantly have to refer back to the list of column names to make sure I’m spelling things correctly and using the right capitalization.\n\n\nThe magic of labs()\nA few years ago, if you wanted to add labels to a plot with ggplot, you had to use specific functions, like xlab() and ylab() for the axis labels and ggtitle() for the plot title, like this:\n\nlibrary(tidyverse)\n\n# The old way; don't do this!\nggplot(mpg, aes(x = cyl, y = displ, color = drv)) +\n  geom_point() +\n  xlab(\"Cylinders\") +\n  ylab(\"Displacement\") +\n  ggtitle(\"A plot showing stuff\")\n\n\n\n\n\n\n\n\nLots of older code examples on the internet will still use these functions. However, those have all been replaced with a newer, nicer labs() function that lets you control all the different plot labels in one layer. Here’s how to make that same plot above with labs() instead:\n\n# The better way; do this!\nggplot(mpg, aes(x = cyl, y = displ, color = drv)) +\n  geom_point() +\n  labs(\n    x = \"Cylinders\",\n    y = \"Displacement\",\n    title = \"A plot showing stuff\"\n  )\n\n\n\n\n\n\n\n\nThe labs() function also lets you do subtitles and captions and lets you control the legend titles too. Here we mapped drv to the color aesthetic, so we can change its title by using color in labs():\n\n# The better way; do this!\nggplot(mpg, aes(x = cyl, y = displ, color = drv)) +\n  geom_point() +\n  labs(\n    x = \"Cylinders\",\n    y = \"Displacement\",\n    title = \"A plot showing stuff\",\n    subtitle = \"Super neat\",\n    caption = \"I still have no idea what displacement is\",\n    color = \"Drive\"\n  )"
  },
  {
    "objectID": "news/2023-09-05_zeroes-gradebook.html",
    "href": "news/2023-09-05_zeroes-gradebook.html",
    "title": "Zeroes in the gradebook",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nOne quick note about your grades on iCollege. A few of you have noticed that you now have 0s for assignments that you haven’t turned in yet. Don’t worry! I just had iCollege update all the missing grades to 0 for the sake of more accurate grade calculation. Before today, if you checked your grades on iCollege, you’d be failing even if you turned in everything and got 100% on everything. That’s because the denominator was 650 points—the total number of points you can get this semester. But that’s inaccurate—we’ve only done exercises 1 and 2, so if you got the maximum score on those, you’d have a 40/650, or a 6% F. That’s not useful information! There’s no use including those in the denominator for now.\nSo now you should be able to see what your current grade is with the correct denominator (i.e. the points for everything from the beginning of the summer up to today, which is just 40 points), meaning the percentages you see are more accurate.\nIf you have a 0, don’t worry! Once you turn in past exercises the 0 will get replaced with the actual score. My not-caring-about-late-work policy still stands. The 0 is just a mathematical quirk to get iCollege to cooperate."
  },
  {
    "objectID": "news/2023-09-12_faqs_week-03.html",
    "href": "news/2023-09-12_faqs_week-03.html",
    "title": "Week 3 FAQs",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nMany of you had similar questions and I left lots of similar comments and tips on iCollege, so I’ve compiled the most common issues here. There are a bunch here, but they’re hopefully all useful.\n\nWhy are my axis labels all crowded and on top of each other? How do I fix that?\nThis was a common problem with the LOTR data and it will be a problem with exercise 4 (and future assignments too)—categories on the x-axis often overlap when you knit your document. That’s because there’s not enough room to fit them all comfortably in a standard image size. Fortunately there are a few quick and easy ways to fix this, such as changing the width of the image (see below), rotating the labels, dodging the labels, or (my favorite!) automatically adding line breaks to the labels so they don’t overlap. This blog post (by me) has super quick examples of all these different (easy!) approaches.\n\n\nHow can I add a line break to the text in my plot?\nIf you don’t want to use the fancier techniques from the blog post about long labels, a quick and easy way to deal with longer text is to manually insert a linebreak yourself. This is super easy: include a \\n where you want a new line:\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  labs(\n    title = \"Everyone's favorite\\nexample plot\",\n    x = \"Displacement\\n(proxy for car weight)\",\n    y = \"MPG\\n(highway)\",\n    color = \"Drive\"\n  )\n\n\n\n\n\n\n\n\n\n\nHow can I control the dimensions of my figures?\nBy default, R creates plots that are 7″×7″ squares:\n\nlibrary(tidyverse)\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +\n  geom_point()\n\n\n\n\n\n\n\n\nOften, though, those plots are excessively large and can result in text that is too small and dimensions that feel off. You generally want to have better control over the dimensions of the figures you make. For instance, you can make them landscape when there’s lots of text involved. To do this, you can use the fig.width and fig.height chunk options to control the, um, width and height of the figures:\n\n```{r landscape-plot, fig.width=5, fig.height=3}\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +\n  geom_point()\n```\n\n\n\n\n\n\n\n\nThe dimensions are also reflected in RStudio itself when you’re working with inline images, so it’s easy to tinker with different values and rerun the chunk without needing to re-knit the whole document over and over again:\n\n\n\nA 3″×5″ image in RStudio\n\n\n\nBonus artsy mathy fun: Golden rectangles and golden ratios\nBecause I’m a super nerd, I try to make the dimensions of all my landscape images be golden rectangles, which follow the golden ratio—a really amazing ancient number that gets used all the time in art and design. Check out this neat video or this one to learn more.\nBasically, a golden rectangle is a special rectangle where if you cut it at a specific point, you get a square and a smaller rectangle that is also a golden rectangle. You can then cut that smaller rectangle at the magic point and get another square and another even smaller golden rectangle, and so on.\nMore formally and mathematically, it’s a rectangle where the ratio of the height and width of the subshapes are special values. Note how here the blue square is a perfect square with side lengths a, while the red rectangle is another smaller golden rectangle with side lengths a and b:\n\\[\n\\frac{a + b}{a} = \\frac{a}{b} = \\phi\n\\]\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n## font family not found in Windows font database\n\n\n\n\n\n\n\n\nIt turns out that if you do the algebra to figure out that ratio or \\(\\phi\\) (the Greek letter “phi,” pronounced as either “fee” or “fie”), it’s this:\n\\[\n\\phi = \\frac{1 + \\sqrt{5}}{2} \\approx 1.618\n\\]\nThat’s all really mathy, but it’s really just a matter of using that 1.618 number with whatever dimensions you want. For instance, if I want my image to be 6 inches wide, I’ll divide it by \\(\\phi\\) or 1.618 (or multiply it by 0.618, which is the same thing) to find the height to make a golden rectangle: 6 inches × 0.618 = 3.708 = 3.7 inches\nR can even do the math for you in the chunk options:\n\n```{r landscape-plot-golden, fig.width=6, fig.height=(6 / 1.618)}\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +\n  geom_point()\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShortcut\n\n\n\nIf you can’t remember that the magic golden ratio \\(\\phi\\) is 1.618 or the gross complicated \\(\\frac{1 + \\sqrt{5}}{2}\\), you can cheat a little and remember \\(\\frac{5}{3}\\), which is 1.667, which is often close enough.\n\n\nI don’t do this with all my figures, and I often have to fudge the numbers a bit when there are titles and subtitles (i.e. making the height a little taller so that the rectangle around just the plot area still roughly follows the golden ratio), but it makes nice rectangles and I just think they’re neat.\nFor bonus fun, if you draw a curve between the opposite corners of each square of the golden rectangles, you get something called the golden spiral or Fibonacci spiral, which is replicated throughout nature and art. Graphic designers and artists often make the dimensions of their work fit in golden rectangles and will sometimes even overlay a golden spiral over their work and lay out text and images in specific squares and rectangles. See this and this for some examples.\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s the difference between geom_bar() and geom_col()?\nIn exercise 3, you made lots of bar charts to show the counts of words spoken in The Lord of the Rings movies. To do this, you used geom_col() to add columns to the plots. However, confusingly ggplot has another geom layer named geom_bar(), which you’d understandably think you could use to make a bar chart. If you tried using it, though, it probably didn’t work.\nBoth geom_col() and geom_bar() make bar graphs, but there’s a subtle difference between the two: with geom_col(), you have to specify both an x and a y aesthetic; with geom_bar(), you only specify an x aesthetic and ggplot automatically figures out the y for you.\nHere’s a quick example using the mpg data. Let’s say you want to make a plot that shows the count of cars with different drives (front, rear, and four). With geom_col(), you’re in charge of calculating those totals first before plotting, typically with group_by() %&gt;% summarize():\n\n# Get a count of cars by drive\ncars_by_drive &lt;- mpg %&gt;% \n  group_by(drv) %&gt;% \n  summarize(total = n())\n\n# Specify both x and y\nggplot(cars_by_drive, aes(x = drv, y = total)) +\n  geom_col()\n\n\n\n\n\n\n\n\nYou can make that same plot with geom_bar() instead and let ggplot handle the counting:\n\n# Use the full dataset and only specify x, not y\nggplot(mpg, aes(x = drv)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nIt seems like you’d always want to use geom_bar() since the code is so much shorter and you can outsource a lot of the work to ggplot—there’s no need to use group_by() and summarize() and do extra calculations! But that’s not necessarily the case!\nPersonally, I prefer to use geom_col() and do my own calculations anyway because it gives me more control over what is getting calculated. For instance, if I want to plot percentages instead of counts, it’s far easier to do that in a separate dataset than somehow hack geom_bar() into showing percents. Or if I want to group by multiple things, it’s easier to do that with group_by() instead of tricking geom_bar() into getting it right. Plus I can look at the intermediate cars_by_drive data before plotting to make sure everything was calculated correctly.\nFor instance, if I want to find the proportion of car drives across the two different years in the dataset, it’s a lot easier to create my own y variable with group_by() %&gt;% summarize() and use geom_col() instead of fiddling around with the automatic settings of geom_bar():\n\ncars_drive_year &lt;- mpg %&gt;% \n  # Make year a categorical variable instead of a number\n  mutate(year = factor(year)) %&gt;% \n  group_by(drv, year) %&gt;% \n  summarize(total = n()) %&gt;% \n  # Group by year to get the proportions of drives within each year\n  group_by(year) %&gt;% \n  mutate(prop = total / sum(total))\n\n# Specify x and y and use geom_col()\nggplot(cars_drive_year, aes(x = year, y = prop, fill = drv)) +\n  geom_col(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\nWhat’s the difference between read_csv() vs. read.csv()?\nIn all the code I’ve given you in this class, you’ve loaded CSV files using read_csv(), with an underscore. In lots of online examples of R code, and in lots of other peoples’ code, you’ll see read.csv() with a period. They both load CSV files into R, but there are subtle differences between them.\nread.csv() (read dot csv) is a core part of R and requires no external packages (we say that it’s part of “base R”). It loads CSV files. That’s its job. However, it can be slow with big files, and it can sometimes read text data in as categorical data, which is weird (that’s less of an issue since R 4.0; it was a major headache in the days before R 4.0). It also makes ugly column names when there are “illegal” columns in the CSV file—it replaces all the illegal characters with .s\n\n\n\n\n\n\nLegal column names\n\n\n\nR technically doesn’t allow column names that (1) have spaces in them or (2) start with numbers.\nYou can still access or use or create column names that do this if you wrap the names in backticks, like this:\n\nmpg %&gt;% \n  group_by(drv) %&gt;% \n  summarize(`A column with spaces` = mean(hwy))\n## # A tibble: 3 × 2\n##   drv   `A column with spaces`\n##   &lt;chr&gt;                  &lt;dbl&gt;\n## 1 4                       19.2\n## 2 f                       28.2\n## 3 r                       21\n\n\n\nread_csv() (read underscore csv) comes from {readr}, which is one of the 9 packages that get loaded when you run library(tidyverse). Think of it as a new and improved version of read.csv(). It handles big files a better, it doesn’t ever read text data in as categorical data, and it does a better job at figuring out what kinds of columns are which (if it detects something that looks like a date, it’ll treat it as a date). It also doesn’t rename any columns—if there are illegal characters like spaces, it’ll keep them for you, which is nice.\nMoral of the story: use read_csv() instead of read.csv(). It’s nicer.\n\n\nWhy does R keep yelling at me with warnings and messages?\nBy now you’ve seen ominous looking red text in R, like 'summarise()' has grouped output by 'Gender'. You can override using the '.groups' argument or Warning: Removed 2 rows containing missing values, and so on. You might have panicked a little after seeing this and thought you were doing something wrong.\nNever fear! You’re most likely not doing anything wrong.\nR shows red text in the console pane in three different situations:\n\nErrors: When the red text is a legitimate error, it will be prefaced with “Error in…” and will try to explain what went wrong. Generally when there’s an error, the code will not run. For example, if you see Error in ggplot(...) : could not find function \"ggplot\", it means that the ggplot() function is not accessible because the package that contains the function (ggplot2) was not loaded with library(ggplot2) (or library(tidyverse), which loads ggplot2). Thus you cannot use the ggplot() function without the ggplot2 package being loaded first.\nWarnings: When the red text is a warning, it will be prefaced with “Warning:” and R will try to explain why there’s a warning. Generally your code will still work, but with some caveats. For example if you create a scatterplot based on a dataset where two of the rows of data have missing entries that would be needed to create points in the scatterplot, you will see this warning: Warning: Removed 2 rows containing missing values (geom_point). R will still produce the scatterplot with all the remaining non-missing values, but it is warning you that two of the points aren’t there.\nMessages: When the red text doesn’t start with either “Error” or “Warning”, it’s just a friendly message. You’ll see these messages when you load R packages or when you read data saved in spreadsheet files with read_csv(). These are helpful diagnostic messages and they don’t stop your code from working. This is what 'summarise()' has grouped output by 'Gender'... is—just a helpful note.\n\nRemember, when you see red text in the console, don’t panic. It doesn’t necessarily mean anything is wrong. Rather:\n\nIf the text starts with “Error”, figure out what’s causing it. Think of errors as a red traffic light: something is wrong!\nIf the text starts with “Warning”, figure out if it’s something to worry about. For instance, if you get a warning about missing values in a scatterplot and you know there are missing values, you’re fine. If that’s surprising, look at your data and see what’s missing. Think of warnings as a yellow traffic light: everything is working fine, but watch out/pay attention.\nOtherwise, the text is just a message. Read it, wave back at R, and thank it for talking to you. Think of messages as a green traffic light: everything is working fine and keep on going!\n\n\n\nHow can I hide warnings and messages?\nIn general, you’ll want to try to deal with errors and warnings, often by adjusting or clarifying something in your code. In your final knitted documents, you typically want to have nice clean output without any warnings or messages. You can fix these warnings and messages in a couple ways: (1) change your code to deal with them, or (2) just hide them.\nFor instance, if you do something like this to turn off the fill legend:\n# Not actual code; don't try to run this\nggplot(data = whatever, aes(x = blah, y = blah, fill = blah)) +\n  geom_col() +\n  guides(fill = FALSE)\nYou’ll get this warning:\n## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" \n## instead as of ggplot2 3.3.4.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\nYou’ll still get a plot and the fill legend will be gone and that’s great, but the warning is telling you that that code has been deprecated and is getting phased out and will eventually stop working. ggplot helpfully tells you how to fix it: use guides(fill = \"none\") instead. Changing that code removes the warning and everything will work just fine:\n# Not actual code; don't try to run this\nggplot(data = whatever, aes(x = blah, y = blah, fill = blah)) +\n  geom_col() +\n  guides(fill = \"none\")\nIn other cases, though, nothing’s wrong and R is just being talkative. For instance, when you load {tidyverse}, you get a big wall of text:\n\nlibrary(tidyverse)\n## ── Attaching core tidyverse packages ─────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.2     ✔ readr     2.1.4\n## ✔ forcats   1.0.0     ✔ stringr   1.5.0\n## ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n## ✔ purrr     1.0.1     \n## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package to force all conflicts to become errors\n\nThat’s all helpful information—it tells you that R loaded 9 related packages for you ({ggplot2}, {dplyr}, etc.). But none of that needs to be in a knitted document. You can turn off those messages and warnings using chunk options:\n\n```{r load-packages, warning=FALSE, message=FALSE}\nlibrary(tidyverse)\n```\n\nThe same technique works for other messages too. In exercise 3, for instance, you saw this message a lot:\n## `summarise()` has grouped output by 'Gender'. \n## You can override using the .groups` argument.\nThat’s nothing bad and you did nothing wrong—that’s just R talking to you and telling you that it did something behind the scenes. When you use group_by() with one variable, like group_by(Gender), once you’re done summarizing and working with the groups, R ungroups your data automatically. When you use group_by() with two variables, like group_by(Gender, Film), once you’re done summarizing and working with the groups, R ungroups the last of the variables and gives you a data frame that is still grouped by the other variables. So with group_by(Gender, Film), after you’ve summarized stuff, R stops grouping by Film and groups by just Gender. That’s all the summarise() has grouped output by... message is doing—it’s telling you that it’s still grouped by something. It’s no big deal.\nSo, to get rid of the message in this case, you can use message=FALSE in the chunk options to disable the message:\n\n```{r lotr-use-two-groups, message=FALSE}\nlotr_gender_film &lt;- lotr %&gt;% \n  group_by(Gender, Film) %&gt;% \n  summarize(total = sum(Words))\n```"
  },
  {
    "objectID": "news/2023-09-19_faqs_week-04.html",
    "href": "news/2023-09-19_faqs_week-04.html",
    "title": "Week 4 FAQs",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nGreat work this week with the essential construction data from exercise 4! It was a big dataset and you were able to show some interesting patterns with neat plots!\nJust a few FAQs this week—enjoy!\n\nHow can I make my R code more readable?\n(This was a tip from last week, but it’s a good reminder.)\nUnlike other programming langauges, R is pretty forgiving about code style. You can have lines of code that are indented however you want, that are as long as you want, and that have line breaks generally wherever you want.\nThere are some basic grammar-like guidelines you can follow to make your code more readable, more consistent, and easier for people to follow. I’ve compiled a mini style guide here—you should check it out and try implementing the suggestsions.\n\n\nMy axis labels are overlapping and ugly. How can I fix that?\n(This was a question from last week too, but lots of you asked about it again)\nSometimes you’ll have text that is too long to fit comfortably as axis labels and the labels will overlap and be gross:\n\n\n\nOverlapping axis labels\n\n\nCheck out this blog post for a bunch of different ways to fix this and make nicer labels, like this:\n\n\n\nNon-overlapping axis labels\n\n\n\n\nWhy did we need to group a second time when calculating the proportions for the heatmap?\nAt the end of exercise 4, you created a heatmap showing the proportions of different types of construction projects across different boroughs. In the instructions, I said you’d need to use group_by() twice to get predictable proportions. Some of you have wondered what this means. Here’s a quick illustration.\nWhen you group by a column, R splits your data into separate datasets behind the scenes, and when you use summarize(), it calculates summary statistics (averages, counts, medians, etc.) for each of those groups. So when you used group_by(BOROUGH, CATEGORY), R made smaller datasets of Bronx Affordable Housing, Bronx Approved Work, Brooklyn Affordable Housing, Brooklyn Approved Work, and so on. Then when you used summarize(total = n()), you calculated the number of rows in each of those groups, thus giving you the number of projects per borough per category. That’s basic group_by() %&gt;% summarize() stuff.\nOnce you have a count of projects per borough, you have to decide how you want to calculate proportions. In particular, you need to figure out what your denominator is. Do you want the proportion of all projects within each borough (e.g. X% of projects in the Bronx are affordable housing, Y% in the Bronx are approved work, and so on until 100% of the projects in the Bronx are accounted for), so that each borough constitutes 100%? Do you want the proportion of boroughs for each project (e.g. X% of affordable housing projects are in the Bronx, Y% of affordable housing projects are in Brooklyn, and so on until 100% of the affordable housing projects are accounted for). This is where the second group_by() matters.\nFor example, if you group by borough and then use mutate to calculate the proportion, the proportion in each borough will add up to 100%. Notice the denominator column here—it’s unique to each borough (1169 for the Bronx, 2231 for Brooklyn, etc.).\nessential %&gt;%\n  group_by(BOROUGH, CATEGORY) %&gt;%\n  summarize(totalprojects = n()) %&gt;%\n  group_by(BOROUGH) %&gt;%\n  mutate(denominator = sum(totalprojects),\n         proportion = totalprojects / denominator)\n#&gt; # A tibble: 33 × 5\n#&gt; # Groups:   BOROUGH [5]\n#&gt;    BOROUGH  CATEGORY               totalprojects denominator proportion\n#&gt;    &lt;fct&gt;    &lt;fct&gt;                          &lt;int&gt;       &lt;int&gt;      &lt;dbl&gt;\n#&gt;  1 Bronx    Affordable Housing                80        1169   0.0684  \n#&gt;  2 Bronx    Approved Work                    518        1169   0.443   \n#&gt;  3 Bronx    Homeless Shelter                   1        1169   0.000855\n#&gt;  4 Bronx    Hospital / Health Care            55        1169   0.0470  \n#&gt;  5 Bronx    Public Housing                   276        1169   0.236   \n#&gt;  6 Bronx    Schools                          229        1169   0.196   \n#&gt;  7 Bronx    Utility                           10        1169   0.00855 \n#&gt;  8 Brooklyn Affordable Housing               168        2231   0.0753  \n#&gt;  9 Brooklyn Approved Work                   1223        2231   0.548   \n#&gt; 10 Brooklyn Hospital / Health Care            66        2231   0.0296  \n#&gt; # … with 23 more rows\nIf you group by category instead, the proportion within each category will add to 100%. Notice how the denominator for affordable housing is 372, approved work is 4189, and so on.\nessential %&gt;%\n  group_by(BOROUGH, CATEGORY) %&gt;%\n  summarize(totalprojects = n()) %&gt;%\n  group_by(CATEGORY) %&gt;%\n  mutate(denominator = sum(totalprojects),\n         proportion = totalprojects / denominator)\n#&gt; # A tibble: 33 × 5\n#&gt; # Groups:   CATEGORY [7]\n#&gt;    BOROUGH  CATEGORY               totalprojects denominator proportion\n#&gt;    &lt;fct&gt;    &lt;fct&gt;                          &lt;int&gt;       &lt;int&gt;      &lt;dbl&gt;\n#&gt;  1 Bronx    Affordable Housing                80         372      0.215\n#&gt;  2 Bronx    Approved Work                    518        4189      0.124\n#&gt;  3 Bronx    Homeless Shelter                   1           5      0.2  \n#&gt;  4 Bronx    Hospital / Health Care            55         259      0.212\n#&gt;  5 Bronx    Public Housing                   276        1014      0.272\n#&gt;  6 Bronx    Schools                          229        1280      0.179\n#&gt;  7 Bronx    Utility                           10          90      0.111\n#&gt;  8 Brooklyn Affordable Housing               168         372      0.452\n#&gt;  9 Brooklyn Approved Work                   1223        4189      0.292\n#&gt; 10 Brooklyn Hospital / Health Care            66         259      0.255\n#&gt; # … with 23 more rows\nYou can also ungroup completely before calculating the proportion. This makes it so the entire proportion column adds to 100%:\nessential %&gt;%\n  group_by(BOROUGH, CATEGORY) %&gt;%\n  summarize(totalprojects = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(denominator = sum(totalprojects),\n         proportion = totalprojects / denominator)\n#&gt; # A tibble: 33 × 5\n#&gt;    BOROUGH  CATEGORY               totalprojects denominator proportion\n#&gt;    &lt;fct&gt;    &lt;fct&gt;                          &lt;int&gt;       &lt;int&gt;      &lt;dbl&gt;\n#&gt;  1 Bronx    Affordable Housing                80        7209   0.0111  \n#&gt;  2 Bronx    Approved Work                    518        7209   0.0719  \n#&gt;  3 Bronx    Homeless Shelter                   1        7209   0.000139\n#&gt;  4 Bronx    Hospital / Health Care            55        7209   0.00763 \n#&gt;  5 Bronx    Public Housing                   276        7209   0.0383  \n#&gt;  6 Bronx    Schools                          229        7209   0.0318  \n#&gt;  7 Bronx    Utility                           10        7209   0.00139 \n#&gt;  8 Brooklyn Affordable Housing               168        7209   0.0233  \n#&gt;  9 Brooklyn Approved Work                   1223        7209   0.170   \n#&gt; 10 Brooklyn Hospital / Health Care            66        7209   0.00916 \n#&gt; # … with 23 more rows\nWhich one you do is up to you—it depends on the story you’re trying to tell.\n\n\nWhy isn’t the example code using data = whatever and mapping = aes() in ggplot() anymore? Do we not have to use argument names?\nIn the first few sessions, you wrote code that looked like this:\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +\n  geom_point()\n\nIn R, you feed functions arguments like data and mapping and I was having you explicitly name the arguments, like data = mpg and mapping = aes(...).\nIn general it’s a good idea to use named arguments, since it’s clearer what you mean.\nHowever, with really common functions like ggplot(), you can actually skip the names. If you look at the documentation for ggplot() (i.e. run ?ggplot in your R console or search for “ggplot” in the Help panel in RStudio), you’ll see that the first expected argument is data and the second is mapping.\n\n\n\nThe documentation for the ggplot() function\n\n\nIf you don’t name the arguments, like this…\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n…ggplot() will assume that the first argument (mpg) really means data = mpg and that the second really means mapping = aes(...).\nIf you don’t name the arguments, the order matters. This won’t work because ggplot will think that the aes(...) stuff is really data = aes(...):\n\nggplot(aes(x = displ, y = hwy), mpg) +\n  geom_point()\n\nIf you do name the arguments, the order doesn’t matter. This will work because it’s clear that data = mpg (even though this feels backwards and wrong):\n\nggplot(mapping = aes(x = displ, y = hwy), data = mpg) +\n  geom_point()\n\nThis works with all R functions. You can either name the arguments and put them in whatever order you want, or you can not name them and use them in the order that’s listed in the documentation.\nIn general, you should name your arguments for the sake of clarity. For instance, with aes(), the first argument is x and the second is y, so you can technically do this:\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point()\n\nThat’s nice and short, but you have to remember that displ is on the x-axis and hwy is on the y-axis. And it gets extra confusing once you start mapping other columns:\n\nggplot(mpg, aes(displ, hwy, color = drv, size = hwy)) +\n  geom_point()\n\nAll the other aesthetics like color and size are named, but x and y aren’t, which just feels… off.\nSo use argument names except for super common things like ggplot() and the {dplyr} verbs like mutate(), group_by(), filter(), etc.\n\n\nI have numbers like 20000 and want them formatted with commas like 20,000. Can I do that automatically?\nYes you can! There’s an incredible package called {scales}. It lets you format numbers and axes and all sorts of things in magical ways. If you look at the documentation, you’ll see a ton of label_SOMETHING() functions, like label_comma(), label_dollar(), and label_percent().\nYou can use these different labeling functions inside scale_AESTHETIC_WHATEVER() layers in ggplot.\nlabel_comma() adds commas:\n\nlibrary(scales)\n## Warning: package 'scales' was built under R version 4.2.3\nlibrary(gapminder)\n## Warning: package 'gapminder' was built under R version 4.2.3\n\ngapminder_2007 &lt;- gapminder %&gt;% \n  filter(year == 2007)\n\nggplot(gapminder_2007, aes(x = gdpPercap)) +\n  geom_histogram(binwidth = 1000) +\n  scale_x_continuous(labels = label_comma())\n\n\n\n\n\n\n\n\nlabel_dollar() adds commas and includes a “$” prefix:\n\nggplot(gapminder_2007, aes(x = gdpPercap)) +\n  geom_histogram(binwidth = 1000) +\n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\n\n\nlabel_percent() multiplies values by 100 and formats them as percents:\n\ngapminder_percents &lt;- gapminder_2007 %&gt;% \n  group_by(continent) %&gt;% \n  summarize(n = n()) %&gt;% \n  mutate(prop = n / sum(n))\n\nggplot(gapminder_percents, aes(x = continent, y = prop)) +\n  geom_col() +\n  scale_y_continuous(labels = label_percent())\n\n\n\n\n\n\n\n\nYou can also change a ton of the settings for these different labeling functions. Want to format something as Euros and use periods as the number separators instead of commas, like Europeans? Change the appropriate arguments! You can check the documentation for each of the label_WHATEVER() functions to see what you can adjust (like label_dollar() here)\n\nggplot(gapminder_2007, aes(x = gdpPercap)) +\n  geom_histogram(binwidth = 1000) +\n  scale_x_continuous(labels = label_dollar(prefix = \"€\", big.mark = \".\"))\n\n\n\n\n\n\n\n\nAll the label_WHATEVER() functions actually create copies of themselves, so if you’re using lots of custom settings, you can create your own label function, like label_euro() here:\n\n# Make a custom labeling function\nlabel_euro &lt;- label_dollar(prefix = \"€\", big.mark = \".\")\n\n# Use it on the x-axis\nggplot(gapminder_2007, aes(x = gdpPercap)) +\n  geom_histogram(binwidth = 1000) +\n  scale_x_continuous(labels = label_euro)\n\n\n\n\n\n\n\n\nThese labeling functions also work with other aesthetics, like fill and color and size. Use them in scale_AESTHETIC_WHATEVER():\n\nggplot(\n  gapminder_2007, \n  aes(x = gdpPercap, y = lifeExp, size = pop, color = pop)\n) +\n  geom_point() +\n  scale_x_continuous(labels = label_dollar()) +\n  scale_size_continuous(labels = label_comma()) +\n  scale_color_viridis_c(labels = label_comma())\n\n\n\n\n\n\n\n\nThere are also some really neat and fancy things you can do with scales, like formatting logged values, abbreviating long numbers, and many other things. Check out this post for an example of working with logged values.\n\nggplot(\n  gapminder_2007, \n  aes(x = gdpPercap, y = lifeExp, size = pop, color = continent)\n) +\n  geom_point() +\n  scale_x_log10(breaks = 500 * 2^seq(0, 9, by = 1),\n                labels = label_dollar(scale = cut_short_scale())) +\n  scale_size_continuous(labels = label_comma(scale_cut = cut_short_scale()))"
  },
  {
    "objectID": "news/2023-10-03_cleaner-nicer-rmd-output.html",
    "href": "news/2023-10-03_cleaner-nicer-rmd-output.html",
    "title": "Making cleaner, nicer R Markdown output",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nWith mini project 1 coming up, I have a bunch of helpful tips to make sure your documents look as nice and readable as possible. Here we go!\n\nKnit often\nDon’t wait until you’ve finished typing your entire document before knitting. Knit to HTML occasionally to make sure all your code runs from a fresh session and to make sure all the extra warnings and messages and any other strange output is taken care of.\nEven if your final document won’t be an HTML file, I recommend knitting to HTML often because (1) it’s fast and (2) you can see the output in an easily-refreshable web browser immediately. When you knit to PDF, you have to wait a few extra seconds for LaTeX to compile the rendered Markdown file into PDF, and then you have to reopen the PDF file if you had it open previously. When you knit to Word, if you have a previous version of the .docx file open already, your computer will get mad (especially on Windows), so you have to remember to always close the knitted document in Word before reknitting it, and that’s annoying and you’ll inevitably forget.\nSo knit to HTML often. Then at the end, once you know everything is working right and the document looks nice and clean, knit to PDF or Word or whatever your final output will be.\n\n\nCheck that your text looks okay\nWhen knitting, you should also check your text to make sure it looks okay. Here are some common issues that you’ll see with your text:\n\nHeadings that aren’t headings\nOften you’ll want to make a heading for one of your sections and you’ll type this:\n#Task 1: Reflection\nWhen you knit that, though, it’ll look like this:\n\n#Task 1: Reflection\n\nThat’s not a heading—that’s just a regular paragraph. For a #-style heading to turn into an actual heading, you need a space after the # and an empty line after the heading text:\n# Task 1: Reflection\n\nSome text\n\n## A subheading\n\nSome more text\n\n\nParagraphs that aren’t paragraphs\nYou’re used to indenting paragraphs in Word or Google Docs. First-line indentation is a normal thing with word processors.\nIndenting lines is unnecessary with Markdown and will mess up your text.\nFor example, let’s say you type something like this:\n    It was the best of times, it was the worst of times, it was the age of \nwisdom, it was the age of foolishness, it was the epoch of belief, it was the \nepoch of incredulity, it was the season of Light, it was the season of Darkness, \nit was the spring of hope, it was the winter of despair, we had everything \nbefore us, we had nothing before us, we were all going direct to Heaven, we were \nall going direct the other way—in short, the period was so far like the present \nperiod, that some of its noisiest authorities insisted on its being received, \nfor good or for evil, in the superlative degree of comparison only.\n\n    There were a king with a large jaw and a queen with a plain face, on the \nthrone of England; there were a king with a large jaw and a queen with a fair \nface, on the throne of France. In both countries it was clearer than crystal to \nthe lords of the State preserves of loaves and fishes, that things in general \nwere settled for ever. \nThat looks like Word-style text, with indented paragraphs. When you knit it, though, it’ll turn into code-formatted monospaced text that runs off the edge of the page:\n\nIt was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way—in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\n\nThere were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n\nThat’s because Markdown treats anything that is indented with four spaces as code, not as text.\nYou shouldn’t indent your text. Instead, add an empty line between each paragraph to separate them.\n\n\nLists that aren’t lists\nDue to esoteric historical reasons, the end of lines (i.e. what happens when you press enter when typing) is different on Macs and Windows. I use a Mac, and I typed the exercise templates on a Mac, so they have Mac-style line endings. That means that on Windows, you’ll sometimes see text that looks normal like this:\n- Here's a list\n- Stuff goes here\n- Neato\nThat should theoretically turn into a list, but when you knit it, you see this:\n\n\nHere’s a list - Stuff goes here - Neato\n\n\nThat’s not a list! The easiest way to fix that is to delete the line break at the end of “list” in that first line and then press enter (and so on), so that it turns in to this when knitting:\n\n\nHere’s a list\nStuff goes here\nNeato\n\n\n\n\n\nDon’t install packages in your document\nMake sure you don’t include code to install packages in your Rmd files. Like, don’t include install.packages(\"ggtext\") or whatever. If you do, R will reinstall that package every time you knit your document, which is excessive. All you need to do is load the package with library()\nTo help myself remember to not include package installation code in my document, I make an effort to either install packages with my mouse by clicking on the “Install” button in the Packages panel in RStudio, or only ever typing (or copying/pasting) code like install.packages(\"whatever\") directly in the R console and never putting it in a chunk.\n\n\nClean up warnings and messages\nIdeally, your document shouldn’t have any errors, warnings, or messages in it. Some packages are talkative, like {tidyverse}:\n\nlibrary(tidyverse)\n## Warning: package 'tidyverse' was built under R version 4.2.3\n## Warning: package 'ggplot2' was built under R version 4.2.3\n## Warning: package 'tibble' was built under R version 4.2.3\n## Warning: package 'tidyr' was built under R version 4.2.3\n## Warning: package 'purrr' was built under R version 4.2.3\n## Warning: package 'dplyr' was built under R version 4.2.3\n## Warning: package 'stringr' was built under R version 4.2.3\n## Warning: package 'forcats' was built under R version 4.2.3\n## Warning: package 'lubridate' was built under R version 4.2.3\n## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n## ✔ purrr     1.0.2     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nSome functions are talkative, like summarize() after working on more than one group:\n\nmpg %&gt;% \n  group_by(year, drv) %&gt;% \n  summarize(avg = mean(hwy))\n## `summarise()` has grouped output by 'year'. You can override using the\n## `.groups` argument.\n## # A tibble: 6 × 3\n## # Groups:   year [2]\n##    year drv     avg\n##   &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n## 1  1999 4      18.8\n## 2  1999 f      27.9\n## 3  1999 r      20.6\n## 4  2008 4      19.5\n## 5  2008 f      28.4\n## 6  2008 r      21.3\n\nOr geom_smooth():\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth()\n## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nWhen you have code that you know will return some sort of message, add message=FALSE to the chunk options for it. When you have code that you know will make some sort of warning, (1) try to fix it so that the warning doesn’t happen, or (2) add warning=FALSE to the chunk options for it.\n\n```{r load-packages, warning=FALSE, message=FALSE}\nlibrary(tidyverse)\n```\n\nKnitting often and scrolling through the rendered document will help you check that you don’t have any extra output like warnings, messages, and errors too.\n\n\nGlobal chunk options\nYou can control the settings for all the chunks in your document by using knitr::opts_chunk$set(). You can use any knitr chunk options (see here or here for a complete list).\n\n\n\n\n\n\nLet R make the code for you\n\n\n\nI never remember the full name of that function, but you can get it if you go to “File &gt; New File &gt; R Markdown”; it’s part of the Rmd template RStudio makes for you.\n\n\nFor instance, if you want to hide all the code in your document, you could either go add echo=FALSE to each chunk individually, or you could add it to the global chunk options:\nknitr::opts_chunk$set(echo = FALSE)\nYou could hide all warnings and messages there too:\nknitr::opts_chunk$set(warning = FALSE, message = FALSE)\nBUT I don’t recommend doing that ↑ normally. Warnings are helpful—when you get them, you’ll typically want to try to deal with them. Instead of hiding them, try to fix them. If you can’t fix them (like when you run library(tidyverse)), then you can hide them for that chunk.\nYou can also set default figure dimensions and lots of other settings. I typically include something like this at the start of my documents:\nknitr::opts_chunk$set(\n  fig.width = 6, \n  fig.height = 6 * 0.618,  # Golden rectangles!\n  fig.align = \"center\", \n  out.width = \"80%\"\n)\nMake sure you hide the chunk that contains those settings with something like include=FALSE, otherwise you’ll see it in your document:\n```{r document-chunk-options, include=FALSE}\nknitr::opts_chunk$set(\n  fig.width = 6, \n  fig.height = 6 * 0.618,  # Golden rectangles!\n  fig.align = \"center\", \n  out.width = \"80%\"\n)\n```\n\n\nFancy document settings\nYou can make your documents prettier by changing the settings for different outputs (see this for complete documentation).\nFor example, if you want a different font in PDFs, you can include something like this in your frontmatter. See here for complete details.\n---\ntitle: \"Whatever\"\ndate: \"Whatever\"\noutput:\n  pdf_document:\n    latex_engine: xelatex  # This lets LaTeX work with different fonts\nmainfont: Garamond\nfontsize: 10pt\n---\nIf you want a different template for HTML (you can use these, or if you know HTML and CSS, you can make your own), you can do something like this. See here for complete details.\n---\ntitle: \"Whatever\"\ndate: \"Whatever\"\noutput:\n  html_document:\n    theme: united\n---\nYou can’t do much with Word because Microsoft makes it hard to do anything to .docx files, but you can create a .docx file where you define a bunch of style settings (e.g., make Heading 1 a specific sans serif font, left-aligned, bold; make Heading 2 the same font and not bold; make main paragraphs an 11pt serif font; and so on), and then you can inject those styles into the knitted document like this. See here for complete details.\n---\ntitle: \"Whatever\"\ndate: \"Whatever\"\noutput:\n  word_document:\n    reference_docx: my-neat-styles.docx\n---\nYou can keep all those output-specific settings in the same document—knitr will only use the settings required for each output (i.e. if you knit to PDF it’ll ignore the HTML settings):\n---\ntitle: \"Whatever\"\ndate: \"Whatever\"\noutput:\n  pdf_document:\n    latex_engine: xelatex  # This lets LaTeX work with different fonts\n  html_document:\n    theme: united\n  word_document:\n    reference_docx: my-neat-styles.docx\nmainfont: Garamond\nfontsize: 10pt\n---\nYou can get really fancy with this stuff. For example, this Rmd file creates four different kinds of customized output (see here):\n\nPDF\nManuscripty PDF\nHTML\nWord\n\n\n\nQuarto\nIf you want even fancier documents, try the next generation of R Markdown called Quarto. It makes it a billion times easier to do things like change fonts, use cross references (i.e. write things like “see Figure 1” or “see Table 3”), use citations (automatic APA or Chicago citation styles!), use fancier layouts, and so on.\nThis course website is built with Quarto. Check out this for an example of an academic paper written with Quarto:\n\n.qmd file\nPDF, manuscripty PDF, and HTML output for that .qmd file\n\nQuarto has great documentation, it’s fully supported in RStudio already, and you basically already know how to use it. It’s just .Rmd with fancier, newer features.\n\n\nChunk names\nLabeling your R chunks is a good thing to do, since it helps with document navigation and is generally good practice. If you’re using chunk labels make sure you don’t use spaces in them. R will still knit a document with spaceful names, but it converts the spaces to underscores before doing it. So instead of naming chunks like {r My Neat Chunk, message=FALSE}, use something like {r my-neat-chunk} or {r my_neat_chunk}."
  },
  {
    "objectID": "news/2023-10-03_mini-project-1.html",
    "href": "news/2023-10-03_mini-project-1.html",
    "title": "Time for mini project 1!",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nMini project 1 is due in less than two weeks, so it’s a good time to start thinking about it. The complete instructions are accessible here, along with starter code that should help you clean and prepare the data, and the rubric I’ll use to grade the project.\nI’m giving you a big dataset with 100,000+ rows in it. Your job is to tell some sort of story with it. You’ll likely want to summarize it somehow (similar to what you’ve done with essential pandemic construction and words in The Lord of the Rings in past exercises).\nUnlike exercises, there are late penalties for this mini project (mostly to help keep you on track and on schedule).\nIf you need help, please reach out on Slack! Do not suffer in silence!\nGood luck!"
  },
  {
    "objectID": "news/2023-10-12_messy-broken-code-tips.html",
    "href": "news/2023-10-12_messy-broken-code-tips.html",
    "title": "Tips for debugging and cleaning broken code",
    "section": "",
    "text": "← News\n\n\n\nLots of the code you run in this class is actually a big long chain of functions or plot layers, like {dplyr} functions that are all connected with %&gt;%s or {ggplot2} functions that are all connected with +s.\nInevitably, something will go wrong at some point in the chain—often a misspelled word or a misplaced comma or a misplaced parenthesis. Tracking down (or debugging) the issue can be often be tricky!\nFor example, four (4) things are wrong in this code. See if you can spot them without running it—good luck!\n\ngapminder_gdp_thing &lt;- gapminder %&gt;% filter(year &gt; 1990) %&gt;% \n  mutate(gdp_total = gdpPercap * pop),\n     is_africa = ifelse(continent == \"Africa\", \n\"Is Africa\", \"Isn't Africa\"))) %&gt;%  \n           group_by(year, continent) %&gt;% \n  mutate(gdp_above_continent_median = \n           ifelse(gdp_total &gt; median(gdp_total)), TRUE, FALSE)) %&gt;%\nungroup() %&gt;% arrange(desc(gdp_total)) %&gt;%\n  mutate(continent = fct_inorder(continent) = country = fct_inorder(country))\n\n\n\n\n\n\n\nClick here for the answers\n\n\n\n\n\n\ngapminder_gdp_thing &lt;- gapminder %&gt;% filter(year &gt; 1990) %&gt;% \n1  mutate(gdp_total = gdpPercap * pop),\n     is_africa = ifelse(continent == \"Africa\", \n2\"Is Africa\", \"Isn't Africa\"))) %&gt;%\n           group_by(year, continent) %&gt;% \n  mutate(gdp_above_continent_median = \n3           ifelse(gdp_total &gt; median(gdp_total)), TRUE, FALSE)) %&gt;%\nungroup() %&gt;% arrange(desc(gdp_total)) %&gt;%\n4  mutate(continent = fct_inorder(continent) = country = fct_inorder(country))\n\n\n1\n\nThere’s a closing parenthesis after pop) that shouldn’t be there—it ends the mutate() too early and is_africa = BLAH ends up not being inside mutate()\n\n2\n\nThere’s an extra parenthesis at the end of \"Isn't Africa\")))\n\n3\n\nThere’s an extra parenthesis after median(gdp_total))\n\n4\n\nThere’s an = instead of a comma in between fct_inorder(continent) and country\n\n\n\n\nHere’s what the fixed, reindented version looks like:\n\ngapminder_gdp_thing &lt;- gapminder %&gt;% \n  filter(year &gt; 1990) %&gt;% \n  mutate(\n1    gdp_total = gdpPercap * pop,\n    is_africa = ifelse(\n      continent == \"Africa\", \n      \"Is Africa\", \n2      \"Isn't Africa\")\n  ) %&gt;%\n  group_by(year, continent) %&gt;% \n  mutate(\n    gdp_above_continent_median = ifelse(\n      gdp_total &gt; median(gdp_total), \n      TRUE, \n3      FALSE)\n  ) %&gt;%\n  ungroup() %&gt;% \n  arrange(desc(gdp_total)) %&gt;%\n  mutate(continent = fct_inorder(continent), \n4         country = fct_inorder(country))\n\n\n1\n\nFixed!\n\n2\n\nFixed!\n\n3\n\nFixed!\n\n4\n\nFixed!\n\n\n\n\n\n\n\nIt’s nearly impossible to figure out what’s wrong here without running it. And even if you do run it, you’ll get somewhat cryptic errors.\nI have two important techniques and tips that fix 90% of my debugging problems:\n\nReformat the code by reindenting it and breaking it into multiple lines\nRun the code incrementally, line by line\n\nEach of these techniques help track down issues in the code above and are good skills to know in general. I’ll explain each approach and give a little video demonstration below.\n\nReformat the code\nIn the R style suggestions in the Resources section, it explains that each layer of a %&gt;%-chained pipeline or ggplot plot should be on separate lines, with the %&gt;% or the + at the end of the line, indented with two spaces.\n\nggplot(data = blah, mapping = aes(x = thing, y = thing2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous() +\n  theme_minimal()\n\nAdditionally, it’s often a good idea to add lines in between the arguments inside functions and line them up within the ()s of the function.\nThis makes it so you can clearly see each step of the pipeline or plot, and you can clearly see each of the arguments inside each function.\nPeople tend to take one of two approaches to argument alignment—aligning argument names at the same level as the opening ( of the function like this:\n\nsome_object &lt;- some_dataset %&gt;%\n  a_function() %&gt;%\n  another_function(argument = 1,\n                   argument = 2,\n                   argument = some_function(thing1 = \"a\", \n                                            thing2 = \"b\")) %&gt;%\n  yet_another_function()\n\n…or aligning argument names two spaces to the right of where the argument starts, like this:\n\nsome_object &lt;- some_dataset %&gt;%\n  a_function() %&gt;%\n  another_function(\n    argument = 1,\n    argument = 2,\n    argument = some_function(\n      thing1 = \"a\", \n      thing2 = \"b\"\n    )\n  ) %&gt;%\n  yet_another_function()\n\nRStudio can actually reindent code for you automatically, and it can use either of these approaches. If you want the first approach (where argument names align after the opening (), check “Tools &gt; Global Options &gt; Code &gt; Vertically align arguments in auto-indent”; if you want the second approach (where argument names are all a little indented from where the argument starts), make sure that option is unchecked.\nTo have RStudio reindent code for you, select the code you want to be reindented and go to “Code &gt; Reindent lines”, or use the keyboard shortcut ⌘I on macOS or ctrl + I on Windows.\nHere’s what that looks like. Notice how distorted the indentation is initially—RStudio is smart enough to fix it all:\n\nWith keyboard shortcutWith menuWithout vertical argument alignment\n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n\nNot only does reindentation make it easier to read your code, it can reveal issues with the code. Remember that code from the beginning of this post with four things wrong? If we reindent it, the line that starts with is_africa = ifelse( is indented funny—it gets put at the start of the line, when really it should be at the same level as gdp_total, since those are both arguments for the mutate() function. If you look at the line above, you’ll see that there’s a ) after gdpPercap * pop, which closes mutate() prematurely, so is_africa isn’t actually inside mutate(). If we get rid of the ) at the end of pop and reindent again, is_africa shows up in the right place.\n\n\n  \n\n\n \n\n\n\n\n\n\nExtra strength formatting\n\n\n\nReindenting your code only shifts things around horizontally. If you want more powerful code reformatting, try using “Code &gt; Reformat Code” (or use ⌘⇧A on macOS or ctrl + shift + A on Windows). It’s a more aggressive form of reformatting that will add extra line breaks and other things to make the code more readable:\n\n\n  \n\n\n \nIt doesn’t fix everything—there should be a line break after each %&gt;% in that example ↑ so you’d need to add your own line break before filter(year &gt; 1990) and arrange(desc(gdp_total)), but it works well.\n\n\nI’d recommend trying to keep things indented consistently as you write your code, and periodically reindenting stuff just to make sure everything is nice and aligned. Ultimately R doesn’t care how your code is indented (other languages do, like Python, where one errant space can mess up everything), but humans do care and nicer indentation will help others (and future you!).\n\n\nRun the code incrementally\nYour code is often a series of functions or layers connected with %&gt;% or +. If something goes wrong at any step in the chain of functions, your code won’t work. When that happens, the best strategy for figuring out what went wrong is to run the code incrementally. Just run a few layers of it at a time and then check to see how it looks. Run the first two lines, look at the results, make sure it worked, the run the first three lines, look at the results, make sure it worked, and so on.\nThis is also a good approach for writing your code initially. That big gapminder-based plot at the beginning of this post? I didn’t write that all at once. I started with the initialy ggplot() + geom_point(), ran it, then added another layer or two, ran it with those, then added some more layers or changed some settings inside existing layers, then ran it with those, and so on until the whole thing was built.\nThere are a couple ways to do this. One way is to select just the code you want to run (like from the beginning of ggplot() to right before a + on some layer), then press ⌘ + return on macOS or ctrl + enter on Windows to run just that selection. If it worked as expected, select from the beginning again (i.e. at ggplot()) and go to right before a + on some other layer and run that selection:\n\n\n  \n\n\n \nIf you don’t want to keep using your mouse and want to keep your hands at your keyboard, you can add a # right before a + or %&gt;% to comment it out. That essentially breaks the chain of functions at that point, so when you type ⌘ + return or ctrl + enter, R only runs the code up to that point. Then you can remove the #, put it before another + or %&gt;%, and run it again.\n\n\n  \n\n\n \nHere’s what my typical process for dealing with weirdly indented, broken code looks like. I try to run the whole thing initially, then when it breaks, I reindent it to see if anything is obvious from that. Then I start running it incrementally and check the results of each step to make sure it works up to that point. I do that over and over until the whole pipeline works."
  },
  {
    "objectID": "news/2023-11-06_faqs-weeks-08-09-10.html",
    "href": "news/2023-11-06_faqs-weeks-08-09-10.html",
    "title": "Weeks 8, 9, and 10 tips and FAQs",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nApologies for the delay on FAQ posts! I’ve got some extra good tips here, though, to make up for the past couple weeks!\n\nIn my coefficient plot, the coefficient for per_capita_income didn’t have error bars around it. Why?\n(Technically this was from week 7, but I got a lot of questions about it after I posted the FAQs for week 7, so here it is.)\nIn exercise 7 you created a coefficient plot for a regression model predicting Democratic vote share in the 2016 presidential election based on a bunch of county-level characteristics:\n\nlibrary(tidyverse)\nresults_2016 &lt;- read_csv(\"https://datavizf23.classes.andrewheiss.com/files/data/external_data/results_2016.csv\")\n\n\nlibrary(broom)\n## Warning: package 'broom' was built under R version 4.2.3\n\nmodel &lt;- lm(percent_dem ~ median_age + percent_white + \n              per_capita_income + median_rent + state,\n            data = results_2016)\n\nresults &lt;- tidy(model, conf.int = TRUE) %&gt;% \n  filter(!str_detect(term, \"state\"))\n\nresults\n## # A tibble: 5 × 7\n##   term               estimate std.error statistic   p.value  conf.low conf.high\n##   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)       58.5      1.66          35.3  5.73e-229 55.2      61.8     \n## 2 median_age         0.166    0.0368         4.52 6.33e-  6  0.0942    0.238   \n## 3 percent_white     -0.682    0.0110       -61.9  0         -0.704    -0.661   \n## 4 per_capita_income  0.000271 0.0000497      5.46 5.18e-  8  0.000174  0.000369\n## 5 median_rent        0.0200   0.00163       12.3  6.23e- 34  0.0168    0.0232\n\nThe point ranges in the plot show the coefficient and the confidence interval around the coefficient. Median rent and per capita income here are just dots, though. Did something go wrong? Why is there no confidence interval?\n\nggplot(filter(results, term != \"(Intercept)\"), \n       aes(x = estimate, y = term)) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high))\n\n\n\n\n\n\n\n\nThere is a confidence interval, it’s just really tiny! That’s because of how these variables are measured in the data. Each county has a dollar amount recorded for rent and income:\n\nresults_2016 %&gt;% \n  select(state, county, per_capita_income, median_rent)\n## # A tibble: 3,158 × 4\n##    state   county   per_capita_income median_rent\n##    &lt;chr&gt;   &lt;chr&gt;                &lt;dbl&gt;       &lt;dbl&gt;\n##  1 Alabama Autauga              24571         668\n##  2 Alabama Baldwin              26766         693\n##  3 Alabama Barbour              16829         382\n##  4 Alabama Bibb                 17427         351\n##  5 Alabama Blount               20730         403\n##  6 Alabama Bullock              18628         276\n##  7 Alabama Butler               17403         331\n##  8 Alabama Calhoun              20828         422\n##  9 Alabama Chambers             19291         374\n## 10 Alabama Cherokee             22030         375\n## # ℹ 3,148 more rows\n\nWhen looking at the regression coefficient, we interpret then using single dollars. We’d say something like “A $1 increase in per capita income is associated with a 0.00027 percentage point increase in Democratic vote share, on average.”\nTalking about $1 changes, though, is weird because that kind of change in income really isn’t detectable. That’d be like looking at what happens to voting patterns if income in a county goes from $30,000 to $30,001 or $65,000 to $65,001. Nothing’s going to happen to voting patterns! If the original democratic vote share was 60%, a $1 increase in income would increase that by 0.00027 percentage points to 60.00027%.\nTo make that coefficient more interpretable we can scale down the income column and think of it as hundreds or thousands of dollars instead. We can do that by making a new column in the data…\n\nresults_2016 &lt;- results_2016 %&gt;% \n  mutate(per_capita_income_1000 = per_capita_income / 1000)\n\nresults_2016 %&gt;% \n  select(state, county, per_capita_income_1000, per_capita_income)\n## # A tibble: 3,158 × 4\n##    state   county   per_capita_income_1000 per_capita_income\n##    &lt;chr&gt;   &lt;chr&gt;                     &lt;dbl&gt;             &lt;dbl&gt;\n##  1 Alabama Autauga                    24.6             24571\n##  2 Alabama Baldwin                    26.8             26766\n##  3 Alabama Barbour                    16.8             16829\n##  4 Alabama Bibb                       17.4             17427\n##  5 Alabama Blount                     20.7             20730\n##  6 Alabama Bullock                    18.6             18628\n##  7 Alabama Butler                     17.4             17403\n##  8 Alabama Calhoun                    20.8             20828\n##  9 Alabama Chambers                   19.3             19291\n## 10 Alabama Cherokee                   22.0             22030\n## # ℹ 3,148 more rows\n\n…and then using that new per_capita_income_1000 in the model instead:\n\nmodel &lt;- lm(percent_dem ~ median_age + percent_white + \n              per_capita_income_1000 + median_rent + state,\n            data = results_2016)\n\nresults &lt;- tidy(model, conf.int = TRUE) %&gt;% \n  filter(!str_detect(term, \"state\"))\n\nresults\n## # A tibble: 5 × 7\n##   term                 estimate std.error statistic   p.value conf.low conf.high\n##   &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)           58.5      1.66        35.3  5.73e-229  55.2      61.8   \n## 2 median_age             0.166    0.0368       4.52 6.33e-  6   0.0942    0.238 \n## 3 percent_white         -0.682    0.0110     -61.9  0          -0.704    -0.661 \n## 4 per_capita_income_1…   0.271    0.0497       5.46 5.18e-  8   0.174     0.369 \n## 5 median_rent            0.0200   0.00163     12.3  6.23e- 34   0.0168    0.0232\n\nNow the coefficient is bigger and we have a more visible confidence interval:\n\nggplot(filter(results, term != \"(Intercept)\"), \n       aes(x = estimate, y = term)) +\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high))\n\n\n\n\n\n\n\n\nNow we can say something like “A $1,000 increase in per capita income is associated with a 0.27 percentage point increase in Democratic vote share, on average” (or going from something like 60% to 60.27%)\n\n\nThe importance of layer order\nSo far this semester, most of your plots have involved one or two geom_* layers. At one point in some video (I think), I mentioned that layer order doesn’t matter with ggplot. These two chunks of code create identical plots:\nggplot(...) +\n  geom_point(...) +\n  theme_minimal(...) +\n  scale_fill_viridis_c(...) +\n  facet_wrap(...) +\n  labs(...)\n\nggplot(...) +\n  geom_point(...) +\n  labs(...) +\n  theme_minimal(...) +\n  facet_wrap(...) +\n  scale_fill_viridis_c(...)\nAll those functions can happen in whatever order you want, with one exception. The order of the geom layers matters. The first geom layer you specify will be plotted first, the second will go on top of it, and so on.\nLet’s say you want to have a violin plot with jittered points on top. If you put geom_point() first, the points will be hidden by the violins:\n\nlibrary(palmerpenguins)\n## Warning: package 'palmerpenguins' was built under R version 4.2.3\npenguins &lt;- penguins %&gt;% drop_na(sex)\n\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_point(position = position_jitter(seed = 1234), size = 0.5) +\n  geom_violin(aes(fill = species))\n\n\n\n\n\n\n\n\nTo fix it, make sure geom_violin() comes first:\n\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_violin(aes(fill = species)) +\n  geom_point(position = position_jitter(seed = 1234), size = 0.5)\n\n\n\n\n\n\n\n\nYou saw this back in exercise 7 too when you plotted the predicted values of your regression model with geom_line() and a confidence interval with geom_ribbon(). If you put the line first, then the ribbon, the ribbon covers up the line—here that line should be bright red, but the grey of the ribbon makes it darker:\n\nlibrary(marginaleffects)\n## Warning: package 'marginaleffects' was built under R version 4.2.3\n\nmy_predictions &lt;- predictions(\n  model,\n  newdata = datagrid(median_age = seq(21, 60, by = 1),\n                     state = \"Georgia\"))\n\nggplot(my_predictions, aes(x = median_age, y = estimate)) +\n  geom_line(linewidth = 1, color = \"red\") +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.5)\n\n\n\n\n\n\n\n\nIf you put the ribbon first and then the line, the line is the correct shade of red:\n\nggplot(my_predictions, aes(x = median_age, y = estimate)) +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.5) +\n  geom_line(linewidth = 1, color = \"red\")\n\n\n\n\n\n\n\n\nThis layer order applies to annotation layers too. If you want to highlight an area of the plot (or highlight a recession, like in session 11), adding a rectangle after the geom layers will cover things up, like this ugly yellow rectangle here:\n\nggplot(penguins, aes(x = bill_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  annotate(geom = \"rect\", xmin = 40, xmax = 60, ymin = 5000, ymax = 6100,\n           fill = \"yellow\", alpha = 0.75)\n\n\n\n\n\n\n\n\nTo fix that, put that annotate() layer first, then add other geoms on top:\n\nggplot(penguins, aes(x = bill_length_mm, y = body_mass_g, color = species)) +\n  annotate(geom = \"rect\", xmin = 40, xmax = 60, ymin = 5000, ymax = 6100,\n           fill = \"yellow\", alpha = 0.75) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis doesn’t mean all annotate() layers should come first—if you want an extra label on top of a geom, make sure it comes after:\n\nggplot(penguins, aes(x = bill_length_mm, y = body_mass_g, color = species)) +\n  # Yellow rectangle behind everything\n  annotate(geom = \"rect\", xmin = 40, xmax = 60, ymin = 5000, ymax = 6100,\n           fill = \"yellow\", alpha = 0.75) +\n  # Points\n  geom_point() +\n  # Label on top of the points and the rectangle\n  annotate(geom = \"label\", x = 50, y = 5500, label = \"chonky birds\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy personal preferred general layer order\n\n\n\nWhen I make my plots, I try to keep my layers in logical groups. I’ll do my geoms and annotations first, then scale adjustments, then guide adjustments, then labels, then facets (if any), and end with theme adjustments, like this:\n\nlibrary(scales)\n\nggplot(penguins, aes(x = bill_length_mm, y = body_mass_g, color = species)) +\n  # Annotations and geoms\n  annotate(geom = \"rect\", xmin = 40, xmax = 60, ymin = 5000, ymax = 6100,\n           fill = \"yellow\", alpha = 0.75) +\n  geom_point() +\n  annotate(geom = \"label\", x = 50, y = 5500, label = \"chonky birds\") +\n  # Scale adjustments\n  scale_x_continuous(labels = label_comma(scale_cut = cut_si(\"mm\"))) +\n  scale_y_continuous(labels = label_comma(scale_cut = cut_si(\"g\"))) +\n  scale_color_viridis_d(option = \"plasma\", end = 0.6) +\n  # Guide adjustments\n  guides(color = guide_legend(title.position = \"left\")) +\n  # Labels\n  labs(x = \"Bill length\", y = \"Body mass\", color = \"Species:\",\n       title = \"Some title\", subtitle = \"Penguins!\", caption = \"Blah\") +\n  # Facets\n  facet_wrap(vars(sex)) +\n  # Theme stuff\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", size = rel(1.4)),\n        plot.caption = element_text(color = \"grey50\", hjust = 0),\n        axis.title.x = element_text(hjust = 0),\n        axis.title.y = element_text(hjust = 1),\n        strip.text = element_text(hjust = 0, face = \"bold\"),\n        legend.position = \"bottom\",\n        legend.justification = c(-0.04, 0),\n        legend.title = element_text(size = rel(0.9)))\n\n\n\n\n\n\n\n\nThis is totally arbitrary though! All that really matters is that the geoms and annotations are in the right order and that any theme adjustments you make with theme() come after a more general theme like theme_grey() or theme_minimal(), etc.. I’d recommend you figure out your own preferred style and try to stay consistent—it’ll make your life easier and more predictable.\n\n\n\n\nMy plot didn’t translate perfectly to ggplotly—why?\nIn session 10 you used ggplotly() to convert a ggplot object into an interactive plot, which I think is magical:\n\nlibrary(plotly)\n\nbasic_plot &lt;- ggplot(penguins, aes(x = bill_length_mm, y = body_mass_g, color = species)) +\n  geom_point()\n\nggplotly(basic_plot)\n\n\n\n\n\n \nHowever, lots of you discovered that Plotly does not translate everything perfectly. Plotly is a separate Javascript library and it doesn’t support every option ggplot does. ggplotly() tries its best to translate between R and Javascript, but it can’t get everything. For instance, subtitles, captions, and labels disappear:\n\nfancy_plot &lt;- ggplot(penguins, aes(x = bill_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  annotate(geom = \"label\", x = 50, y = 5500, label = \"chonky birds\") +\n  labs(title = \"Penguin bill length and weight\",\n       subtitle = \"Neato\", \n       caption = \"Here's a caption\")\n\nggplotly(fancy_plot)\n\n\n\n\n\n \nThat’s just a limitation with ggplot and plotly. If you want a perfect translation, you’ll need to hack into the guts of the translated Javascript and HTML and edit it manually to add those things.\nAlternatively, you can check out other interactive plot packages. {ggiraph} makes really great and customizable interactive plots (and it supports things like subtitles and captions and labels and other annotations ggplotly can’t), but with slightly different syntax:\n\nlibrary(ggiraph)\n## Warning: package 'ggiraph' was built under R version 4.2.3\n\nplot_thing &lt;- ggplot(data = penguins) +\n  geom_point_interactive(aes(x = bill_length_mm, y = body_mass_g, color = species,\n                             tooltip = species, data_id = species)) +\n  annotate(geom = \"label\", x = 50, y = 5500, label = \"chonky birds\") +\n  labs(title = \"Penguin bill length and weight\",\n       subtitle = \"Neato\", \n       caption = \"Here's a caption\")\n\ngirafe(ggobj = plot_thing)"
  },
  {
    "objectID": "news/2023-11-06_graphics-editors.html",
    "href": "news/2023-11-06_graphics-editors.html",
    "title": "Graphics editors",
    "section": "",
    "text": "← News\n\n\n\nFor exercise 14, mini-project 2, and your final project, you’ll create plots in R, export them as PDFs, and then edit, enhance, and lay out those PDFs using some sort of graphics editor.\nIn the past, I’ve had students use Adobe Illustrator for this, since GSU used to provide free student access to Adobe Creative Cloud, which includes Illustrator. The most common Creative Cloud programs people use for data visualization-related work are:\n\nPhotoshop: edit photos and other bitmap images\nIllustrator: edit vector images\nInDesign: lay out text and images for books, magazines, brochures, posters, and all other printed things\n\nHowever, GSU no longer provides off-campus access to Adobe software. If you’re faculty or staff, you can still access Creative Cloud for free; if you’re a student you have to use an on-campus computer lab.\nThis is sad because knowing how to use programs like Illustrator is incredibly valuable. Even if you never touch R again after this class, the graphic design programs included in Creative Cloud are industry-standard and used literally everywhere, and knowing how to use them is an important skill!\nThere are some alternative options though.\n\nCreative Cloud as a student\nYou can use the whole Creative Cloud Suite for $20/month as a student, and they have a 14-day free trial. Creative Cloud is a subscription service, so you can subscribe on and off as often as you want.\n\n\nAffinity Suite\nAffinity is a smaller rival to Adobe and they have their own set of three programs for graphic design-related tasks. Unlike Creative Cloud, the programs in the Affinity Suite are not subscription-based. You buy them and then you own them forever.\nThere are three programs that are general equivalents of the core Adobe programs:\n\nAffinity Photo = Photoshop\nAffinity Designer = Illustrator\nAffinity Publisher = InDesign\n\n\n\n\n\n\n\nCaveat!\n\n\n\nBig caveat here: I’ve never actually used any of these Affinity programs. I use Adobe stuff for all my work. But I’ve heard fantastic things about them and have seen them in action—they’re as good as Adobe’s stuff.\n\n\n\n\nOpen source alternatives\nThe open source community has created free programs that are rough equivalents of these core Adobe programs too:\n\nGIMP = Photoshop\nInkscape = Illustrator\nScribus = InDesign\n\nThese are all free and they work on macOS and Windows (and Linux if you’re into that), but they can be a little lot rough around the edges and tricky to work with. Adobe, Affinity, and other companies have full time developers focused on making good user interfaces and experiences; these open source clones do not. You can make the same kind of output with GIMP, Inkscape, and Scribus that you can with Adobe Creative Cloud and the Affinity Suite, but there’s a bit of an extra learning curve (and a lot of bumps along the way).\nBut you can’t beat free.\n\n\nSummary\n\n\n\n\n\n\n  \n    \n    \n    \n    \n  \n  \n    \n    \n      File type\n      Adobe\n      Affinity\n      Open source\n    \n  \n  \n    Bitmaps\n\nPhotoshop\n\nPhoto\n\nGIMP\n\n    Vectors\n\nIllustrator\n\nDesigner\n\nInkscape\n\n    Layout\n\nInDesign\n\nPublisher\n\nScribus\n\n    Cost\n\n$$$Monthly Creative Cloud subscription\n\n$One-time purchase\n\nFree\n\n    Notes\n\nIndustry standard\n\n\nFree, but rough learning curve"
  },
  {
    "objectID": "news/2023-11-29_faqs_week-11-12-13.html",
    "href": "news/2023-11-29_faqs_week-11-12-13.html",
    "title": "Weeks 11, 12, and 13 tips and FAQs",
    "section": "",
    "text": "← News\n\n\n\nHi everyone!\nJust a few quick tips from the past few weeks:\n\nCan I use geom_label_repel() with maps?\nYou learned about the {ggrepel} package in session 9, with its geom_text_repel() and geom_label_repel() functions that make sure none of your labels overlap:\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(ggrepel)\n\nsmall_mpg &lt;- mpg %&gt;% \n  # Only use the first 10 rows\n  slice(1:10) %&gt;% \n  # Make a label column\n  mutate(fancy_label = paste0(manufacturer, \" \", model, \" (\", year, \")\"))\n\nggplot(small_mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  geom_label_repel(aes(label = fancy_label), seed = 1234)\n\n\n\n\n\n\n\n\nIn session 12, you learned about geom_sf_text() and geom_sf_label() for adding text and labels to maps. But what if your map labels overlap, like this?\n\n# Download cb_2022_us_county_5m.zip under \"County\" from\n# https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html\nga_counties &lt;- read_sf(\"data/cb_2022_us_county_5m/cb_2022_us_county_5m.shp\") %&gt;% \n  filter(STATEFP == 13)\n\n\nga_places &lt;- tribble(\n  ~city, ~lat, ~long,\n  \"Atlanta\", 33.748955, -84.388099,\n  \"Alpharetta\", 34.075318, -84.294105,\n  \"Duluth\", 34.002262, -84.143614\n) %&gt;% \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(\"EPSG:4326\"))\n\nggplot() +\n  geom_sf(data = ga_counties, linewidth = 0.1) +\n  geom_sf(data = ga_places) +\n  geom_sf_label(data = ga_places, aes(label = city)) +\n  theme_void()\n\n\n\n\n\n\n\n\nUnfortunately there’s no such thing as geom_sf_label_repel(). BUT there’s still a way to use geom_label_repel() and geom_text_repel() with maps, with a couple little tweaks:\n\nYou have to map the geometry column in the data to the geometry aesthetic in geom_text/label_repel()\nYou have to tell geom_text/label_repel() to use the “sf_coordinates” stat so that it uses the latitude and longitude coordinates for x/y\n\n\nggplot() +\n  geom_sf(data = ga_counties, linewidth = 0.1) +\n  geom_sf(data = ga_places) +\n  geom_label_repel(\n    data = ga_places,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\", seed = 1234\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nI tried to make a map and countries are missing—why?\nMany of you were brave and made a map of refugee counts for mini project 2. That’s fantastic!\nIf you did, you likely ran into an issue with plotting the countries and getting an incomplete map. Here’s an example with our beloved gapminder data.\n\nlibrary(countrycode)  # For dealing with country names, abbreviations, and codes\nlibrary(gapminder)    # Global health and wealth\n\n# Add an ISO country code column to gapminder for joining\ngapminder_clean &lt;- gapminder %&gt;% \n  mutate(ISO_A3 = countrycode(country, \"country.name\", \"iso3c\"))\n\n# Load the world map data from exercise 12\n# Download \"Admin 0 – Countries\" from\n# https://www.naturalearthdata.com/downloads/110m-cultural-vectors/\nworld_map &lt;- read_sf(\"data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp\") %&gt;% \n  filter(ISO_A3 != \"ATA\") %&gt;%  # Bye penguins\n  mutate(ISO3 = ADM0_A3)  # Use ADM0_A3 as the main country code column\n\nLet’s take just 2007 from gapminder and map life expectancy. To do this we’ll need to combine or join the two datasets. One logical way to do this would be to take gapminder, join the world map data to it, and then plot it:\n\ngapminder_with_map &lt;- gapminder_clean %&gt;% \n  filter(year == 2007) %&gt;% \n  left_join(world_map, by = join_by(ISO3))\n\nggplot() +\n  geom_sf(data = gapminder_with_map, aes(fill = lifeExp))\n## Error in `geom_sf()`:\n## ! Problem while computing stat.\n## ℹ Error occurred in the 1st layer.\n## Caused by error in `compute_layer()`:\n## ! `stat_sf()` requires the following missing aesthetics: geometry\n\noh no there’s an error! When we joined the map data, the special attributes of the geometry column in world_map got lost. The column is still there, but it won’t automatically plot with geom_sf(). We can fix that by specifying that the column named “geometry” does indeed contain all the geographic data with st_set_geometry():\n\ngapminder_with_map &lt;- gapminder_clean %&gt;% \n  filter(year == 2007) %&gt;% \n  left_join(world_map, by = join_by(ISO3)) %&gt;% \n  # Fix the geometry column\n  st_set_geometry(\"geometry\")\n\nggplot() +\n  geom_sf(data = gapminder_with_map, aes(fill = lifeExp)) +\n  theme_void()\n\n\n\n\n\n\n\n\nWe have a… map? It’s missing a bunch of countries (Russia is the most glaringly obvious hole!). That’s because those countries aren’t in gapminder, so their corresponding maps didn’t come over when using left_join(). We can confirm by counting rows. The original map data has maps for 176 countries. Gapminder has 142 countries in 2007. The combined gapminder_with_map dataset only has 142 rows—we’re not plotting 34 countries, since they’re not in gapminder.\n\nnrow(world_map)\n## [1] 176\nnrow(gapminder_clean %&gt;% filter(year == 2007))\n## [1] 142\nnrow(gapminder_with_map)\n## [1] 142\n\nOne quick and easy way to fix this is to use two geom_sf() layers: one with the whole world and one with the partial gapminder-only map:\n\nggplot() +\n  geom_sf(data = world_map) +\n  geom_sf(data = gapminder_with_map, aes(fill = lifeExp)) +\n  theme_void()\n\n\n\n\n\n\n\n\nThe better way to fix this is to join the two datasets in a different order—start with the full map data and then add gapminder to it. This maintains the specialness of the geometry column and keeps all the original rows in world_map. For countries that are in the map data but not in gapminder, they’ll still be in the final map_with_gapminder data, but they’ll have NA for life expectancy:\n\nmap_with_gapminder &lt;- world_map %&gt;% \n  left_join(filter(gapminder_clean, year == 2007), by = join_by(ISO3))\n\nggplot() +\n  geom_sf(data = map_with_gapminder, aes(fill = lifeExp)) +\n  theme_void() +\n  # Make the countries with missing data a different color\n  scale_fill_gradient(na.value = \"grey90\")\n\n\n\n\n\n\n\n\nWhat if we want to facet though? This is just one year—what if we want to show panels for multiple years? This gets a little tricky. The gapminder data has rows for different country/year combinations (Afghanistan 1952, Afghanistan 1957, Albania 1952, etc.), but the world map data only has rows for countries. If we join the gapminder data to the world map data and gapminder has multiple rows for years, there’s no clear place for the gapminder rows to connect with the world map rows. R will try to make it work and repeat world_map rows for each of the repeated years, but it can be unpredictable.\nThe best approach I’ve found for doing this is to create what I call a “skeleton” data frame that has all the possible combinations of (1) unique countries in the map data and (2) unique years in gapminder (or the refugee data if you’re using that). The expand_grid() function does this automatically. Like, look what happens if we tell it to make rows for every combination of A, B, C and 1, 2, 3—we get A1, A2, A3, B1, B2, and so on:\n\nexpand_grid(column_1 = c(\"A\", \"B\", \"C\"),\n            column_2 = c(1, 2, 3))\n## # A tibble: 9 × 2\n##   column_1 column_2\n##   &lt;chr&gt;       &lt;dbl&gt;\n## 1 A               1\n## 2 A               2\n## 3 A               3\n## 4 B               1\n## 5 B               2\n## 6 B               3\n## 7 C               1\n## 8 C               2\n## 9 C               3\n\nWe’ll make a similar skeleton with all the countries in the map and all the years we care about in gapminder. We’ll just show two panels—1952 and 2007—so we’ll make a little filtered dataset first. Then we’ll use expand_grid() to make a dataset with all those combinations: Afghanistan 1952, Afghanistan 2007, Albania 1952, Albania 2007, and so on:\n\ngapminder_smaller &lt;- gapminder_clean %&gt;%\n  filter(year %in% c(1952, 2007))\n\nskeleton &lt;- expand_grid(ISO3 = unique(world_map$ISO3),\n                        year = unique(gapminder_smaller$year))\nskeleton\n## # A tibble: 352 × 2\n##    ISO3   year\n##    &lt;chr&gt; &lt;int&gt;\n##  1 FJI    1952\n##  2 FJI    2007\n##  3 TZA    1952\n##  4 TZA    2007\n##  5 SAH    1952\n##  6 SAH    2007\n##  7 CAN    1952\n##  8 CAN    2007\n##  9 USA    1952\n## 10 USA    2007\n## # ℹ 342 more rows\n\nNeat, that works. There’s Fiji in 1952 and 2007, Tanzania in 1952 and 2007, and so on. Those are all the possible countries in world_map with all the possible years in gapminder_smaller.\nNext we can join in the gapminder data for each country and year, and join in the map data for each country. Notice how it has the same number of rows as skeleton (352). If a country doesn’t have gapminder data (like Fiji here), it gets an NA for lifeExp and pop and gdpPercap. But it still has map data for both 1952 and 2007, so it’ll show up in a plot.\n\nfull_gapminder_map &lt;- skeleton %&gt;% \n  left_join(gapminder_smaller, by = join_by(ISO3, year)) %&gt;%\n  left_join(world_map, by = join_by(ISO3)) %&gt;% \n  # The geometry column lost its magic powers after joining, so add it back\n  st_set_geometry(\"geometry\")\nfull_gapminder_map\n## Simple feature collection with 352 features and 175 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -180 ymin: -55.61183 xmax: 180 ymax: 83.64513\n## Geodetic CRS:  WGS 84\n## # A tibble: 352 × 176\n##    ISO3   year country  continent lifeExp     pop gdpPercap featurecla scalerank\n##    &lt;chr&gt; &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;       &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;          &lt;int&gt;\n##  1 FJI    1952 &lt;NA&gt;     &lt;NA&gt;         NA   NA            NA  Admin-0 c…         1\n##  2 FJI    2007 &lt;NA&gt;     &lt;NA&gt;         NA   NA            NA  Admin-0 c…         1\n##  3 TZA    1952 Tanzania Africa       41.2  8.32e6      717. Admin-0 c…         1\n##  4 TZA    2007 Tanzania Africa       52.5  3.81e7     1107. Admin-0 c…         1\n##  5 SAH    1952 &lt;NA&gt;     &lt;NA&gt;         NA   NA            NA  Admin-0 c…         1\n##  6 SAH    2007 &lt;NA&gt;     &lt;NA&gt;         NA   NA            NA  Admin-0 c…         1\n##  7 CAN    1952 Canada   Americas     68.8  1.48e7    11367. Admin-0 c…         1\n##  8 CAN    2007 Canada   Americas     80.7  3.34e7    36319. Admin-0 c…         1\n##  9 USA    1952 United … Americas     68.4  1.58e8    13990. Admin-0 c…         1\n## 10 USA    2007 United … Americas     78.2  3.01e8    42952. Admin-0 c…         1\n## # ℹ 342 more rows\n## # ℹ 167 more variables: LABELRANK &lt;int&gt;, SOVEREIGNT &lt;chr&gt;, SOV_A3 &lt;chr&gt;,\n## #   ADM0_DIF &lt;int&gt;, LEVEL &lt;int&gt;, TYPE &lt;chr&gt;, TLC &lt;chr&gt;, ADMIN &lt;chr&gt;,\n## #   ADM0_A3 &lt;chr&gt;, GEOU_DIF &lt;int&gt;, GEOUNIT &lt;chr&gt;, GU_A3 &lt;chr&gt;, SU_DIF &lt;int&gt;,\n## #   SUBUNIT &lt;chr&gt;, SU_A3 &lt;chr&gt;, BRK_DIFF &lt;int&gt;, NAME &lt;chr&gt;, NAME_LONG &lt;chr&gt;,\n## #   BRK_A3 &lt;chr&gt;, BRK_NAME &lt;chr&gt;, BRK_GROUP &lt;chr&gt;, ABBREV &lt;chr&gt;, POSTAL &lt;chr&gt;,\n## #   FORMAL_EN &lt;chr&gt;, FORMAL_FR &lt;chr&gt;, NAME_CIAWF &lt;chr&gt;, NOTE_ADM0 &lt;chr&gt;, …\n\nNow we can plot it and we’ll have consistent countries in each panel:\n\nggplot() +\n  geom_sf(data = full_gapminder_map, aes(fill = lifeExp)) +\n  facet_wrap(vars(year), ncol = 1) +\n  scale_fill_gradient(na.value = \"grey90\") +\n  theme_void()\n\n\n\n\n\n\n\n\nPerfect!\n\n\nSome of the words in my word frequency/tf-idf plot were out of order—how can I fix that?\nIn the example for week 13, I showed the 15 most frequent words in Hamlet, Macbeth, Romeo and Juliet, and King Lear, faceted by play. Only Romeo and Juliet, though, has the words in the correct order. The other plays have strange ordering. Note how “lord” and “king” are weirdly misplaced in Macbeth and Hamlet and how “love” is weirdly misplaced in Hamlet:\n\n\n\nWords in the wrong order across panels\n\n\nThe word “lord” is the second most common word in Hamlet, so R thinks it is the second most common word across all the plays. It doesn’t know that there’s a difference between “lord” in Hamlet and “lord” in Macbeth. As a result, any common words that are shared across the plays will appear out of order.\nThis is fixable though! See this blog post by Julia Silge, one of the authors of {tidytext}. Basically, you need to use reorder_within() to sort the words correctly inside each play, then add scale_y_reordered() to make them display correctly.\nHere’s what that looks like with the Shakespeare words.\n\nlibrary(tidyverse)\nlibrary(gutenbergr)  # For getting books from Project Gutenberg\nlibrary(tidytext)    # For working with text\n\n\ntragedies_raw &lt;- gutenberg_download(\n  c(\n    1524,  # Hamlet\n    1532,  # King Lear\n    1533,  # Macbeth\n    1513   # Romeo and Juliet\n  ),\n  meta_fields = \"title\"\n)\n\n\n# Clean up the tragedies text\ntop_words_tragedies &lt;- tragedies_raw %&gt;% \n  drop_na(text) %&gt;% \n  unnest_tokens(word, text) %&gt;% \n  # Remove stop words\n  anti_join(stop_words) %&gt;% \n  # Get rid of old timey words and stage directions\n  filter(!(word %in% c(\"thou\", \"thy\", \"haue\", \"thee\", \n                      \"thine\", \"enter\", \"exeunt\", \"exit\"))) %&gt;% \n  # Count all the words in each play\n  count(title, word, sort = TRUE) %&gt;% \n  # Keep top 15 in each play\n  group_by(title) %&gt;% \n  top_n(15) %&gt;% \n  ungroup()\ntop_words_tragedies\n## # A tibble: 62 × 3\n##    title                     word           n\n##    &lt;chr&gt;                     &lt;chr&gt;      &lt;int&gt;\n##  1 Hamlet, Prince of Denmark hamlet       461\n##  2 Romeo and Juliet          romeo        300\n##  3 Macbeth                   macbeth      282\n##  4 The Tragedy of King Lear  lear         229\n##  5 Hamlet, Prince of Denmark lord         223\n##  6 Hamlet, Prince of Denmark king         196\n##  7 Romeo and Juliet          juliet       178\n##  8 The Tragedy of King Lear  kent         170\n##  9 The Tragedy of King Lear  gloucester   169\n## 10 Hamlet, Prince of Denmark horatio      156\n## # ℹ 52 more rows\n\nBecause we used top_n(), these words are already sorted in order of frequency (with “hamlet” appearing the most at 461 times). In example 13, we locked in that order by making the word column an ordered factor, like this:\n\ntop_words_tragedies_order_wrong &lt;- top_words_tragedies %&gt;%\n  # Make the words an ordered factor so they plot in order\n  mutate(word = fct_inorder(word)) \n\nggplot(top_words_tragedies_order_wrong, aes(y = fct_rev(word), x = n, fill = title)) + \n  geom_col() + \n  guides(fill = \"none\") +\n  labs(y = \"Count\", x = NULL, \n       title = \"15 most frequent words in four Shakespearean tragedies\") +\n  facet_wrap(vars(title), scales = \"free_y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nBut that’s wrong!\nInstead of using fct_inorder(), we need to use reorder_within() and tell it to sort the words by count within each play:\n\ntop_words_tragedies_order_right &lt;- top_words_tragedies %&gt;%\n  # Make the words an ordered factor so they plot in order\n  mutate(word = reorder_within(word, n, title)) \n\ntop_words_tragedies_order_right\n## # A tibble: 62 × 3\n##    title                     word                                      n\n##    &lt;chr&gt;                     &lt;fct&gt;                                 &lt;int&gt;\n##  1 Hamlet, Prince of Denmark hamlet___Hamlet, Prince of Denmark      461\n##  2 Romeo and Juliet          romeo___Romeo and Juliet                300\n##  3 Macbeth                   macbeth___Macbeth                       282\n##  4 The Tragedy of King Lear  lear___The Tragedy of King Lear         229\n##  5 Hamlet, Prince of Denmark lord___Hamlet, Prince of Denmark        223\n##  6 Hamlet, Prince of Denmark king___Hamlet, Prince of Denmark        196\n##  7 Romeo and Juliet          juliet___Romeo and Juliet               178\n##  8 The Tragedy of King Lear  kent___The Tragedy of King Lear         170\n##  9 The Tragedy of King Lear  gloucester___The Tragedy of King Lear   169\n## 10 Hamlet, Prince of Denmark horatio___Hamlet, Prince of Denmark     156\n## # ℹ 52 more rows\n\nNotice how the word column looks a little weird now. It added the play name to the end of each word, like macbeth___Macbeth. That’s actually a creative hack for fixing the ordering. Remember that the main reason the ordering is messed up across facets is because R doesn’t know that the word “love” in Hamlet is different from the word “love” in Romeo and Juliet. By changing the words to love___Romeo and Juliet and love___Hamlet, R can now recognize the different versions of “love” and sort them correctly. Let’s plot this version:\n\nggplot(top_words_tragedies_order_right, aes(y = word, x = n, fill = title)) + \n  geom_col() + \n  guides(fill = \"none\") +\n  labs(y = \"Count\", x = NULL, \n       title = \"15 most frequent words in four Shakespearean tragedies\") +\n  facet_wrap(vars(title), scales = \"free_y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\noh no.\nThe order is right (yay!) but the y-axis is horrible since it’s including the hacky ___play name at the end of each of the words.\nTo fix that, we can use scale_y_reordered(), which cleans up those word labels by removing the three underscores and any text that follows them:\n\nggplot(top_words_tragedies_order_right, aes(y = word, x = n, fill = title)) + \n  geom_col() + \n  scale_y_reordered() +\n  guides(fill = \"none\") +\n  labs(y = \"Count\", x = NULL, \n       title = \"15 most frequent words in four Shakespearean tragedies\") +\n  facet_wrap(vars(title), scales = \"free_y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nPerfect!\n\n\nCleaning up text is always specific and specialized\nIn the Shakespeare example, we removed common stop words like “the” and “a” with anti_join() and then manually removed some other more specific words like “thou” and “thee” and “exit”:\n\n# Clean up the tragedies text\ntop_words_tragedies &lt;- tragedies_raw %&gt;% \n  drop_na(text) %&gt;% \n  unnest_tokens(word, text) %&gt;% \n  # Remove stop words\n  anti_join(stop_words) %&gt;% \n  # Get rid of old timey words and stage directions\n  filter(!(word %in% c(\"thou\", \"thy\", \"haue\", \"thee\", \n                       \"thine\", \"enter\", \"exeunt\", \"exit\")))\n\nThat’s because in these specific plays, those are common words that we want to ignore—they’re basically our own custom stop words. We should also probably get rid of words like “act” and “scene” too, but we didn’t here.\nMany of you kept that exact code in exercise 13, removing “thou”, “thy”, “exeunt”, and those other words from your own text. But that’s not necessary or helpful. If you’re working with something like Harry Potter or Jane Austen or Ernest Hemmingway or anything more modern than Shakespeare, those words aren’t really in there. In the Shakespeare example, we removed “enter” and “exit” because those are stage directions, but in other books those are regular actual words and probably shouldn’t be removed.\nThere’s no one universal set of stop words that you can use—every text is unique and has its own quirks that you need to take care of.\nFor example, one of you looked at four books by W. E. B. Du Bois and did this to clean up the stop words:\n\ndubois_clean %&gt;%\n  anti_join(stop_words) %&gt;% \n  filter(!(word %in% c(\"1\", \"2\", \"cong\", \"sess\", \"act\", \"pp\", \"_ibid\",\n                       \"_house\", \"3\", \"doc\")))\n\nThat’s awesome. Those are all words that are specific to those four books and that were likely appearing in the frequency plot. One (or more) of the books probably mentioned lots of congressional activity, like congressional sessions, acts of congress, stuff happening in the House of Representatives, and so on. There were probably also a lot of citations, with things like “pp.” (the abbreviation for “pages”, like “pp. 125-127”) and “ibid” (the abbreviation for “see the previous citation”). That list of words is specific to those four books and should not be applied to other books—like, there’s no reason to remove those words from the Shakespeare tragedies or from Little Women or from Harry Potter because none of those mention congressional sessions or use “ibid”.\nData cleaning is always context specific.\n\n\nSaving data that takes a long time to make\nIn these later sessions, I’ve had you do things with data from different places on the internet. In exercise 13 you grabbed books from Project Gutenberg. Some of you used osrmRoute() in exercise 12 to create a mapped route between cities. Some of you used {tidygeocoder} to geocode addresses in exercise 12. In past sessions you’ve used WDI() to download data from the World Bank.\nWhen you knit a document, R starts with a brand new empty session without any packages or data loaded, and then it runs all your chunks to load packages, load data, and run all your other code. If you have code that grabs data from the internet, it will run every time you knit your document. Remember my suggestion to knit often? You’ll re-download the data, re-create routes, re-geocode addresses, and so on every time you keep re-knitting. This is excessive, slow, and—most especially—bad R etiquette. You don’t want to keep accessing those servers and recalculate things and redownload things you don’t need to update.\nBUT at the same time, you should care about reproducibility. You want others—and future you—to be able to run your code and create the same plots and tables and get the same data. But you don’t want to do all that excessively and impolitely.\nThe solution is to be a little tricky with your R Markdown file. If you have code that needs to grab something from the internet, put it in a chunk that doesn’t run—use eval=FALSE in its chunk options. Then, in an invisible chunk (with include=FALSE) load the pre-downloaded data manually. I showed this in example 8 (with {WDI}) and example 11 (with {tidyquant}) and example 13 (with {gutenberger})\nHere’s a quick basic example with Project Gutenberg book data. There are two chunks: get-book-fake and load-book-real:\n```{r get-book-fake, eval=FALSE}\nlittle_women_raw &lt;- gutenberg_download(514, meta_fields = \"title\")\n```\n\n```{r load-book-data-real, include=FALSE}\nlittle_women_file &lt;- \"data/little_women_raw.csv\"\n\nif (file.exists(little_women_file)) {\n  little_women_raw &lt;- read_csv(little_women_file)\n} else {\n  little_women_raw &lt;- gutenberg_download(514, meta_fields = \"title\")\n  \n  write_csv(little_women_raw, little_women_file)\n}\n```\n\nThe first chunk (get-book-fake) contains the code for downloading data with gutenberg_download(). It will appear in the document, but it will not run since it has eval=FALSE on. It will not try to grab anything from the internet. If someone were to follow along with the code in your document, they could run that code and get the book (good!), and it won’t run repeatedly on your end (also good!).\nThe second chunk (load-book-data-real) does a neat little trick. It first checks to see if a CSV file named data/little_women_raw.csv exists. If it does, it’ll just load it with read_csv(). If it doesn’t, it’ll grab data from the internet with gutenberg_download() and then it will save that as data/little_women_raw.csv. This is really neat stuff. If you’re knitting your document for the first time, you won’t have the Little Women data yet, so the code will connect to Project Gutenberg and get it. The code will then save that data to your computer as a CSV file. The next time you knit, R won’t need to connect to the internet again—it’ll load the CSV file instead of grabbing it from Project Gutenberg. You can knit as many times as you want—you won’t need to reconnect to any remote servers again.\n\nAgain, the general pattern for this is to create two chunks: (1) a fake one that people will see in the document but won’t run, and (2) a real one that will run and load data locally if it exists, but that people won’t see.\n\n\n\n\n\n\nHow should you save your intermediate data?\n\n\n\nIn the example above, I saved the Little Women data from Project Gutenberg as a CSV file. This is fine—a CSV file is plain text, so it can store any kind of text-based data like numbers and text without any problems.\nBut sometimes you’ll work with slightly more complex types of data. For instance, with geographic data, the magical geometry column contains a whole host of extra metadata, like projection details and multiple points (if it’s something like country boundaries). If you save a data frame with a geometry column as a CSV file you’ll lose all that data—CSVs can’t store that extra nested metadata.\nSimilarly, if you have a factor or categorical variable (i.e. something like “Strongly disagree”, “Disagree”, “Agree”, “Strongly agree”), behind the scenes R stores those as numbers (i.e. 1, 2, 3, 4) with labels attached to the numbers (1 = “Strongly disagree”, 2 = “Disagree”, etc.). If you save a data frame with a categorical column like that as a CSV, by default R will only store the numbers and you’ll lose the labels. You could convert the categorical column to text before saving as CSV and then the text labels would get stored, but if the variable is ordered (i.e. “Strongly disagree” is lower than “disagree”, etc.), you’ll lose that ordering when saving as CSV.\nThe safest way to save intermediate files like this is to actually not use CSV, but instead use a special kind of file called .rds, which lets you take an entire object from your Environment panel and save it as a file. The .rds file will keep all the extra metadata and attributes (i.e. the projection details and nested points inside a geometry column; factor labels and ordering for categorical variables, and so on).\nSo instead of saving that Little Women book as a CSV file, the better approach is to use saveRDS() and readRDS() to store it and load it as an .rds file, like this:\n```{r get-book-fake, eval=FALSE}\nlittle_women_raw &lt;- gutenberg_download(514, meta_fields = \"title\")\n```\n\n```{r load-book-data-real, include=FALSE}\nlittle_women_file &lt;- \"data/little_women_raw.rds\"\n\nif (file.exists(little_women_file)) {\n  little_women_raw &lt;- readRDS(little_women_file)\n} else {\n  little_women_raw &lt;- gutenberg_download(514, meta_fields = \"title\")\n  \n  saveRDS(little_women_raw, little_women_file)\n}\n```\n\n\n\n\n\n\n\n\nThe fancy pro version of all this\n\n\n\nIf you want to be super cool, check out the {targets} package, which is like the professional version of this approach to caching data. {targets} lets you keep track of all your different objects and it will only re-run stuff if absolutely necessary.\nFor instance, imagine that in a document you load data, clean it, and plot it. Standard stuff. There’s a linear relationship between all this—the raw data leads to the clean data, which leads to a plot. If you change code in your plot, the data cleaning and loading code didn’t change, so there’s no real reason to need to re-run it. If you change your data cleaning code, your downstream plot will be affected and its code would need to be re-run.\n{targets} keeps track of all these dependencies and re-runs code only when there are upstream changes. It’s great for plots and models that take a long time to run, or for grabbing data from the internet.\nThe best way to learn {targets} is to play with their short little walkthrough tutorial here, which has you make a simple document that loads data, builds a regression model, and makes a plot.\nI use {targets} for all my projects (including this course website!) and it makes life a ton easier for any kind of project that involves more than one .Rmd file or R script (see this for an example). I highly recommend checking it out."
  },
  {
    "objectID": "news/2023-12-13_what-next.html",
    "href": "news/2023-12-13_what-next.html",
    "title": "What comes next?",
    "section": "",
    "text": "← News\nHi everyone!\nI’ve finished grading all your final projects and have submitted your grades. I’m so proud of how much you all accomplished!\nA bunch of you have asked me what other R-related classes you can take. If you’re a grad student, good news—there are a bunch!1 The Andrew Young School has a whole policy analytics initiative and online graduate certificate, with classes like these:\nI also teach a class called Evaluation Research (PMAP 8521) in the spring. It’s a really fun and useful class (I’ve had students get jobs because of it!). It’s basically a class in econometrics, or econ-flavored statistics focused on causation.\nIn the class you’ll learn all about causal inference, or how to legally claim causation with statistics. In your past classes you were always taught “correlation isn’t causation,” which is mostly true, except when it’s not. In PMAP 8521 you’ll get to legitimately make causal claims. We’ll cover fun tools like directed acyclic graphs (DAGs), randomized controlled trials, difference-in-differences analysis, regression discontinuity analysis, and instrumental variables.\nIt’s all in R, too, so you’ll learn how to do fancy stats in addition to making beautiful graphics. The R part is why former students have found jobs—tons of organizations are looking for R skills nowadays.\nIf you can interpret regression results and if you know what statistical significance means, you have the stats background for the class.\nYou can see the website from the spring 2023 version of the class here (the 2024 one will be roughly the same at evalsp24 dot classes dot…), and you can see that it’s full of videos and interactive code tutorials and other resources. It’s an in-person class, but because we’re still in a pandemic, I use a flipped-classroom approach and have students watch all the lecture videos before class, and we spend class time working on R stuff together in person.\nHave a great break!"
  },
  {
    "objectID": "news/2023-12-13_what-next.html#footnotes",
    "href": "news/2023-12-13_what-next.html#footnotes",
    "title": "What comes next?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you’re an undergrad, there’s not as much available; but you should consider graduate school at the Andrew Young School to get more policy analytics in the future!↩︎"
  },
  {
    "objectID": "resource/citations.html",
    "href": "resource/citations.html",
    "title": "Citations and bibliography",
    "section": "",
    "text": "You can access a shared group Zotero library of all the non-web-based class reading references and link it to your Zotero account to easily cite the course materials.\nAlternatively, you can download a BibTeX file of all the references. You can open it in BibDesk on macOS, JabRef on Windows, or Zotero on macOS, Windows, and online.\n\n  Zotero group library     BibTeX file"
  },
  {
    "objectID": "resource/design.html",
    "href": "resource/design.html",
    "title": "Design",
    "section": "",
    "text": "Vischeck: Simulate how your images look for people with different forms of colorblindness (web-based)\nColor Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)"
  },
  {
    "objectID": "resource/design.html#accessibility",
    "href": "resource/design.html#accessibility",
    "title": "Design",
    "section": "",
    "text": "Vischeck: Simulate how your images look for people with different forms of colorblindness (web-based)\nColor Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)"
  },
  {
    "objectID": "resource/design.html#colors",
    "href": "resource/design.html#colors",
    "title": "Design",
    "section": "Colors",
    "text": "Colors\n\nAdobe Color: Create, share, and explore rule-based and custom color palettes.\nColourLovers: Like Facebook for color palettes.\nCoolors: Generate random palettes that look great.\nviridis: Perceptually uniform color scales.\nScientific Colour-Maps: Perceptually uniform color scales like viridis. Use them in R with {scico}.\nColorBrewer: Sequential, diverging, and qualitative color palettes that take accessibility into account.\nCARTOColors: More sequential, diverging, and qualitative color palettes that take accessibility into account. Use them with {rcartocolor}.\nPaletteer: The {paletteer} package for R includes hundreds of different palettes.\nHCL palettes: The {colorspace} package for R includes a ton of qualitative, sequential, and diverging HCL (hue-chroma-luminance) palettes that use fancy mathematical rules that maintain perceptual distance\nColorgorical: Create color palettes based on fancy mathematical rules for perceptual distance.\nColorpicker for data: More fancy mathematical rules for color palettes (explanation).\niWantHue: Yet another perceptual distance-based color palette builder.\nPhotochrome: Word-based color palettes.\nPolicyViz Design Color Tools: Large collection of useful color resources"
  },
  {
    "objectID": "resource/design.html#fonts",
    "href": "resource/design.html#fonts",
    "title": "Design",
    "section": "Fonts",
    "text": "Fonts\n\nGoogle Fonts: Huge collection of free, well-made fonts.\nThe Ultimate Collection of Google Font Pairings: A list of great, well-designed font pairings from all those fonts hosted by Google (for when you’re looking for good contrasting or complementary fonts)."
  },
  {
    "objectID": "resource/design.html#graphic-assets",
    "href": "resource/design.html#graphic-assets",
    "title": "Design",
    "section": "Graphic assets",
    "text": "Graphic assets\n\nImages\n\nUse the Creative Commons filters on Google Images or Flickr\nUnsplash\nPexels\nPixabay\nStockSnap.io\nBurst\nfreephotos.cc\n\n\n\nVectors\n\nNoun Project: Thousands of free simple vector images\naiconica: 1,000+ vector icons\nVecteezy: Thousands of free vector images\n\n\n\nVectors, photos, videos, and other assets\n\nStockio"
  },
  {
    "objectID": "resource/index.html",
    "href": "resource/index.html",
    "title": "Helpful resources",
    "section": "",
    "text": "I have included a bunch of extra resources and guides related to graphic design, visualization, R, data, and other relevant topics. Enjoy!"
  },
  {
    "objectID": "resource/markdown.html",
    "href": "resource/markdown.html",
    "title": "Using Markdown",
    "section": "",
    "text": "Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)"
  },
  {
    "objectID": "resource/markdown.html#math",
    "href": "resource/markdown.html#math",
    "title": "Using Markdown",
    "section": "Math",
    "text": "Math\nMarkdown uses LaTeX to create fancy mathematical equations. There are like a billion little options and features available for math equations—you can find helpful examples of the the most common basic commands here.\nYou can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like $y = mx + b$:\n\n\n\n\n\n\nInline math\n\n\n\nType…\nBased on the DAG, the regression model for estimating the effect of education on wages\nis $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon$, or $\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon$.\n…to get…\n\nBased on the DAG, the regression model for estimating the effect of education on wages is \\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\\), or \\(\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\\)\n\n\n\nTo put an equation on its own line in a display block, wrap it in double dollar signs, like this:\n\n\n\n\n\n\nBlock math\n\n\n\nType…\nThe quadratic equation was an important part of high school math:\n\n$$\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n$$\n\nBut now we just use computers to solve for $x$.\n…to get…\n\nThe quadratic equation was an important part of high school math:\n\\[\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n\\]\nBut now we just use computers to solve for \\(x\\).\n\n\n\n\nBecause dollar signs are used to indicate math equations, you can’t just use dollar signs like normal if you’re writing about actual dollars. For instance, if you write This book costs $5.75 and this other costs $40, Markdown will treat everything that comes between the dollar signs as math, like so: “This book costs \\(5.75 and this other costs \\)40”.\nTo get around that, put a backslash (\\) in front of the dollar signs, so that This book costs \\$5.75 and this other costs \\$40 becomes “This book costs $5.75 and this other costs $40”."
  },
  {
    "objectID": "resource/markdown.html#tables",
    "href": "resource/markdown.html#tables",
    "title": "Using Markdown",
    "section": "Tables",
    "text": "Tables\nThere are 4 different ways to hand-create tables in Markdown—I say “hand-create” because it’s normally way easier to use R to generate these things with packages like {gt} or {knitr} or {kableExtra}. The two most common are simple tables and pipe tables. You should look at the full documentation here.\n\n\n\n\n\n\nSimple tables\n\n\n\nFor simple tables, type…\n  Right     Left     Center     Default\n-------     ------ ----------   -------\n     12     12        12            12\n    123     123       123          123\n      1     1          1             1\n\nTable: Caption goes here\n…to get…\n\nCaption goes here\n\n\nRight\nLeft\nCenter\nDefault\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\nPipe tables\n\n\n\nFor pipe tables, type…\n| Right | Left | Default | Center |\n|------:|:-----|---------|:------:|\n|   12  |  12  |    12   |    12  |\n|  123  |  123 |   123   |   123  |\n|    1  |    1 |     1   |     1  |\n\nTable: Caption goes here\n…to get…\n\nCaption goes here\n\n\nRight\nLeft\nDefault\nCenter\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1"
  },
  {
    "objectID": "resource/markdown.html#footnotes",
    "href": "resource/markdown.html#footnotes",
    "title": "Using Markdown",
    "section": "Footnotes",
    "text": "Footnotes\nThere are two different ways to add footnotes (see here for complete documentation): regular and inline.\nRegular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like [^1], but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:\n\n\n\n\n\n\nFootnotes\n\n\n\nType…\nHere is a footnote reference[^1] and here is another [^note-on-dags].\n\n[^1]: This is a note.\n\n[^note-on-dags]: DAGs are neat. \n\nAnd here's more of the document.\n…to get…\n\nHere is a footnote reference1 and here is another.2\nAnd here’s more of the document.\n\n\n\n\n\nThis is a note.↩︎\n\n\n\n\nDAGs are neat.↩︎\n\n\n\n\n\n\n\nYou can also use inline footnotes with ^[Text of the note goes here], which are often easier because you don’t need to worry about identifiers:\n\n\n\n\n\n\nInline footnotes\n\n\n\nType…\nCausal inference is neat.^[But it can be hard too!]\n…to get…\n\nCausal inference is neat.1\n\n\n\n\n\nBut it can be hard too!↩︎"
  },
  {
    "objectID": "resource/markdown.html#front-matter",
    "href": "resource/markdown.html#front-matter",
    "title": "Using Markdown",
    "section": "Front matter",
    "text": "Front matter\nYou can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named YAML (or “YAML Ain’t Markup Language”) that follows this basic outline: setting: value for setting. Here’s an example YAML metadata section. Note that it must start and end with three dashes (---).\n---\ntitle: Title of your document\ndate: \"January 13, 2020\"\nauthor: \"Your name\"\n---\nYou can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you’re using has a colon (:) in it, it’ll confuse Markdown since it’ll be something like title: My cool title: a subtitle, which has two colons. It’s better to do this:\n---\ntitle: \"My cool title: a subtitle\"\n---\nIf you want to use quotes inside one of the values (e.g. your document is An evaluation of \"scare quotes\"), you can use single quotes instead:\n---\ntitle: 'An evaluation of \"scare quotes\"'\n---"
  },
  {
    "objectID": "resource/markdown.html#citations",
    "href": "resource/markdown.html#citations",
    "title": "Using Markdown",
    "section": "Citations",
    "text": "Citations\nOne of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a BibTeX file (ends in .bib) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like BibDesk on macOS), or you can use Zotero (macOS and Windows) to export a .bib file. You can download an example .bib file of all the readings from this class here.\nComplete details for using citations can be found here. In brief, you need to do three things:\n\nAdd a bibliography: entry to the YAML metadata:\n---\ntitle: Title of your document\ndate: \"January 13, 2020\"\nauthor: \"Your name\"\nbibliography: name_of_file.bib\n---\nChoose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ at this repository. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):\n---\ntitle: Title of your document\ndate: \"January 13, 2020\"\nauthor: \"Your name\"\nbibliography: name_of_file.bib\ncsl: \"https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\"\n---\nSome of the most common CSLs are:\n\nChicago author-date\nChicago note-bibliography\nChicago full note-bibliography (no shortened notes or ibids)\nAPA 7th edition\nMLA 8th edition\n\nCite things in your document. Check the documentation for full details of how to do this. Essentially, you use @citationkey inside square brackets ([]):\n\n\n\n\n\n\n\nType…\n…to get…\n\n\n\n\nCausal inference is neat [@Rohrer:2018;\n@AngristPischke:2015].\nCausal inference is neat (Rohrer 2018; Angrist and Pischke 2015).\n\n\nCausal inference is neat [see @Rohrer:2018, p. 34;\nalso @AngristPischke:2015, chapter 1].\nCausal inference is neat (see Rohrer 2018, 34; also Angrist and Pischke 2015, chap. 1).\n\n\nAngrist and Pischke say causal inference is neat\n[-@AngristPischke:2015; see also @Rohrer:2018].\nAngrist and Pischke say causal inference is neat (2015; see also Rohrer 2018).\n\n\n@AngristPischke:2015 [chapter 1] say causal\ninference is neat, and @Rohrer:2018 agrees.\nAngrist and Pischke (2015, chap. 1) say causal inference is neat, and Rohrer (2018) agrees.\n\n\n\nAfter compiling, you should have a perfectly formatted bibliography added to the end of your document too:\n\nAngrist, Joshua D., and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nRohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629."
  },
  {
    "objectID": "resource/markdown.html#other-references",
    "href": "resource/markdown.html#other-references",
    "title": "Using Markdown",
    "section": "Other references",
    "text": "Other references\nThese websites have additional details and examples and practice tools:\n\nCommonMark’s Markdown tutorial: A quick interactive Markdown tutorial.\nMarkdown tutorial: Another interactive tutorial to practice using Markdown.\nMarkdown cheatsheet: Useful one-page reminder of Markdown syntax.\nThe Plain Person’s Guide to Plain Text Social Science: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown."
  },
  {
    "objectID": "resource/rmarkdown.html",
    "href": "resource/rmarkdown.html",
    "title": "Using R Markdown",
    "section": "",
    "text": "R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown. This whole course website is created with a fancier version of R Markdown named Quarto.\nThe documentation for R Markdown is extremely comprehensive, and their tutorials and cheatsheets are excellent—rely on those.\nHere are the most important things you’ll need to know about R Markdown in this class:"
  },
  {
    "objectID": "resource/rmarkdown.html#key-terms",
    "href": "resource/rmarkdown.html#key-terms",
    "title": "Using R Markdown",
    "section": "Key terms",
    "text": "Key terms\n\nDocument: A Markdown file where you type stuff\nChunk: A piece of R code that is included in your document. It looks like this:\n\n```{r}\n# Code goes here\n```\n\nThere must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.\nKnit: When you “knit” a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through pandoc to convert it to HTML or PDF or Word (or whatever output you’ve selected).\nYou can knit by clicking on the “Knit” button at the top of the editor window, or by pressing ⌘⇧K on macOS or control + shift + K on Windows."
  },
  {
    "objectID": "resource/rmarkdown.html#add-chunks",
    "href": "resource/rmarkdown.html#add-chunks",
    "title": "Using R Markdown",
    "section": "Add chunks",
    "text": "Add chunks\nThere are three ways to insert chunks:\n\nPress ⌘⌥I on macOS or control + alt + I on Windows\nClick on the “Insert” button at the top of the editor window\n\n\n\n\n\n\n\n\n\nManually type all the backticks and curly braces (don’t do this)"
  },
  {
    "objectID": "resource/rmarkdown.html#chunk-names",
    "href": "resource/rmarkdown.html#chunk-names",
    "title": "Using R Markdown",
    "section": "Chunk names",
    "text": "Chunk names\nYou can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they’ll appear in the list. If you don’t include a name, the chunk will still show up, but you won’t know what it does.\n\n\n\n\n\n\n\n\n\nTo add a name, include it immediately after the {r in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. All chunk names in your document must be unique.\n\n```{r name-of-this-chunk}\n# Code goes here\n```"
  },
  {
    "objectID": "resource/rmarkdown.html#chunk-options",
    "href": "resource/rmarkdown.html#chunk-options",
    "title": "Using R Markdown",
    "section": "Chunk options",
    "text": "Chunk options\nThere are a bunch of different options you can set for each chunk. You can see a complete list in the RMarkdown Reference Guide or at {knitr}’s website.\nOptions go inside the {r} section of the chunk:\n\n```{r a-named-chunk-with-options, warning=FALSE, message=FALSE}\n# Code goes here\n```\n\nThe most common chunk options are these:\n\nfig.width=5 and fig.height=3 (or whatever number you want): Set the dimensions for figures\necho=FALSE: The code is not shown in the final document, but the results are\nmessage=FALSE: Any messages that R generates (like all the notes that appear after you load a package) are omitted\nwarning=FALSE: Any warnings that R generates are omitted\ninclude=FALSE: The chunk still runs, but the code and results are not included in the final document\n\nYou can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:"
  },
  {
    "objectID": "resource/rmarkdown.html#inline-chunks",
    "href": "resource/rmarkdown.html#inline-chunks",
    "title": "Using R Markdown",
    "section": "Inline chunks",
    "text": "Inline chunks\nYou can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use `r r_code_here`.\nIt’s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…\n```{r find-avg-mpg, echo=FALSE}\navg_mpg &lt;- mean(mtcars$mpg)\n```\n\nThe average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon.\n… would knit into this:\n\nThe average fuel efficiency for cars from 1974 was 20.1 miles per gallon."
  },
  {
    "objectID": "resource/rmarkdown.html#output-formats",
    "href": "resource/rmarkdown.html#output-formats",
    "title": "Using R Markdown",
    "section": "Output formats",
    "text": "Output formats\nYou can specify what kind of document you create when you knit in the YAML front matter.\ntitle: \"My document\"\noutput:\n  html_document: default\n  pdf_document: default\n  word_document: default\nYou can also click on the down arrow on the “Knit” button to choose the output and generate the appropriate YAML. If you click on the gear icon next to the “Knit” button and choose “Output options”, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.\n\n\n\n\n\n\n\n\n\nThe first output type listed under output: will be what is generated when you click on the “Knit” button or press the keyboard shortcut (⌘⇧K on macOS; control + shift + K on Windows). If you choose a different output with the “Knit” button menu, that output will be moved to the top of the output section.\nThe indentation of the YAML section matters, especially when you have settings nested under each output type. Here’s what a typical output section might look like:\n---\ntitle: \"My document\"\nauthor: \"My name\"\ndate: \"January 13, 2020\"\noutput: \n  html_document: \n    toc: yes\n    fig_caption: yes\n    fig_height: 8\n    fig_width: 10\n  pdf_document: \n    latex_engine: xelatex  # More modern PDF typesetting engine\n    toc: yes\n  word_document: \n    toc: yes\n    fig_caption: yes\n    fig_height: 4\n    fig_width: 5\n---"
  },
  {
    "objectID": "resource/unzipping.html",
    "href": "resource/unzipping.html",
    "title": "Unzipping files",
    "section": "",
    "text": "Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention. Here’s a helpful guide to unzipping files on both macOS and Windows."
  },
  {
    "objectID": "resource/unzipping.html#unzipping-files-on-macos",
    "href": "resource/unzipping.html#unzipping-files-on-macos",
    "title": "Unzipping files",
    "section": "Unzipping files on macOS",
    "text": "Unzipping files on macOS\nDouble click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started."
  },
  {
    "objectID": "resource/unzipping.html#unzipping-files-on-windows",
    "href": "resource/unzipping.html#unzipping-files-on-windows",
    "title": "Unzipping files",
    "section": "Unzipping files on Windows",
    "text": "Unzipping files on Windows\ntl;dr: Right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This can be is incredibly confusing! Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top, and there’s a “Ratio” column that shows how much each file is compressed.\n\n\n\n\n\n\n\n\n\nIt is very tempting to try to open files from this view. However, if you do, things will break and you won’t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\n\n\n\n\n\n\n\n\n\nYou most likely won’t be able to open any data files or save anything, which will be frustrating.\nInstead, you need to right click on the .zip file and select “Extract All…”:\n\n\n\n\n\n\n\n\n\nThen choose where you want to unzip all the files and click on “Extract”\n\n\n\n\n\n\n\n\n\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Here’s your roadmap for the semester!\n\nContent (): This page contains the readings, slides, and recorded lectures for the topic. Read and watch these first.\nLesson (): This page contains an interactive lesson that teaches you the principles and code you need to know. Go through these after doing the content.\nExample (): This page contains fully annotated R code that you can use as a reference for creating your own visualizations. This is only a reference page—you don’t have to necessarily do anything here. Each section also contains videos of me live coding the examples so you can see what it looks like to work with R in real time. This page will be very helpful as you work on your assignments.\nAssignment (): This page contains the instructions for either the session exercise (1–3 brief tasks), or for the two mini projects and final project. Assignments are due by 11:59 PM on the Monday after their corresponding sessions. That’s confusing in sentence form—see the schedule table below to see how it works.\n\n\n\n\n\ntl;dr: You should follow this general process for each session:\n\nDo everything on the content page ()\nWork through the lesson page ()\nComplete the assignment () while referencing the example ()\n\n\n\n\n\n\n\n\n\n\n\n\nSubscribe!\n\n\n\nYou can subscribe to this calendar URL in Outlook, Google Calendar, or Apple Calendar:\n\n\n\n Download\n\n\n\n\n\n\n\n\n\n\n\nFoundations\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nJanuary 20–January 24(Session 1)\n\n\nTruth, beauty, and data + R and tidyverse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 27\n\n\nAssignment for session 1 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary 27–January 31(Session 2)\n\n\nGraphic design\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 3\n\n\nAssignment for session 2 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 3–February 7(Session 3)\n\n\nMapping data to graphics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 10\n\n\nAssignment for session 3 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCore types of graphics\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nFebruary 10–February 14(Session 4)\n\n\nAmounts and proportions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 17\n\n\nAssignment for session 4 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 17–February 21(Session 5)\n\n\nThemes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 24\n\n\nAssignment for session 5 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 24–February 28(Session 6)\n\n\nUncertainty\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 3\n\n\nAssignment for session 6 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 3–March 7(Session 7)\n\n\nRelationships\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 10\n\n\nAssignment for session 7 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 10–March 14(Session 8)\n\n\nComparisons\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 17\n\n\nAssignment for session 8 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 17\n\n\n Mini project 1 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial applications\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nMarch 17–March 21(Session 9)\n\n\nAnnotations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 24\n\n\nAssignment for session 9 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 24–March 28(Session 10)\n\n\nInteractivity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 31\n\n\nAssignment for session 10 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 31–April 4(Session 11)\n\n\nTime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 7\n\n\nAssignment for session 11 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 7–April 11(Session 12)\n\n\nSpace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 14\n\n\nAssignment for session 12 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 14\n\n\n Mini project 2 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 14–April 18(Session 13)\n\n\nText\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 21\n\n\nAssignment for session 13 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 21–April 25(Session 14)\n\n\nEnhancing graphics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 28\n\n\nAssignment for session 14 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusions\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nApril 28–April 28(Session 15)\n\n\nTruth, beauty, and data revisited\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril 29\n\n\nAssignment for session 15 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 2\n\n\n Final project due  (submit by 11:59 PM)"
  }
]